<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ğŸ”’ Linux/DevOps Command Reference - Monitoring & Security (Top 5% Industry Standard)</title>
      <link rel="icon" type="image/png" href="assets/favicon.png" />
      <link rel="stylesheet" href="assets/styles.css" />
      <script src="assets/scripts.js" defer></script>
    </head>
    <body class="index-page">
      <!-- CONTENT SECTION -->
      <div id="content">
        <div class="container">
          <div class="part-links">
            <strong>ğŸ“š Navigate:</strong>
            <a href="index.html">ğŸ  HOME</a>
            <a href="system-admin.html">ğŸ’» Part 1: System Administration</a>
            <a href="devops-tools.html">ğŸ› ï¸ Part 2: DevOps Tools</a>
            <a href="monitoring-security.html" class="active-part"
            >ğŸ”’ Part 3: Monitoring & Security</a
            >
          </div>

          <div class="hero-section">
            <h1>ğŸ”’ Monitoring & Security</h1>
            <p class="hero-subtitle">ğŸ† Top 5% Industry Standard - Section 3 of 3</p>
            <p style="font-size: 0.95em; opacity: 0.9">
            Comprehensive reference covering enterprise-grade commands and best practices
          </p>
        </div>
        <div class="content">

          <h1>ğŸ“š Linux/DevOps Command Reference Guide</h1>

          <div class="warning">
            <strong>Important Security Note:</strong> Many security commands
            require root/sudo privileges. Use with caution and only on systems you
            own or have permission to test.
          </div>
          <div class="section-index">
            <h3>ğŸ“‹ Table of Contents</h3>
            <ul id="toc-list">
              <!-- JS will populate this automatically -->
            </ul>
          </div>

          <h2 id="monitoring">ğŸ“Š 42. MONITORING COMMANDS</h2>
          <div class="command-block">
            <h4>ğŸ“Š Understanding Modern Monitoring Stack</h4>
            <pre><code>
            # The Monitoring Ecosystem:
            #
            # Prometheus (Metrics Collection & Alerting)
            #   â†“ scrapes metrics from
            # Exporters (Node Exporter, MySQL Exporter, etc.)
            #   â†“ stores time-series data
            # TSDB (Time Series Database)
            #   â†“ visualized by
            # Grafana (Dashboards & Visualization)
            #   â†“ alerts via
            # Alertmanager (Alert Routing & Notifications)

            # Why Prometheus?
            # - Pull-based model (scrapes metrics from targets)
            # - Service discovery (automatic target detection)
            # - Powerful query language (PromQL)
            # - Multi-dimensional data model (labels)
            # - No external dependencies

            # Prometheus vs Traditional Monitoring:
            # Traditional: Agent pushes metrics to central server
            # Prometheus: Server pulls metrics from targets
            #   - Better for dynamic environments (Kubernetes)
            #   - Easier troubleshooting (see what's scraped)
            #   - No agent configuration needed

            # Key Concepts:
            # Metric: Named time series (e.g., http_requests_total)
            # Label: Key-value pair for dimensions (e.g., method="GET")
            # Target: Endpoint to scrape (e.g., localhost:9100/metrics)
            # Job: Collection of similar targets (e.g., node-exporters)
            # Scrape: Act of collecting metrics from target

            # Common Exporters:
            # node_exporter - System metrics (CPU, memory, disk, network)
            # mysqld_exporter - MySQL database metrics
            # blackbox_exporter - Endpoint availability (HTTP, TCP, ICMP)
            # postgres_exporter - PostgreSQL metrics
            # redis_exporter - Redis metrics
          </code></pre>
        </div>

        <h3>ğŸ”¥ Prometheus Commands</h3>

        <div class="command-block">
          <h4>âš™ï¸ Prometheus service (systemd) - Enterprise Service Management</h4>
          <pre><code>
          # Prometheus Service Management (Enterprise Standards)
          # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

          # Service Control (Enterprise Operations)
          systemctl status prometheus                    # Check Prometheus service status
          systemctl start prometheus                     # Start Prometheus service
          systemctl stop prometheus                      # Stop Prometheus service
          systemctl enable prometheus                    # Enable at boot (Persistence)
          systemctl disable prometheus                   # Disable at boot
          systemctl restart prometheus                   # Restart Prometheus service
          systemctl reload prometheus                    # Reload configuration

          # Enterprise Service Diagnostics
          systemctl is-active prometheus                 # Check if service is active
          systemctl is-enabled prometheus                # Check if enabled at boot
          systemctl is-failed prometheus                 # Check if service failed
          journalctl -u prometheus -f                    # Follow Prometheus logs
          journalctl -u prometheus --since "1 hour ago"  # Recent Prometheus logs

          # Prometheus Configuration (Enterprise)
          # /etc/prometheus/prometheus.yml                                   # Main configuration file
          # /etc/systemd/system/prometheus.service.d/override.conf          # Service overrides
          systemctl daemon-reload                        # Reload systemd configuration
          systemctl restart prometheus                   # Apply configuration changes

          # Enterprise Process Management
          ps aux | grep prometheus                       # Check Prometheus processes
          kill -USR2 $(pidof prometheus)                 # Send SIGUSR2 for config reload
          kill -HUP $(pidof prometheus)                  # Send SIGHUP for graceful reload
        </code></pre>
      </div>

      <div class="command-block">
        <h4>âš™ï¸ Prometheus configuration - Enterprise Config Management</h4>
        <pre><code>
        # Prometheus Configuration (Enterprise Standards)
        # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

        # Configuration File Locations (Enterprise)
        /etc/prometheus/prometheus.yml                                              # Main configuration file
        /etc/prometheus/rules/*.yml                                                 # Alert rules directory
        /etc/prometheus/file_sd/*.yml                                               # Service discovery files
        /usr/local/etc/prometheus/prometheus.yml
        # Alternative location (some distributions)

        # Configuration Validation (Enterprise)
        promtool check config /etc/prometheus/prometheus.yml                        # Validate main config
        promtool check config --config.file=/etc/prometheus/prometheus.yml          # Full path validation
        promtool check config --lint                                                # Lint configuration

        # Configuration Security (Enterprise)
        chown prometheus:prometheus /etc/prometheus/prometheus.yml                  # Set proper ownership
        chmod 640 /etc/prometheus/prometheus.yml                                    # Secure file permissions
        ls -la /etc/prometheus/                                                     # Verify permissions

        # Configuration Backup (Enterprise)
        cp /etc/prometheus/prometheus.yml /backup/prometheus-$(date +%Y%m%d).yml    # Backup config
        tar -czf /backup/prometheus-config-$(date +%Y%m%d).tar.gz /etc/prometheus/  # Full backup

        # Configuration Templates (Enterprise)
        cp /etc/prometheus/prometheus.yml.template /etc/prometheus/prometheus.yml   # Use template
        envsubst < /etc/prometheus/prometheus.yml.template > /etc/prometheus/prometheus.yml
        # Environment variable substitution
      </code></pre>
    </div>
    <div class="command-block">
      <h4>ğŸ“ Basic prometheus.yml configuration - Enterprise Setup</h4>
      <pre><code>
      # Prometheus Configuration (Enterprise Standards)
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      # /etc/prometheus/prometheus.yml
      global:
      scrape_interval: 15s                     # How frequently to scrape targets
      evaluation_interval: 15s                 # How frequently to evaluate rules
      external_labels:
      cluster: 'production-cluster'            # Cluster identification
      region: 'us-west-2'                      # Geographic region

      rule_files:
      - "/etc/prometheus/rules/*.yml"          # Alert rules location
      - "/etc/prometheus/recording_rules.yml"  # Recording rules

      alerting:
      alertmanagers:
      - static_configs:
      - targets:
      - alertmanager:9093                      # Alertmanager endpoint

      scrape_configs:
      # Prometheus self-monitoring (Enterprise)
      - job_name: 'prometheus'
      static_configs:
      - targets: ['localhost:9090']
      scrape_interval: 30s                     # Less frequent for self-monitoring
      metrics_path: /metrics

      # Node Exporter (Enterprise Infrastructure)
      - job_name: 'node-exporter'
      static_configs:
      - targets: ['node1:9100', 'node2:9100', 'node3:9100']
      scrape_interval: 10s                     # More frequent for system metrics
      params:
      collect[]:
      - cpu
      - meminfo
      - diskstats
      - netdev
      - filesystem
      - filefd
      - time
      - uname
      - vmstat
      - entropy
      - wifi
      - thermal
      - processes
      - mdadm
      - xfs
      - btrfs
      - zfs
      - rapl
      - edac
      - perf
      - slabinfo
      - cpufreq
      - pressure
      - debugfs
      - procfs
      - netclass
      - devstat
      - textfile
      - mountstats
      - systemd
      - logind
      - conntrack
      - interrupts
      - ksmd
      - vm
      - linuxfirmware
      - netdev
      - qdisc
      - bcache
      - infiniband
      - ipvs
      - nfs
      - nfsd
      - pressure
      - rapl
      - selinux
      - sockstat
      - stat
      - thermal
      - timex
      - udpqueues
      - wifi
      - xfs
      - zfs
      - hwmon
      - ipmi
      - infiniband
      - mellanox
      - ntp
      - processes
      - rapl
      - schedstat
      - slip
      - softirqs
      - stat
      - thermal
      - udp
      - vm
      - xfs
      - zfs

      # Application monitoring (Enterprise)
      - job_name: 'application-metrics'
      consul_sd_configs:
      - server: 'consul:8500'
      services: ['web', 'api', 'database']
      relabel_configs:
      - source_labels: [__meta_consul_service]
      target_label: service
      - source_labels: [__meta_consul_node]
      target_label: node

      # Enterprise Storage Configuration
      storage:
      tsdb:
      path: /var/lib/prometheus/metrics2
      retention.time: 30d                      # Data retention period
      retention.size: 10GB                     # Maximum storage size
      wal.compression: true                    # Compress WAL
      wal.segment.size: 20MB                   # WAL segment size
    </code></pre>
  </div>

  <div class="command-block">
    <h4>ğŸ”„ Reload configuration - Enterprise Configuration Management</h4>
    <pre><code>
    # Prometheus Configuration Reload (Enterprise Standards)
    # Supported OS: Red Hat, Debian, Ubuntu, openSUSE, Arch


    # API-Based Reload (Enterprise)
    curl -X POST http://localhost:9090/-/reload                     # Reload via HTTP API
    curl -X POST http://localhost:9090/-/reload --verbose           # Verbose reload with headers
    curl -X POST -s http://localhost:9090/-/reload                  # Silent reload (automation-friendly)


    # Signal-Based Reload (Enterprise)
    kill -HUP $(pidof prometheus)                                   # Send SIGHUP for graceful reload
    kill -USR2 $(pidof prometheus)                                  # Send SIGUSR2 for config reload


    # Enterprise Reload Validation
    sleep 2 && curl -s http://localhost:9090/api/v1/status/config | jq .  # Verify config loaded
    journalctl -u prometheus --since "1 minute ago" | grep reload         # Check reload logs


    # Service-Based Reload (Enterprise)
    systemctl reload prometheus                                     # Systemd reload
    systemctl restart prometheus                                    # Full restart (if reload fails)


    # Configuration Validation Before Reload
    promtool check config /etc/prometheus/prometheus.yml && \
    curl -X POST http://localhost:9090/-/reload                     # Validate then reload


    # Enterprise Automation
    curl -X POST http://localhost:9090/-/reload \
    && echo "Reload successful" \
    || echo "Reload failed"                                         # Status check
  </code></pre>


</div>

<div class="command-block">
  <h4>âš¡ Prometheus CLI - Enterprise Command Line Tools</h4>
  <pre><code>
  # Prometheus CLI Tools (Enterprise Standards)
  # Supported OS: Red Hat, Debian, Ubuntu, openSUSE, Arch


  # Configuration Validation (Enterprise)
  promtool check config /etc/prometheus/prometheus.yml                  # Validate main configuration
  promtool check config --config.file=/etc/prometheus/prometheus.yml   # Full path validation
  promtool check config --lint                                         # Lint configuration for best practices
  promtool check config --rules-only                                   # Validate rule files only


  # Rule Validation (Enterprise)
  promtool check rules /etc/prometheus/rules/*.yml                     # Validate all rule files
  promtool check rules /etc/prometheus/rules/alerts.yml                # Validate specific rule file
  promtool check rules --dry-run                                       # Test rules without executing


  # Query Testing (Enterprise)
  promtool query instant 'up{job="node"}'                               # Test instant query
  promtool query instant 'cpu_usage > 0.8' --time=2024-01-01T00:00:00Z # Query at specific time
  promtool query range 'rate(http_requests_total[5m])' \
  --start=2024-01-01T00:00:00Z --end=2024-01-01T01:00:00Z --step=1m    # Range query


  # Rule Testing (Enterprise)
  promtool test rules /etc/prometheus/rules/test.yml                   # Test rule execution
  promtool test rules /etc/prometheus/rules/test.yml \
  --rules-file=/etc/prometheus/rules/alerts.yml                        # Test with specific rules


  # Enterprise Debugging
  promtool debug config /etc/prometheus/prometheus.yml                 # Debug configuration issues
  promtool debug rules /etc/prometheus/rules/alerts.yml                # Debug rule issues


  # Performance Analysis (Enterprise)
  promtool query instant 'prometheus_tsdb_head_series_appended_total'  # Check series ingestion
  promtool query instant 'prometheus_config_last_reload_successful'    # Check reload status


  # Backup and Migration (Enterprise)
  promtool check config prometheus.yml.backup                          # Validate backup config
  promtool config hash /etc/prometheus/prometheus.yml                  # Generate config hash for integrity
</code></pre>


</div>

<div class="command-block">
  <h4>ğŸ“ˆ PromQL queries (via API or UI)</h4>
  <pre><code>
  # up - Check if targets are up
  # rate(http_requests_total[5m]) - Request rate
  # sum(rate(http_requests_total[5m])) by (job) - Sum by job
  # avg(node_cpu_seconds_total) - Average CPU
</code></pre>
</div>

<h3>ğŸš¨ Alertmanager Commands</h3>

<div class="command-block">
  <h4>âš™ï¸ Alertmanager service</h4>
  <pre><code>
  systemctl status alertmanager
  systemctl start alertmanager
  systemctl stop alertmanager
  systemctl restart alertmanager
</code></pre>
</div>

<div class="command-block">
  <h4>âš™ï¸ Configuration</h4>
  <pre><code>
  /etc/alertmanager/alertmanager.yml  # Main configuration file
</code></pre>
</div>
<div class="command-block">
  <h4>ğŸ›£ï¸ Alertmanager routing configuration</h4>
  <pre><code>
  # /etc/alertmanager/alertmanager.yml
  route:
  group_by: ['alertname', 'cluster']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'
  routes:
  - match:
  severity: critical
  receiver: 'pager'
  - match:
  severity: warning
  receiver: 'email'

  receivers:
  - name: 'web.hook'
  webhook_configs:
  - url: 'http://127.0.0.1:5001/'
  - name: 'pager'
  pagerduty_configs:
  - service_key: '<your-service-key>'
  - name: 'email'
  email_configs:
  - to: 'admin@example.com'
  from: 'alertmanager@example.com'
  smarthost: 'smtp.example.com:587'
  auth_username: 'user'
  auth_password: 'password'
</code></pre>
</div>

<div class="command-block">
  <h4>âœ… Validate configuration</h4>
  <pre><code>
  amtool check-config /etc/alertmanager/alertmanager.yml
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ”„ Reload configuration</h4>
  <pre><code>
  kill -HUP $(pidof alertmanager)
</code></pre>
</div>
<div class="command-block">
  <h4>âš¡ Alertmanager CLI (amtool)</h4>
  <pre><code>
  amtool alert query                                        # Query active alerts
  amtool alert add alertname instance=host --start=now      # Add test alert
  amtool silence add alertname instance=host --duration=1h  # Silence alert for 1 hour
  amtool silence query                                      # List active silences
  amtool silence expire SILENCE_ID                          # Expire specific silence
  amtool config routes                                      # Show routing tree
  amtool config routes test --config.file=alertmanager.yml  # Test routing configuration
</code></pre>
</div>

<h3>ğŸ“¡ Node Exporter Commands</h3>

<div class="command-block">
  <h4>âš™ï¸ Node Exporter service</h4>
  <pre><code>
  systemctl status node_exporter
  systemctl start node_exporter
  systemctl restart node_exporter
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“Š Default metrics endpoint</h4>
  <pre><code>
  curl http://localhost:9100/metrics  # View metrics
</code></pre>
</div>

<h3>ğŸ“ˆ Grafana Commands</h3>

<div class="command-block">
  <h4>âš™ï¸ Grafana service</h4>
  <pre><code>
  systemctl status grafana-server
  systemctl start grafana-server
  systemctl stop grafana-server
  systemctl restart grafana-server
</code></pre>
</div>

<div class="command-block">
  <h4>âš¡ Grafana CLI</h4>
  <pre><code>
  grafana-cli plugins list-remote                     # List available plugins
  grafana-cli plugins install plugin-name             # Install plugin
  grafana-cli plugins update plugin-name              # Update plugin
  grafana-cli plugins remove plugin-name              # Remove plugin
  grafana-cli admin reset-admin-password newpassword  # Reset admin password
</code></pre>
</div>
<div class="command-block">
  <h4>ğŸ“¥ Import dashboard from Grafana.com</h4>
  <pre><code>
  # Grafana Dashboard Import
  # Source: https://grafana.com/grafana/dashboards/
  # Example Dashboard ID: 1860 (Node Exporter Full)


  # Method 1: Web UI Import

  # Navigate:
  # Dashboards â†’ Import â†’ Enter Dashboard ID â†’ Load â†’ Save


  # Method 2: Import via Grafana API

  curl -X POST http://admin:admin@localhost:3000/api/dashboards/db \
  -H "Content-Type: application/json" \
  -d '{
  "dashboard": {
  "id": null,
  "title": "Node Exporter Full"
  },
  "overwrite": true
  }'


  # Method 3: Provisioning (Production Recommended)
  # File: /etc/grafana/provisioning/dashboards/dashboards.yml

  apiVersion: 1
  providers:
  - name: default
  orgId: 1
  folder: ''
  type: file
  disableDeletion: false
  updateIntervalSeconds: 10
  allowUiUpdates: true
  options:
  path: /var/lib/grafana/dashboards
</code></pre>

</div>

<div class="command-block">
  <h4>âš™ï¸ Grafana configuration</h4>
  <pre><code>
  /etc/grafana/grafana.ini  # Main config file
</code></pre>
</div>

<h3>ğŸ” Elasticsearch Commands</h3>

<div class="command-block">
  <h4>âš™ï¸ Elasticsearch service</h4>
  <pre><code>
  systemctl status elasticsearch
  systemctl start elasticsearch
  systemctl stop elasticsearch
  systemctl restart elasticsearch
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ’š Cluster health</h4>
  <pre><code>
  curl -X GET "localhost:9200/_cluster/health?pretty"
  curl -X GET "localhost:9200/_cat/health?v"
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ–¥ï¸ Nodes</h4>
  <pre><code>
  curl -X GET "localhost:9200/_cat/nodes?v"
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“š Indices</h4>
  <pre><code>
  curl -X GET "localhost:9200/_cat/indices?v"
  curl -X PUT "localhost:9200/myindex"     # Create index
  curl -X DELETE "localhost:9200/myindex"  # Delete index
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“„ Documents</h4>
  <pre><code>
  curl -X POST "localhost:9200/myindex/_doc/1" -H 'Content-Type: application/json' -d '{"field":"value"}'   # Index document

  curl -X GET "localhost:9200/myindex/_doc/1" # Get document

  curl -X DELETE "localhost:9200/myindex/_doc/1"
  # Delete document
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ” Search</h4>
  <pre><code>
  curl -X GET "localhost:9200/myindex/_search?pretty" -H 'Content-Type: application/json' -d '{"query":{"match_all":{}}}'
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“¦ Bulk operations</h4>
  <pre><code>
  curl -X POST "localhost:9200/_bulk" -H 'Content-Type: application/json' --data-binary "@bulk.json"
</code></pre>
</div>
<div class="command-block">
  <h4>ğŸ“¸ Snapshots (Backup/Restore)</h4>
  <pre><code>
  curl -X PUT "localhost:9200/_snapshot/backup_repo" -H 'Content-Type: application/json' -d '{"type":"fs","settings":{"location":"/mnt/backups"}}'
  # Create repository

  curl -X PUT "localhost:9200/_snapshot/backup_repo/snapshot_1?wait_for_completion=true"
  # Create snapshot

  curl -X GET "localhost:9200/_snapshot/backup_repo/_all"
  # List snapshots

  curl -X POST "localhost:9200/_snapshot/backup_repo/snapshot_1/_restore"
  # Restore snapshot

  curl -X DELETE "localhost:9200/_snapshot/backup_repo/snapshot_1"
  # Delete snapshot
</code></pre>
</div>

<h3>ğŸ“ Logstash Commands</h3>

<div class="command-block">
  <h4>âš™ï¸ Logstash service</h4>
  <pre><code>
  systemctl status logstash
  systemctl start logstash
  systemctl stop logstash
  systemctl restart logstash
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ§ª Test configuration</h4>
  <pre><code>
  /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/logstash.conf --config.test_and_exit
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ—ï¸ Understanding ELK/EFK Stack Architecture</h4>
  <pre><code>
  # ELK Stack Components:
  #
  # Elasticsearch (Storage & Search Engine)
  #   â†‘ receives data from
  # Logstash (Processing & Transformation)
  #   â†‘ collects logs from
  # Beats (Lightweight Shippers)
  #   â†‘ monitors
  # Applications/Systems (Log Sources)
  #   â†“ visualized by
  # Kibana (Visualization & Dashboards)

  # EFK Stack (Alternative):
  # Replace Logstash with Fluentd (lighter, more flexible)

  # Why ELK Stack?
  # 1. Centralized logging - All logs in one place
  # 2. Full-text search - Find anything instantly
  # 3. Real-time analysis - Live log monitoring
  # 4. Scalability - Horizontal scaling (add nodes)
  # 5. Flexibility - Custom parsers, filters, outputs

  # Elasticsearch Concepts:
  # Index: Collection of documents (like database)
  # Document: Basic unit of data (like row in table)
  # Shard: Subset of index for distribution
  # Replica: Copy of shard for redundancy
  # Node: Single Elasticsearch instance
  # Cluster: Group of nodes working together

  # Logstash Pipeline:
  # Input â†’ Filter â†’ Output
  #
  # Input: Where data comes from (files, beats, syslog)
  # Filter: Transform data (grok, mutate, date parsing)
  # Output: Where to send (elasticsearch, file, kafka)

  # Beats Family:
  # Filebeat - Log files
  # Metricbeat - System metrics
  # Packetbeat - Network packets
  # Auditbeat - Audit events
  # Heartbeat - Uptime monitoring
  # Winlogbeat - Windows event logs

  # Common Use Cases:
  # - Application log analysis
  # - Security event monitoring (SIEM)
  # - Infrastructure monitoring
  # - APM (Application Performance Monitoring)
  # - Business analytics

  # Data Flow Example:
  # Application â†’ Filebeat â†’ Logstash â†’ Elasticsearch â†’ Kibana
  # 1. App writes logs to file
  # 2. Filebeat tails log file
  # 3. Logstash parses and enriches logs
  # 4. Elasticsearch indexes logs
  # 5. Kibana displays dashboards
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ”„ Reload configuration</h4>
  <pre><code>
  systemctl reload logstash
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸƒ Run Logstash manually</h4>
  <pre><code>
  /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/logstash.conf
</code></pre>
</div>

<h3>ğŸ“Š Kibana Commands</h3>

<div class="command-block">
  <h4>âš™ï¸ Kibana service</h4>
  <pre><code>
  systemctl status kibana
  systemctl start kibana
  systemctl stop kibana
  systemctl restart kibana
</code></pre>
</div>

<div class="command-block">
  <h4>âš™ï¸ Kibana configuration</h4>
  <pre><code>
  /etc/kibana/kibana.yml  # Main config file
</code></pre>
</div>
<div class="command-block">
  <h4>ğŸ¯ Create Kibana index pattern via API</h4>
  <pre><code>
  # List existing indices in Elasticsearch
  curl -X GET "localhost:9200/_cat/indices?v"

  # Create index pattern in Kibana (for filebeat-* indices)
  curl -X POST "localhost:5601/api/saved_objects/index-pattern/filebeat-*" \
  -H 'kbn-xsrf: true' \
  -H 'Content-Type: application/json' \
  -d '{
  "attributes": {
  "title": "filebeat-*",
  "timeFieldName": "@timestamp"
  }
  }'

  # Set as default index pattern
  curl -X POST "localhost:5601/api/kibana/settings/defaultIndex" \
  -H 'kbn-xsrf: true' \
  -H 'Content-Type: application/json' \
  -d '{"value": "filebeat-*"}'
</code></pre>
</div>

<h3>ğŸ“ Filebeat Commands</h3>

<div class="command-block">
  <h4>âš™ï¸ Filebeat service</h4>
  <pre><code>
  systemctl status filebeat
  systemctl start filebeat
  systemctl stop filebeat
  systemctl restart filebeat
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ§ª Test configuration</h4>
  <pre><code>
  filebeat test config -c /etc/filebeat/filebeat.yml
  filebeat test output -c /etc/filebeat/filebeat.yml
</code></pre>
</div>

<div class="command-block">
  <h4>âš™ï¸ Setup</h4>
  <pre><code>
  filebeat setup -e                # Setup index template and dashboards
  filebeat modules list            # List available modules
  filebeat modules enable system   # Enable module
  filebeat modules disable system  # Disable module
</code></pre>
</div>

<div class="command-block">
  <h4>âš™ï¸ Filebeat configuration</h4>
  <pre><code>
  /etc/filebeat/filebeat.yml  # Main config file
</code></pre>
</div>
<h3>ğŸ“Š Metricbeat Commands</h3>

<div class="command-block">
  <h4>âš™ï¸ Metricbeat service</h4>
  <pre><code>
  systemctl status metricbeat
  systemctl start metricbeat
  systemctl stop metricbeat
  systemctl restart metricbeat
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ§ª Test configuration</h4>
  <pre><code>
  metricbeat test config -c /etc/metricbeat/metricbeat.yml
  metricbeat test output -c /etc/metricbeat/metricbeat.yml
  metricbeat test modules system  # Test specific module
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ”§ Module management</h4>
  <pre><code>
  metricbeat modules list            # List available modules
  metricbeat modules enable system   # Enable module
  metricbeat modules disable system  # Disable module
  metricbeat setup -e                # Setup dashboards and index templates
</code></pre>
</div>

<div class="command-block">
  <h4>âš™ï¸ Metricbeat configuration</h4>
  <pre><code>
  /etc/metricbeat/metricbeat.yml  # Main config file
  /etc/metricbeat/modules.d/      # Module configurations
</code></pre>
</div>

<h2 id="logging-auditing">ğŸ“‹ 43. LOGGING & AUDITING (Advanced ELK/EFK stack, centralized logging, alerting rules)</h2>

<h3>ğŸ“ Centralized Logging</h3>
<div class="command-block">
  <h4>ğŸ“¤ Forward System Logs</h4>
  <pre><code>
  rsyslogd -N1                                  # Check rsyslog config
  systemctl restart rsyslog                     # Restart logging service
  journalctl -f | nc logserver.example.com 514  # Forward logs to central server
  filebeat setup -e                             # Setup dashboards and indices
</code></pre>
</div>

<h3>ğŸš¨ Alerting Rules</h3>
<div class="command-block">
  <h4>ğŸš¨ Create Alerts in Prometheus</h4>
  <pre><code>
  groups:
  - name: node_alerts
  rules:
  - alert: HighCPU
  expr: 100 - (avg by(instance)(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
  for: 5m
  labels:
  severity: critical
  annotations:
  summary: "High CPU usage detected"
</code></pre>
</div>

<h2 id="observability">ğŸ” 44. OBSERVABILITY (Metrics + Logs + Traces, OpenTelemetry)</h2>

<h3>ğŸ“Š Metrics, Logs, Traces</h3>
<div class="command-block">
  <h4>ğŸ”§ OpenTelemetry Collector</h4>
  <pre><code>
  otelcol --config otel-config.yaml  # Start collector with config
  curl http://localhost:4317         # Verify gRPC metrics endpoint
  otel-collector logs/otel.log       # Tail collector logs
</code></pre>
</div>

<h3>ğŸ”§ Instrument Application</h3>
<div class="command-block">
  <h4>ğŸ Python Example</h4>
  <pre><code>
  from opentelemetry import metrics, trace
  from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
  # Initialize tracer and metrics to send telemetry data to collector
</code></pre>
</div>
<h3>ğŸ” Jaeger Distributed Tracing</h3>

<div class="command-block">
  <h4>ğŸ”§ Jaeger all-in-one</h4>
  <pre><code>
  jaeger-all-in-one --collector.zipkin.host-port=:9411
  # Start Jaeger with Zipkin compatibility
  # Access Jaeger UI: http://localhost:16686
  docker run -d --name jaeger -p 16686:16686 -p 14268:14268 jaegertracing/all-in-one:latest  # Docker deployment
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ” Query traces</h4>
  <pre><code>
  curl "http://localhost:16686/api/services"                           # List services
  curl "http://localhost:16686/api/traces?service=myservice&limit=20"  # Query traces for service
</code></pre>
</div>

<h2 id="security">ğŸ›¡ï¸ 45. SECURITY AND HARDENING</h2>

<div class="danger">
  <strong>DANGER:</strong> Security commands can lock you out of your
  system if misconfigured. Always test in a safe environment first.
</div>
<h3>ğŸ” Auditd Commands (System Auditing)</h3>

<div class="command-block">
  <h4>âš™ï¸ Audit service</h4>
  <pre><code>
  systemctl status auditd
  systemctl start auditd
  systemctl restart auditd
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“‹ Audit rules management</h4>
  <pre><code>
  auditctl -l                                  # List active rules
  auditctl -w /etc/passwd -p wa -k passwd_chg  # Watch passwd changes
  auditctl -w /var/log/secure -p wa            # Monitor auth logs
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ” Search audit logs</h4>
  <pre><code>
  ausearch -k passwd_chg
  ausearch -m USER_LOGIN -ts today
  aureport -x --summary
</code></pre>
</div>
<h3>ğŸ”’ OpenSCAP (Security Compliance)</h3>

<div class="command-block">
  <h4>ğŸ“¦ Install OpenSCAP</h4>
  <pre><code>
  dnf install openscap-scanner scap-security-guide  # RHEL/CentOS
  apt install libopenscap8 ssg-base ssg-debderived  # Debian/Ubuntu
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ” Scan system compliance</h4>
  <pre><code>
  # PCI-DSS scan
  oscap xccdf eval --profile xccdf_org.ssgproject.content_profile_pci-dss /usr/share/xml/scap/ssg/content/ssg-rhel8-ds.xml

  # CIS benchmark
  oscap xccdf eval --profile xccdf_org.ssgproject.content_profile_cis /usr/share/xml/scap/ssg/content/ssg-rhel8-ds.xml

  # Generate HTML report
  oscap xccdf eval --report report.html --profile pci-dss /usr/share/xml/scap/ssg/content/ssg-rhel8-ds.xml
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“‹ List available profiles</h4>
  <pre><code>
  oscap info /usr/share/xml/scap/ssg/content/ssg-rhel8-ds.xml  # Show available profiles
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ”§ Remediate findings</h4>
  <pre><code>
  # Auto-remediate (CAUTION)
  oscap xccdf eval --remediate --profile pci-dss /usr/share/xml/scap/ssg/content/ssg-rhel8-ds.xml
</code></pre>
</div>

<h3>ğŸš« fail2ban Commands</h3>

<div class="command-block">
  <h4>âš™ï¸ fail2ban service</h4>
  <pre><code>
  systemctl status fail2ban
  systemctl start fail2ban
  systemctl stop fail2ban
  systemctl restart fail2ban
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ’» fail2ban-client</h4>
  <pre><code>
  fail2ban-client status                         # Show status
  fail2ban-client status sshd                    # Show jail status
  fail2ban-client set sshd banip 192.168.1.10    # Ban IP
  fail2ban-client set sshd unbanip 192.168.1.10  # Unban IP
  fail2ban-client reload                         # Reload configuration
  fail2ban-client ping                           # Test if running
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“ Configuration files</h4>
  <pre><code>
  /etc/fail2ban/jail.conf   # Main config (don't edit)
  /etc/fail2ban/jail.local  # Local overrides
</code></pre>
</div>
<div class="command-block">
  <h4>ğŸ› ï¸ Custom fail2ban filter for Nginx</h4>
  <pre><code>
  # /etc/fail2ban/filter.d/nginx-404.conf
  [Definition]
  failregex = ^<HOST>.*"GET.* 404 .*$
  ignoreregex =
  # /etc/fail2ban/jail.local
  [nginx-404]
  enabled = true
  port    = http,https
  filter  = nginx-404
  logpath = /var/log/nginx/access.log
  maxretry = 10
  findtime = 600
  bantime = 3600
  action = iptables[name=nginx, port=http, protocol=tcp]
  # Test the filter
  fail2ban-regex /var/log/nginx/access.log /etc/fail2ban/filter.d/nginx-404.conf
</code></pre>
</div>
<div class="command-block">
  <h4>ğŸ“‹ Persistent audit rules</h4>
  <pre><code>
  /etc/audit/rules.d/audit.rules      # Persistent rules file
  # Add rules to this file, then reload:
  augenrules --load                   # Load rules from /etc/audit/rules.d/
  service auditd restart              # Restart to apply changes
  auditctl -R /etc/audit/audit.rules  # Reload specific rules file
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“‹ Common audit rules</h4>
  <pre><code>
  # File integrity monitoring
  auditctl -w /etc/shadow -p wa -k shadow_changes
  auditctl -w /etc/sudoers -p wa -k sudoers_changes
  # Privileged command monitoring
  auditctl -a always,exit -F arch=b64 -S execve -F euid=0 -k root_commands
  # Network modifications
  auditctl -w /etc/sysconfig/network-scripts/ -p wa -k network_config
</code></pre>
</div>

<h3>ğŸ”’ SELinux Commands</h3>

<div class="command-block">
  <h4>ğŸ“š Understanding SELinux (Security-Enhanced Linux)</h4>
  <pre><code>
  # What is SELinux?
  # - Mandatory Access Control (MAC) system
  # - Originally developed by NSA
  # - Built into Linux kernel
  # - Default security system in RHEL/CentOS/Fedora

  # SELinux vs Traditional Linux Security:
  # Traditional: Discretionary Access Control (DAC)
  #   - Users control their own files
  #   - root user has unlimited access
  #   - Relies on file permissions (rwx)
  #
  # SELinux: Mandatory Access Control (MAC)
  #   - System-wide security policy
  #   - Even root is restricted
  #   - Based on security contexts

  # SELinux Modes:
  # Enforcing - SELinux policy is enforced (blocks violations)
  # Permissive - Policy not enforced, only logs violations
  # Disabled - SELinux is turned off (not recommended)

  # SELinux Context Format:
  # user:role:type:level
  #
  # Example: system_u:object_r:httpd_sys_content_t:s0
  #   user: system_u (SELinux user)
  #   role: object_r (object role)
  #   type: httpd_sys_content_t (most important - defines access)
  #   level: s0 (MLS/MCS security level)

  # Type Enforcement (TE):
  # - Most common SELinux policy
  # - Controls what processes (domains) can access what files (types)
  # - Example: httpd_t domain can read httpd_sys_content_t files

  # Common SELinux Types:
  # httpd_sys_content_t - Web server content (read-only)
  # httpd_sys_rw_content_t - Web server content (read-write)
  # httpd_sys_script_exec_t - CGI scripts
  # user_home_t - User home directories
  # tmp_t - Temporary files

  # SELinux Booleans:
  # - Toggle specific SELinux policies on/off
  # - Allow exceptions without writing custom policies
  # - Example: httpd_can_network_connect (allow Apache to make network connections)

  # Common SELinux Issues:
  # 1. New service won't start
  #    Solution: Check AVC denials, set correct context
  #
  # 2. Custom port for service
  #    Solution: semanage port -a -t http_port_t -p tcp 8080
  #
  # 3. Files in non-standard location
  #    Solution: restorecon -Rv /custom/path or chcon

  # Troubleshooting Workflow:
  # 1. Check if SELinux is the problem: setenforce 0
  # 2. If it works, SELinux is blocking it
  # 3. Check logs: ausearch -m avc -ts recent
  # 4. Understand why: audit2why < /var/log/audit/audit.log
  # 5. Fix: Either set context or modify policy
  # 6. Re-enable SELinux: setenforce 1

  # Best Practices:
  # - Never disable SELinux in production
  # - Use permissive mode for testing
  # - Always check AVC denials before writing custom policies
  # - Use semanage for permanent changes
  # - Document any custom policies
</code></pre>
</div>

<div class="command-block">
  <h4>âš™ï¸ SELinux Management</h4>
  <pre><code>
  getenforce                                                # Check SELinux status
  sestatus                                                  # Detailed SELinux status
  setenforce 1                                              # Enable enforcing mode
  setenforce 0                                              # Disable (permissive)
  semanage boolean -l                                       # List SELinux booleans
  semanage boolean --list -C                                # List modified booleans
  semanage boolean --modify --on httpd_can_network_connect  # Modify boolean
  semanage port -l                                          # List port mappings
  semanage port -a -t http_port_t -p tcp 8080               # Add port mapping
  restorecon -Rv /path/to/dir                               # Restore default contexts
  chcon -t httpd_sys_content_t /var/www/html                # Change file context
  ls -Z                                                     # View SELinux contexts
  ps -Z                                                     # View process contexts
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ·ï¸ Contexts</h4>
  <pre><code>
  ls -Z file                                             # Show file context
  ps -Z                                                  # Show process contexts
  chcon -t httpd_sys_content_t /var/www/html/index.html  # Change context
  restorecon -Rv /var/www/html                           # Restore default contexts
</code></pre>
</div>

<div class="command-block">
  <h4>âœ… Booleans</h4>
  <pre><code>
  getsebool -a                               # List all booleans
  getsebool httpd_can_network_connect        # Check boolean
  setsebool -P httpd_can_network_connect on  # Set boolean permanently
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ”§ Troubleshooting</h4>
  <pre><code>
  ausearch -m avc -ts recent            # Recent AVC denials
  audit2why < /var/log/audit/audit.log  # Why was it denied
  audit2allow -a                        # Generate policy to allow
  ausearch -m USER_LOGIN -ts today      # Show todayâ€™s login events
  aureport -u                           # User login summary
</code></pre>
</div>

<h3>ğŸ” OpenSSL Commands</h3>
<div class="command-block">
  <h4>ğŸ” Understanding SSL/TLS Certificates</h4>
  <pre><code>
  # Certificate Components:
  #
  # Private Key - Secret key (NEVER share)
  #   â†“ generates
  # Public Key - Can be shared freely
  #   â†“ creates
  # CSR (Certificate Signing Request) - Request for certificate
  #   â†“ sent to
  # CA (Certificate Authority) - Trusted third party
  #   â†“ signs and returns
  # Certificate - Signed public key + identity info

  # Certificate Types:
  # 1. Self-Signed Certificate
  #    - Signed by own private key (not trusted by browsers)
  #    - Use: Testing, internal services, development
  #    - Command: openssl req -x509 -newkey rsa:2048 -nodes -out cert.crt
  #
  # 2. CA-Signed Certificate
  #    - Signed by trusted Certificate Authority
  #    - Use: Production websites, public services
  #    - Examples: Let's Encrypt, DigiCert, Comodo
  #
  # 3. Wildcard Certificate
  #    - Valid for *.example.com (all subdomains)
  #    - More expensive but covers unlimited subdomains

  # SSL/TLS Versions:
  # SSL 2.0 - Deprecated (insecure)
  # SSL 3.0 - Deprecated (POODLE vulnerability)
  # TLS 1.0 - Deprecated (weak)
  # TLS 1.1 - Deprecated (weak)
  # TLS 1.2 - Current standard (widely used)
  # TLS 1.3 - Latest (faster, more secure)

  # Certificate Validity:
  # - Typically 1-2 years for CA-signed certs
  # - Let's Encrypt: 90 days (auto-renewal recommended)
  # - Check expiry: openssl x509 -in cert.crt -noout -dates

  # Common Use Cases:
  # 1. HTTPS websites (Apache, Nginx)
  # 2. Email encryption (SMTP, IMAP)
  # 3. VPN connections
  # 4. Code signing
  # 5. SSH key pairs (different format, but similar concept)

  # Certificate Chain:
  # Server Certificate
  #   â†“ signed by
  # Intermediate CA Certificate
  #   â†“ signed by
  # Root CA Certificate (pre-installed in browsers/OS)

  # File Formats:
  # .pem - Base64 encoded (most common, text format)
  # .crt - Certificate only (can be PEM or DER)
  # .key - Private key
  # .csr - Certificate Signing Request
  # .der - Binary format (compact)
  # .p12/.pfx - PKCS#12 (contains certificate + private key)

  # Common OpenSSL Operations:
  # Generate private key â†’ Create CSR â†’ Send to CA â†’ Receive certificate
  #
  # For self-signed (testing):
  # openssl req -x509 -newkey rsa:2048 -nodes -keyout key.pem -out cert.pem -days 365

  # Verify Certificate Chain:
  # openssl verify -CAfile ca-bundle.crt server.crt

  # Test SSL/TLS:
  # openssl s_client -connect example.com:443 -servername example.com
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ”‘ Generate private key</h4>
  <pre><code>
  openssl genrsa -out private.key 2048  # RSA 2048-bit
  openssl genrsa -out private.key 4096  # RSA 4096-bit
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ” Generate public key from private</h4>
  <pre><code>
  openssl rsa -in private.key -pubout -out public.key
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“‹ Generate CSR (Certificate Signing Request)</h4>
  <pre><code>
  openssl req -new -key private.key -out cert.csr
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“œ Generate self-signed certificate</h4>
  <pre><code>
  openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout private.key -out cert.crt
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ‘ï¸ View certificate</h4>
  <pre><code>
  openssl x509 -in cert.crt -text -noout
  openssl x509 -in cert.crt -noout -dates    # Show validity dates
  openssl x509 -in cert.crt -noout -subject  # Show subject
  openssl x509 -in cert.crt -noout -issuer   # Show issuer
</code></pre>
</div>

<div class="command-block">
  <h4>âœ… Verify certificate</h4>
  <pre><code>
  openssl verify cert.crt
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ”— Test SSL/TLS connection</h4>
  <pre><code>
  openssl s_client -connect google.com:443
  openssl s_client -connect google.com:443 -showcerts
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ”„ Convert certificate formats</h4>
  <pre><code>
  openssl x509 -in cert.crt -out cert.pem               # CRT to PEM
  openssl x509 -in cert.pem -outform DER -out cert.der  # PEM to DER
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ” Encrypt/Decrypt files</h4>
  <pre><code>
  openssl enc -aes-256-cbc -salt -in file.txt -out file.txt.enc
  openssl enc -aes-256-cbc -d -in file.txt.enc -out file.txt
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ² Generate random data</h4>
  <pre><code>
  openssl rand -base64 32  # 32 bytes base64
  openssl rand -hex 16     # 16 bytes hex
</code></pre>
</div>

<h3>ğŸ”‘ GPG Commands</h3>

<div class="command-block">
  <h4>ğŸ”‘ Generate key pair</h4>
  <pre><code>
  gpg --gen-key            # Interactive
  gpg --full-generate-key  # Full options
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ”‘ List keys</h4>
  <pre><code>
  gpg --list-keys         # Public keys
  gpg --list-secret-keys  # Private keys
  gpg --list-sigs         # Keys with signatures
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“¤ Export keys</h4>
  <pre><code>
  gpg --export -a "User Name" > public.key               # Export public key (ASCII)
  gpg --export-secret-keys -a "User Name" > private.key  # Export private key
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“¥ Import keys</h4>
  <pre><code>
  gpg --import public.key   # Import public key
  gpg --import private.key  # Import private key
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ” Encrypt/Decrypt files</h4>
  <pre><code>
  gpg --encrypt --recipient "User Name" file.txt  # Encrypt
  gpg --output file.txt --decrypt file.txt.gpg    # Decrypt
  gpg -c file.txt                                 # Symmetric encryption (password)
  gpg -d file.txt.gpg                             # Decrypt symmetric
</code></pre>
</div>

<div class="command-block">
  <h4>âœï¸ Sign files</h4>
  <pre><code>
  gpg --sign file.txt         # Sign file
  gpg --clearsign file.txt    # Sign with readable text
  gpg --detach-sign file.txt  # Detached signature
  gpg --verify file.txt.sig   # Verify signature
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ”‘ Key management</h4>
  <pre><code>
  gpg --edit-key "User Name"            # Edit key (interactive)
  gpg --delete-keys "User Name"         # Delete public key
  gpg --delete-secret-keys "User Name"  # Delete private key
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ¤ Trust keys</h4>
  <pre><code>
  gpg --edit-key "User Name"
  # In GPG prompt: trust, then select level (1-5)
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸŒ Send/receive keys (keyserver)</h4>
  <pre><code>
  gpg --keyserver keyserver.ubuntu.com --send-keys KEY_ID
  gpg --keyserver keyserver.ubuntu.com --recv-keys KEY_ID
  gpg --keyserver keyserver.ubuntu.com --search-keys "email@example.com"
</code></pre>
</div>

<h3>ğŸ” Lynis (Security Auditing)</h3>

<div class="command-block">
  <h4>ğŸ“¦ Install Lynis</h4>
  <pre><code>
  dnf install lynis  # RHEL/CentOS
  apt install lynis  # Debian/Ubuntu
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ” Run audit</h4>
  <pre><code>
  lynis audit system            # Full system audit
  lynis audit system --quick    # Quick scan
  lynis audit system --cronjob  # Cron mode (no colors)
</code></pre>
</div>

<div class="command-block">
  <h4>âš™ï¸ Show options</h4>
  <pre><code>
  lynis show options   # Display options
  lynis show version   # Show version
  lynis show commands  # List commands
  lynis show profiles  # Show profiles
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ”„ Update Lynis</h4>
  <pre><code>
  lynis update info  # Check for updates
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“Š Reports</h4>
  <pre><code>
  /var/log/lynis.log         # Audit log
  /var/log/lynis-report.dat  # Report data
</code></pre>
</div>
<h3>ğŸ›¡ï¸ AIDE (File Integrity Monitoring)</h3>

<div class="command-block">
  <h4>ğŸ—„ï¸ Initialize AIDE database</h4>
  <pre><code>
  aide --init                                               # Create initial database
  mv /var/lib/aide/aide.db.new.gz /var/lib/aide/aide.db.gz  # Move database to production
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ” Check for changes</h4>
  <pre><code>
  aide --check                                              # Check for file changes
  aide --check --config=/etc/aide.conf                      # Specify config file
  aide --update                                             # Update database after changes
  mv /var/lib/aide/aide.db.new.gz /var/lib/aide/aide.db.gz  # Replace old database
</code></pre>
</div>

<div class="command-block">
  <h4>âš™ï¸ AIDE configuration</h4>
  <pre><code>
  /etc/aide.conf  # Main configuration file
  # Example rules:
  # /bin    R+b+sha256                                            # Monitor /bin directory
  # /etc    R+b+sha256                                            # Monitor /etc directory
  # !/var/log                                                     # Exclude /var/log
</code></pre>
</div>

<h3>ğŸ¦  ClamAV (Antivirus)</h3>

<div class="command-block">
  <h4>âš™ï¸ ClamAV service</h4>
  <pre><code>
  systemctl status clamd@scan  # Check service
  systemctl start clamd@scan   # Start service
  systemctl enable clamd@scan  # Enable at boot
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ”„ Update virus definitions</h4>
  <pre><code>
  freshclam    # Update virus database
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ¦  Scan files</h4>
  <pre><code>
  clamscan file.txt                     # Scan single file
  clamscan -r /home                     # Recursive scan
  clamscan -r --infected /home          # Show only infected files
  clamscan -r --remove /home            # Remove infected files
  clamscan -r --move=/quarantine /home  # Move infected to quarantine
  clamscan -r -l scan.log /home         # Log to file
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸƒ Background scanning</h4>
  <pre><code>
  clamd            # Start daemon
  clamdscan /path  # Scan using daemon (faster)
</code></pre>
</div>

<h3>ğŸ›¡ï¸ğŸ”’ AppArmor Commands</h3>

<div class="command-block">
  <h4>ğŸ“Š AppArmor status</h4>
  <pre><code>
  apparmor_status  # Show AppArmor status
  aa-status        # Alternative command
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“ Profile management</h4>
  <pre><code>
  aa-enforce /etc/apparmor.d/usr.bin.program   # Set to enforce mode
  aa-complain /etc/apparmor.d/usr.bin.program  # Set to complain mode
  aa-disable /etc/apparmor.d/usr.bin.program   # Disable profile
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ”„ Load/unload profiles</h4>
  <pre><code>
  apparmor_parser -r /etc/apparmor.d/usr.bin.program  # Reload profile
  apparmor_parser -R /etc/apparmor.d/usr.bin.program  # Remove profile
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ”§ Generate profiles</h4>
  <pre><code>
  aa-genprof /usr/bin/program  # Generate profile (interactive)
  aa-logprof                   # Update profiles from logs
</code></pre>
</div>
<h3>ğŸ” Falco (Runtime Threat Detection)</h3>

<div class="command-block">
  <h4>âš™ï¸ Falco service</h4>
  <pre><code>
  systemctl status falco
  systemctl start falco
  systemctl restart falco
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ§ª Test Falco rules</h4>
  <pre><code>
  falco --validate /etc/falco/falco_rules.yaml
  falco --list
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ§ª Trigger test event</h4>
  <pre><code>
  cat /etc/shadow
</code></pre>
</div>

<h2 id="performance">âš¡ 46. PERFORMANCE MONITORING & TUNING</h2>

<h3>ğŸ’¾ iostat (I/O Statistics)</h3>
<div class="command-block">
  <h4>ğŸ“Š Understanding Performance Monitoring Tools</h4>
  <pre><code>
  # Linux Performance Observability Tools:
  #
  # CPU Monitoring:
  # - top/htop - Real-time process viewer
  # - mpstat - Per-CPU statistics
  # - sar -u - Historical CPU usage
  # - pidstat -u - Per-process CPU
  #
  # Memory Monitoring:
  # - free - Memory usage overview
  # - vmstat - Virtual memory stats
  # - sar -r - Historical memory
  # - pidstat -r - Per-process memory
  #
  # Disk I/O Monitoring:
  # - iostat - I/O statistics
  # - iotop - I/O by process
  # - sar -d - Historical disk I/O
  # - pidstat -d - Per-process I/O
  #
  # Network Monitoring:
  # - ss/netstat - Socket statistics
  # - sar -n DEV - Network interface stats
  # - nethogs - Bandwidth by process
  # - iftop - Interface bandwidth monitor

  # Key Performance Metrics:
  #
  # Load Average (uptime):
  # - 1, 5, 15 minute averages
  # - Number of processes waiting for CPU
  # - Rule of thumb: < number of CPU cores is good
  # - Example: Load of 2.0 on dual-core system = 100% utilized
  #
  # CPU Usage (%):
  # - us (user) - User-space processes
  # - sy (system) - Kernel-space processes
  # - id (idle) - Idle time (higher = less utilized)
  # - wa (iowait) - Waiting for I/O (high = I/O bottleneck)
  # - st (steal) - Time stolen by hypervisor (VM environments)
  #
  # Memory Usage:
  # - total - Total physical RAM
  # - used - Used memory
  # - free - Unused memory
  # - available - Memory available for new apps (includes cache)
  # - buffers/cache - Kernel buffers and page cache
  # - swap - Virtual memory on disk (high swap usage = low RAM)
  #
  # Disk I/O:
  # - tps (transactions per second) - I/O operations
  # - kB_read/s - Kilobytes read per second
  # - kB_wrtn/s - Kilobytes written per second
  # - %util - Disk utilization (>80% = bottleneck)

  # Performance Bottleneck Identification:
  #
  # CPU Bottleneck:
  # - High load average (> CPU cores)
  # - Low %id (idle) in top
  # - High %us or %sy
  # Solution: Add CPU cores, optimize code, use caching
  #
  # Memory Bottleneck:
  # - Low "available" memory
  # - High swap usage
  # - Frequent page faults
  # Solution: Add RAM, optimize memory usage, tune swappiness
  #
  # Disk I/O Bottleneck:
  # - High %iowait in top
  # - High %util in iostat
  # - Long response times
  # Solution: Use SSD, RAID, optimize queries, add cache
  #
  # Network Bottleneck:
  # - Packet drops/errors
  # - High bandwidth utilization
  # - Connection timeouts
  # Solution: Upgrade bandwidth, use CDN, optimize traffic

  # Monitoring Best Practices:
  # 1. Establish baseline metrics (normal system behavior)
  # 2. Monitor trends over time (not just current values)
  # 3. Set up alerts for thresholds
  # 4. Collect metrics at regular intervals (e.g., every 5 min)
  # 5. Archive historical data for analysis
  # 6. Use visualization tools (Grafana) for easier interpretation

  # Common Performance Issues:
  #
  # High Load Average + Low CPU Usage = I/O bottleneck
  # High CPU Usage + Low Load = CPU-intensive but not waiting
  # High Memory Usage + Swap = Need more RAM
  # Network Drops + Low Bandwidth = Configuration issue
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“Š Basic I/O stats - Enterprise Disk Performance Analysis</h4>
  <pre><code>
  # Enterprise I/O Statistics Analysis (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Basic I/O Monitoring (Enterprise)
  iostat          # Basic report (current I/O statistics)
  iostat -x       # Extended statistics (detailed I/O metrics)
  iostat -x 1 10  # Every 1 second, 10 times (monitoring interval)
  iostat -d       # Device utilization only (simplified output)
  iostat -c       # CPU utilization only (CPU-focused monitoring)
  iostat -p sda   # Specific device monitoring (targeted analysis)
  iostat -m       # Display in MB/s (megabytes per second)
  iostat -k       # Display in KB/s (kilobytes per second)

  # Enterprise I/O Analysis Parameters (Enterprise)
  # -r/s: Reads per second (disk read operations)
  # -w/s: Writes per second (disk write operations)
  # -rkB/s: Kilobytes read per second (read throughput)
  # -wkB/s: Kilobytes written per second (write throughput)
  # -await: Average I/O wait time (performance indicator)
  # -util: Device utilization percentage (capacity usage)
  # -svctm: System, user, CPU, I/O wait times (detailed timing)

  # Advanced I/O Monitoring (Enterprise)
  iostat -xz 5    # Extended stats, omit zeros, 5-second interval
  iostat -y       # Display in human-readable format
  iostat -t sda   # Show timestamp with statistics
  iostat -j       # JSON output format (scripting integration)
  iostat -h       # Device utilization with human-readable output
  iostat -N 1     # Show first N devices only

  # Enterprise Performance Indicators (Enterprise)
  # - %util < 20%: Underutilized disk (consider consolidation)
  # - %util 20-80%: Healthy utilization (optimal range)
  # - %util > 80%: Overutilized disk (performance bottleneck)
  # - await < 5ms: Excellent I/O performance
  # - await 5-20ms: Good I/O performance
  # - await > 20ms: Poor I/O performance (investigate needed)

  # Enterprise Troubleshooting Scenarios (Enterprise)
  # High await + low %util: I/O bottleneck (investigate disk health)
  # High %util + low await: Disk capacity issue (consider cleanup)
  # High reads/writes: Application I/O intensive (optimize queries)
  # Fluctuating performance: Check for competing processes
  # Consistent high await: Storage hardware issues (plan replacement)

  # Enterprise Best Practices (Enterprise)
  # - Monitor I/O during peak business hours for capacity planning
  # - Use specific device monitoring for critical storage volumes
  # - Combine with other metrics (CPU, memory) for complete analysis
  # - Set up automated alerts for performance threshold breaches
  # - Document baseline performance for comparison
  # - Use historical data for trend analysis and capacity planning
  # - Implement proper logging and alerting for I/O issues

  # Enterprise Integration and Automation (Enterprise)
  # - Integrate with monitoring systems (Prometheus, Grafana)
  # - Use node_exporter for I/O metrics collection
  # - Implement automated reporting for performance trends
  # - Use API integration for dashboard visualization
  # - Configure alert thresholds for proactive monitoring
  # - Use time-series databases for historical analysis
  # - Implement capacity planning based on I/O growth trends

  # Enterprise Security Considerations (Enterprise)
  # - Monitor I/O for unusual access patterns (security monitoring)
  # - Use proper permissions for I/O monitoring tools
  # - Implement audit trails for I/O performance data
  # - Secure I/O metrics transmission (encryption for remote monitoring)
  # - Use I/O monitoring for compliance reporting
  # - Document I/O monitoring procedures and access controls
  # - Regular security audits of I/O monitoring systems
  # - Use encrypted storage for historical I/O data
</code></pre>
</div>

<h3>ğŸ§  vmstat (Virtual Memory Statistics)</h3>

<div class="command-block">
  <h4>ğŸ’¾ Virtual memory stats - Enterprise Memory Analysis</h4>
  <pre><code>
  # Enterprise Virtual Memory Analysis (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Basic Memory Monitoring (Enterprise)
  vmstat               # Basic report (current memory statistics)
  vmstat 1 10          # Every 1 second, 10 times (continuous monitoring)
  vmstat 5 5           # Every 5 seconds, 5 times (periodic sampling)
  vmstat -w              # Wide output format (more columns)
  vmstat -t              # Timestamp output (time-series data)

  # Enterprise Memory Analysis Parameters (Enterprise)
  # procs: Number of processes in different states (r=running, b=blocked)
  # memory: Memory usage information (swpd, free, buff, cache)
  # swap: Swap space usage (si=swap in, so=swap out)
  # io: I/O statistics (bi=blocks in, bo=blocks out)
  # system: System activity (in=interrupts, cs=context switches)

  # Advanced Memory Monitoring (Enterprise)
  vmstat -a            # Active/inactive memory (detailed memory breakdown)
  vmstat -s            # Memory statistics summary (historical averages)
  vmstat -d            # Disk statistics (disk I/O information)
  vmstat -p /dev/sda1  # Partition statistics (specific device analysis)
  vmstat -f              # Full output format (all available statistics)
  vmstat -m 1           # Monitor memory every 1 minute (long-term trends)

  # Enterprise Memory Performance Indicators (Enterprise)
  # - procs r: Number of running processes (system load indicator)
  # - procs b: Blocked processes (resource contention)
  # - swpd: Swap used (memory pressure indicator)
  # - free: Available memory (system capacity)
  # - buff: Buffer memory (disk caching efficiency)
  # - cache: Page cache (memory efficiency)
  # - si/so: Swap activity (memory management efficiency)

  # Enterprise Memory Troubleshooting (Enterprise)
  # - High swpd + low free: Memory shortage (add RAM or optimize)
  # - High blocked processes: Resource contention (investigate bottlenecks)
  # - Low cache efficiency: Poor disk access patterns
  # - High context switches: System overhead (optimize applications)
  # - Frequent swapping: Memory pressure (tune swappiness)

  # Enterprise Memory Optimization (Enterprise)
  # - Monitor memory trends for capacity planning
  # - Use vmstat with other tools for complete analysis
  # - Implement memory alerts for threshold breaches
  # - Optimize application memory usage
  # - Tune kernel parameters (vm.swappiness, vm.vfs_cache_pressure)
  # - Use memory profiling tools for application optimization

  # Enterprise Best Practices (Enterprise)
  # - Monitor memory during peak business hours
  # - Establish baseline memory usage patterns
  # - Use memory monitoring with performance analysis
  # - Implement automated alerting for memory issues
  # - Document memory capacity and growth trends
  # - Use memory monitoring for capacity planning
  # - Combine with other metrics for system health assessment

  # Enterprise Security Considerations (Enterprise)
  # - Monitor memory for unusual patterns (security monitoring)
  # - Use memory monitoring for malware detection
  # - Implement proper access controls for monitoring tools
  # - Secure memory monitoring data transmission
  # - Use memory monitoring for compliance reporting
  # - Document memory monitoring procedures and policies
  # - Regular security audits of memory monitoring systems
  # - Use encrypted storage for historical memory data
</code></pre>
</div>
<h3>ğŸ” strace/ltrace (System Call Tracing)</h3>

<div class="command-block">
  <h4>ğŸ” strace (system calls) - Enterprise System Call Tracing</h4>
  <pre><code>
  # Enterprise System Call Tracing (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Basic System Call Tracing (Enterprise)
  strace command                     # Trace system calls (full trace)
  strace -p PID                      # Attach to running process (live monitoring)
  strace -c command                  # Count calls summary (performance analysis)
  strace -e open,read,write command  # Trace specific calls (targeted analysis)
  strace -o output.txt command       # Save to file (audit trail)
  strace -f command                  # Follow child processes (comprehensive tracing)
  strace -T command                  # Show time spent in calls (performance timing)

  # Advanced System Call Analysis (Enterprise)
  strace -s 1000 command             # Limit string size (output control)
  strace -v command                  # Verbose output (detailed information)
  strace -x command                  # Non-ASCII strings display (binary data)
  strace -ff command                 # Follow forks with output files (process tree)
  strace -d command                  # Debug output (development analysis)
  strace -u username command         # Run as specific user (security context)

  # Enterprise System Call Categories (Enterprise)
  # - File operations: open, read, write, close, stat, lstat
  # - Process operations: fork, execve, exit, wait, kill
  # - Network operations: socket, bind, connect, accept, send, recv
  # - Memory operations: mmap, brk, mprotect, madvise
  # - IPC operations: pipe, shmget, semget, msgget
  # - Signal operations: kill, signal, sigaction, sigprocmask

  # Enterprise Performance Analysis (Enterprise)
  strace -c -e trace=open,read,write command  # Count specific operations
  strace -T -c command             # Time analysis of system calls
  strace -f -o trace.log command  # Comprehensive process tracing
  strace -p PID -c              # Live process performance analysis
  strace -e trace=network command  # Network system call analysis
  strace -e trace=file command     # File system call analysis

  # Enterprise Debugging and Troubleshooting (Enterprise)
  strace -e trace=write command 2>&1 | grep -v "ENOENT"  # Find failed writes
  strace -p PID -e trace=connect,socket  # Network connection analysis
  strace -f -o debug.log command  # Debug child process creation
  strace -e trace=all -c command  # Complete system call profile
  strace -p PID -T -e trace=stat  # File access timing analysis
  strace -e trace=execve command  # Program execution tracking

  # Enterprise Security Analysis (Enterprise)
  strace -e trace=open,access command  # File access security audit
  strace -e trace=connect,bind command  # Network security monitoring
  strace -e trace=setuid,setgid command  # Privilege escalation detection
  strace -u nobody command  # Run as unprivileged user (security testing)
  strace -e trace=write,unlink command  # File modification tracking
  strace -p PID -e trace=execve  # Process execution monitoring
  strace -f -o security.log command  # Comprehensive security audit

  # Enterprise Best Practices (Enterprise)
  # - Use strace for application performance optimization
  # - Monitor specific system calls for targeted analysis
  # - Combine strace with other monitoring tools
  # - Use strace output for security audit trails
  # - Implement proper signal handling for clean tracing
  # - Use appropriate filtering to reduce trace overhead
  # - Document strace procedures and findings
  # - Use strace in controlled environments for security

  # Enterprise Security Considerations (Enterprise)
  # - strace can expose sensitive data (passwords, keys)
  # - Use strace only on systems you own or have permission
  # - Secure strace output files (proper permissions)
  # - Use strace for security auditing and incident analysis
  # - Implement proper access controls for strace usage
  # - Document strace security procedures and policies
  # - Regular security audits of strace usage and data
  # - Use strace data for compliance reporting
  # - Consider privacy implications when tracing user processes
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“š ltrace (library calls) - Enterprise Library Call Tracing</h4>
  <pre><code>
  # Enterprise Library Call Tracing (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Basic Library Call Tracing (Enterprise)
  ltrace command                # Trace library calls (full trace)
  ltrace -p PID                 # Attach to running process (live monitoring)
  ltrace -c command             # Count calls summary (performance analysis)
  ltrace -o output.txt command  # Save to file (audit trail)
  ltrace -f command              # Follow child processes (comprehensive tracing)

  # Advanced Library Call Analysis (Enterprise)
  ltrace -s 1000 command       # Limit string size (output control)
  ltrace -v command              # Verbose output (detailed information)
  ltrace -x command              # Non-ASCII strings display (binary data)
  ltrace -ff command             # Follow forks with output files (process tree)
  ltrace -d command              # Debug output (development analysis)
  ltrace -u username command    # Run as specific user (security context)

  # Enterprise Library Call Categories (Enterprise)
  # - Memory functions: malloc, free, realloc, calloc
  # - String functions: strcpy, strcat, strcmp, strlen
  # - File functions: fopen, fread, fwrite, fclose
  # - Network functions: socket, connect, bind, send, recv
  # - Process functions: fork, exec, wait, kill
  # - Thread functions: pthread_create, pthread_join, mutex

  # Enterprise Performance Analysis (Enterprise)
  ltrace -c -e malloc command  # Count memory allocations
  ltrace -c -e fopen command   # Count file operations
  ltrace -c -e socket command  # Count network operations
  ltrace -T -c command       # Time analysis of library calls
  ltrace -f -o trace.log command  # Comprehensive process tracing
  ltrace -p PID -T -e malloc command  # Live memory allocation monitoring
  ltrace -c -e network command  # Network library call analysis
  ltrace -e trace=exec command  # Program execution tracking

  # Enterprise Debugging and Troubleshooting (Enterprise)
  ltrace -e trace=malloc command 2>&1 | grep -v "ENOENT"  # Find failed allocations
  ltrace -p PID -e trace=connect,socket command  # Network connection analysis
  ltrace -f -o debug.log command  # Debug child process creation
  ltrace -e trace=all -c command  # Complete library call profile
  ltrace -p PID -T -e trace=stat command  # File access timing analysis
  ltrace -e trace=execve command  # Process execution monitoring
  ltrace -f -o security.log command  # Comprehensive security audit

  # Enterprise Security Analysis (Enterprise)
  ltrace -e trace=open,access command  # File access security audit
  ltrace -e trace=connect,bind command  # Network security monitoring
  ltrace -e trace=setuid,setgid command  # Privilege escalation detection
  ltrace -e trace=system command  # System function security monitoring
  ltrace -u nobody command  # Run as unprivileged user (security testing)
  ltrace -e trace=write,unlink command  # File modification tracking
  ltrace -p PID -e trace=execve command  # Process execution security monitoring
  ltrace -f -o security.log command  # Comprehensive security audit

  # Enterprise Best Practices (Enterprise)
  # - Use ltrace for application performance optimization
  # - Monitor specific library calls for targeted analysis
  # - Use ltrace output for security audit trails
  # - Implement proper signal handling for clean tracing
  # - Use appropriate filtering to reduce trace overhead
  # - Document ltrace procedures and findings
  # - Use ltrace in controlled environments for security
  # - Combine ltrace with other monitoring tools
  # - Use ltrace for compatibility testing and debugging

  # Enterprise Security Considerations (Enterprise)
  # - ltrace can expose sensitive data (passwords, keys)
  # - Use ltrace only on systems you own or have permission
  # - Secure ltrace output files (proper permissions)
  # - Use ltrace for security auditing and incident analysis
  # - Implement proper access controls for ltrace usage
  # - Document ltrace security procedures and policies
  # - Regular security audits of ltrace usage and data
  # - Use ltrace data for compliance reporting
  # - Consider privacy implications when tracing user processes
</code></pre>
</div>

<h3>ğŸ“Š sar (System Activity Reporter)</h3>

<div class="command-block">
  <h4>âš¡ CPU usage</h4>
  <pre><code>
  sar          # CPU usage (default)
  sar -u 1 10  # CPU every 1 sec, 10 times
  sar -u ALL   # All CPU statistics
  sar -P ALL   # Per-CPU statistics
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ’¾ Memory usage</h4>
  <pre><code>
  sar -r       # Memory utilization
  sar -r 1 10  # Memory every 1 sec
  sar -R       # Memory statistics
  sar -S       # Swap space utilization
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ’¾ I/O and transfer rate</h4>
  <pre><code>
  sar -b       # I/O transfer rate
  sar -d       # Block device statistics
  sar -d -p    # Pretty names for devices
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸŒ Network statistics</h4>
  <pre><code>
  sar -n DEV   # Network interface statistics
  sar -n EDEV  # Network errors
  sar -n TCP   # TCP statistics
  sar -n SOCK  # Socket statistics
</code></pre>
</div>

<div class="command-block">
  <h4>âš–ï¸ Load average</h4>
  <pre><code>
  sar -q       # Queue length and load average
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“… Historical data</h4>
  <pre><code>
  sar -f /var/log/sa/sa01      # Read from specific file
  sar -s 10:00:00 -e 11:00:00  # Time range
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“Š Generate report</h4>
  <pre><code>
  sar -A       # All statistics
  sadf -d      # Database format
</code></pre>
</div>

<h3>âš¡ mpstat (Multiprocessor Statistics)</h3>

<div class="command-block">
  <h4>âš¡ CPU statistics</h4>
  <pre><code>
  mpstat              # All CPUs summary
  mpstat -P ALL       # Per-CPU statistics
  mpstat -P ALL 1 10  # Every 1 second, 10 times
  mpstat -P 0         # CPU 0 only
</code></pre>
</div>

<h3>ğŸ”„ pidstat (Process Statistics)</h3>

<div class="command-block">
  <h4>ğŸ”„ Process statistics</h4>
  <pre><code>
  pidstat           # Default statistics
  pidstat 1 10      # Every 1 second, 10 times
  pidstat -p PID    # Specific process
  pidstat -u        # CPU usage
  pidstat -r        # Memory usage
  pidstat -d        # I/O statistics
  pidstat -t        # Thread statistics
  pidstat -C httpd  # Processes matching name
</code></pre>
</div>

<h3>ğŸ’¾ iotop (I/O Monitor)</h3>

<div class="command-block">
  <h4>ğŸ’¾ I/O monitoring</h4>
  <pre><code>
  iotop          # Interactive I/O monitor
  iotop -o       # Only processes doing I/O
  iotop -P       # Show processes, not threads
  iotop -a       # Accumulated I/O
  iotop -b -n 3  # Batch mode, 3 iterations
</code></pre>
</div>

<h3>ğŸŒ nethogs (Network Monitor)</h3>

<div class="command-block">
  <h4>ğŸŒ Network bandwidth per process</h4>
  <pre><code>
  nethogs       # Monitor all interfaces
  nethogs eth0  # Specific interface
  nethogs -d 5  # Update every 5 seconds
</code></pre>
</div>

<h3>ğŸ“¡ iftop (Interface Monitor)</h3>

<div class="command-block">
  <h4>ğŸ“¡ Network interface monitoring</h4>
  <pre><code>
  iftop          # Default interface
  iftop -i eth0  # Specific interface
  iftop -n       # No hostname lookup
  iftop -P       # Show port numbers
  iftop -B       # Display bandwidth in bytes
</code></pre>
</div>

<h3>ğŸ“Š dstat (Versatile Resource Statistics)</h3>

<div class="command-block">
  <h4>ğŸ“Š Combined statistics</h4>
  <pre><code>
  dstat            # Default output
  dstat -c         # CPU stats only
  dstat -d         # Disk stats only
  dstat -n         # Network stats only
  dstat -m         # Memory stats only
  dstat -cdnm      # Combined stats
  dstat --top-cpu  # Show top CPU process
  dstat --top-mem  # Show top memory process
  dstat --top-io   # Show top I/O process
</code></pre>
</div>

<h3>ğŸ“ˆ nmon (Performance Monitor)</h3>

<div class="command-block">
  <h4>ğŸ“ˆ Interactive performance monitor</h4>
  <pre><code>
  nmon         # Start nmon
  # Press keys for different views:
  # c - CPU
  # m - Memory
  # d - Disk I/O
  # n - Network
  # t - Top processes
  # q - Quit
</code></pre>
</div>
<h3>ğŸ”§ perf (Linux Profiling)</h3>

<div class="command-block">
  <h4>ğŸ”§ CPU profiling</h4>
  <pre><code>
  perf top                        # Real-time CPU profiler
  perf top -p PID                 # Profile specific process
  perf record -a -g sleep 10      # Record system-wide for 10 seconds
  perf record -p PID -g sleep 10  # Record specific process
  perf report                     # Analyze recorded data
  perf report --stdio             # Text-based report
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“Š Event statistics</h4>
  <pre><code>
  perf stat command                                   # Run command and show stats
  perf stat -p PID sleep 10                           # Stats for running process
  perf list                                           # List available events
  perf stat -e cache-misses,cache-references command  # Specific events
</code></pre>
</div>

<h3>âš™ï¸ Tuning Commands</h3>

<div class="command-block">
  <h4>âš™ï¸ sysctl (Kernel parameters)</h4>
  <pre><code>
  sysctl -a                            # Show all parameters
  sysctl kernel.hostname               # Show specific parameter
  sysctl -w net.ipv4.ip_forward=1      # Set parameter temporarily
  sysctl -p                            # Load from /etc/sysctl.conf
  sysctl -p /etc/sysctl.d/custom.conf  # Load from specific file
</code></pre>
</div>

<div class="command-block">
  <h4>âš™ï¸ Common tuning parameters</h4>
  <pre><code>
  # /etc/sysctl.conf or /etc/sysctl.d/*.conf
  # net.ipv4.ip_forward = 1
  # net.ipv4.tcp_tw_reuse = 1
  # vm.swappiness = 10
  # fs.file-max = 65536
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“„ Transparent Huge Pages</h4>
  <pre><code>
  cat /sys/kernel/mm/transparent_hugepage/enabled           # Check status
  echo never > /sys/kernel/mm/transparent_hugepage/enabled  # Disable
</code></pre>
</div>

<div class="command-block">
  <h4>âš¡ CPU governor</h4>
  <pre><code>
  cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor                 # Check
  echo performance > /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor  # Set
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ’¾ I/O scheduler</h4>
  <pre><code>
  cat /sys/block/sda/queue/scheduler              # Check scheduler
  echo deadline > /sys/block/sda/queue/scheduler  # Set scheduler
</code></pre>
</div>

<h2 id="backup-monitoring-security">ğŸ’¾ 47. BACKUP, RECOVERY, MONITORING & SECURITY</h2>
<h3>ğŸ’¿ Backup & Recovery (Security Perspective)</h3>
<div class="command-block">
  <h4>ğŸ“š Understanding Backup Strategies</h4>
  <pre><code>
  # The 3-2-1 Backup Rule:
  # 3 - Keep 3 copies of data
  # 2 - Store on 2 different media types
  # 1 - Keep 1 copy offsite (cloud, remote datacenter)

  # Backup Types:
  #
  # Full Backup:
  # - Complete copy of all data
  # - Pros: Simple restore, single point recovery
  # - Cons: Slow, storage-intensive, time-consuming
  # - When: Weekly or monthly
  #
  # Incremental Backup:
  # - Only changed files since last backup (any type)
  # - Pros: Fast, minimal storage
  # - Cons: Restore requires all incremental backups
  # - When: Daily
  #
  # Differential Backup:
  # - Changed files since last FULL backup
  # - Pros: Faster restore than incremental
  # - Cons: Grows larger as week progresses
  # - When: Daily between full backups

  # Backup Strategy Example:
  # Sunday - Full backup
  # Monday-Saturday - Incremental backups
  #
  # Restore on Friday needs:
  # - Sunday full backup
  # - Monday, Tuesday, Wednesday, Thursday, Friday incrementals

  # RPO vs RTO:
  # RPO (Recovery Point Objective):
  # - Maximum acceptable data loss
  # - Example: RPO = 1 hour means can lose up to 1 hour of data
  # - Determines backup frequency
  #
  # RTO (Recovery Time Objective):
  # - Maximum acceptable downtime
  # - Example: RTO = 4 hours means must restore within 4 hours
  # - Determines backup/restore method

  # Backup Tools Comparison:
  #
  # tar:
  # - Simple, universal
  # - No compression by default (use tar + gzip)
  # - Good for: Full backups, archives
  #
  # rsync:
  # - Incremental by default (only transfers changes)
  # - Network-efficient (delta transfer)
  # - Good for: Sync directories, remote backups
  #
  # dd:
  # - Bit-by-bit disk copy
  # - Creates exact disk image
  # - Good for: Disk cloning, disaster recovery
  #
  # dump/restore:
  # - Filesystem-aware (ext2/3/4)
  # - Supports incremental levels (0-9)
  # - Good for: Unix/Linux filesystems
  #
  # Database-specific:
  # - mysqldump, pg_dump (logical backups)
  # - Filesystem snapshots (physical backups)
  # - Good for: Database consistency

  # Backup Verification:
  # CRITICAL: Always verify backups can be restored
  #
  # 1. Test restores regularly (monthly)
  # 2. Check backup logs for errors
  # 3. Verify checksums (sha256sum)
  # 4. Monitor backup completion
  # 5. Document restore procedures

  # Backup Security:
  #
  # Encryption at Rest:
  # - Encrypt backup files (gpg, openssl)
  # - Prevents unauthorized access to backup media
  #
  # Encryption in Transit:
  # - Use SSH/TLS for remote backups
  # - Prevents interception during transfer
  #
  # Access Control:
  # - Restrict backup file permissions (600)
  # - Separate backup admin accounts
  # - Audit backup access logs

  # Backup Rotation (GFS):
  # Grandfather-Father-Son
  #
  # Daily (Son) - Last 7 days
  # Weekly (Father) - Last 4 weeks
  # Monthly (Grandfather) - Last 12 months
  #
  # Balances: Storage costs vs retention requirements

  # Cloud Backup Considerations:
  # Pros:
  # - Offsite automatically
  # - No hardware maintenance
  # - Scalable storage
  #
  # Cons:
  # - Ongoing costs
  # - Internet dependency
  # - Data sovereignty concerns
  #
  # Tools: rclone, aws s3, duplicity, restic
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“¦ Basic Backup Commands</h4>
  <pre><code>
  tar -cvf backup.tar /path/to/data                   # Create backup archive
  tar -xvf backup.tar                                 # Extract backup archive
  rsync -av /source/ /destination/                    # Sync directories
  rsync -av --delete /source/ /destination/           # Mirror directories
  rsync -av --ignore-existing /source/ /destination/  # Sync Without overwriting existing files
  dd if=/dev/sda of=/mnt/backup/sda.img bs=4M         # Disk-level backup
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ” Encrypted Backups</h4>
  <pre><code>
  gpg -c backup.tar                                                              # Encrypt backup with passphrase
  gpg --output backup.tar.gpg --encrypt --recipient user@example.com backup.tar  # Asymmetric encryption
  openssl enc -aes-256-cbc -salt -in backup.tar -out backup.enc                  # OpenSSL encryption
  openssl enc -d -aes-256-cbc -in backup.enc -out backup.tar                     # Decrypt backup
</code></pre>
</div>

<div class="command-block">
  <h4>âœ… Backup Verification</h4>
  <pre><code>
  tar -tvf backup.tar                         # List archive contents
  sha256sum backup.tar                        # Generate checksum
  sha256sum -c backup.tar.sha256              # Verify checksum
  gpg --verify backup.tar.gpg                 # Verify GPG encrypted backup
  rsync --dry-run -av /source/ /destination/  # Test sync without actual copy
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸŒ Offsite / Remote Backups</h4>
  <pre><code>
  scp backup.tar user@remote:/path/to/backup      # Copy backup to remote server
  rsync -av /local/dir/ user@remote:/backup/      # Sync to remote host
  rclone copy /local/dir remote:backup            # Cloud storage backup
  aws s3 cp backup.tar s3://bucket-name/          # Upload backup to S3
  aws s3 sync /local/dir s3://bucket-name/        # Sync to S3 bucket
  duplicity /local/dir scp://user@remote//backup  # Encrypted incremental backups
</code></pre>
</div>
<div class="command-block">
  <h4>ğŸ“¦ Rclone Configuration</h4>
  <pre><code>
  rclone config                     # Configure cloud remotes
  rclone listremotes                # Show configured remotes
  rclone sync /data remote:backup   # Sync to cloud
  rclone copy /data remote:backup   # Copy without deletion
  rclone check /data remote:backup  # Verify integrity
</code></pre>
</div>

<h3>ğŸ“Š Monitoring & Security</h3>

<div class="command-block">
  <h4>ğŸ“Š Prometheus Monitoring</h4>
  <pre><code>
  # Node Exporter
  ./node_exporter                          # Start node exporter on target host
  # Prometheus server
  prometheus --config.file=prometheus.yml  # Start Prometheus
  # Alert rules
  promtool check rules alert_rules.yml     # Validate alert rules
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“Š Grafana Visualization</h4>
  <pre><code>
  grafana-server                                  # Start Grafana
  # Access Grafana web UI: http://server_ip:3000
  grafana-cli plugins install plugin_name         # Install plugins
  grafana-cli admin reset-admin-password newpass  # Reset Grafana admin password
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“š ELK Stack (Log Monitoring)</h4>
  <pre><code>
  # Elasticsearch
  systemctl start elasticsearch
  # Logstash
  logstash -f logstash.conf
  # Kibana
  systemctl start kibana
  # Beats
  filebeat -e -c filebeat.yml    # Ship logs to ELK
  auditbeat -e -c auditbeat.yml  # Monitor audit events
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ›¡ï¸ Security Hardening</h4>
  <pre><code>
  # Fail2Ban
  systemctl start fail2ban
  fail2ban-client status                                                 # Check status
  fail2ban-client status sshd                                            # Check jail status
  # SELinux
  sestatus                                                               # Show SELinux status
  setenforce 1                                                           # Enforce mode
  setenforce 0                                                           # Permissive mode
  getsebool -a                                                           # List booleans
  # OpenSSL
  openssl req -new -x509 -days 365 -nodes -out cert.pem -keyout key.pem  # Create self-signed certificate
  openssl verify cert.pem                                                # Validate certificate
  # GPG
  gpg --gen-key                                                          # Generate key
  gpg --encrypt --recipient user file                                    # Encrypt file
  gpg --decrypt file.gpg                                                 # Decrypt file
  # Vulnerability scanning
  nmap -sV target_host                                                   # Service version scan
  lynis audit system                                                     # Security audit
  ossec-logtest                                                          # Test OSSEC rules
</code></pre>
</div>

<div class="command-block">
  <h4>âš¡ Performance Monitoring & Tuning</h4>
  <pre><code>
  iostat -xz 5                                 # CPU, disk I/O statistics
  # -x : extended stats
  # -z : omit zero values
  #  5 : interval in seconds
  vmstat 5                                     # CPU, memory, swap
  sar -u 5 3                                   # CPU usage history
  sar -r 5 3                                   # Memory usage history
  sar -n DEV 5 3                               # Network stats
  # Kernel tuning
  sysctl -a                                    # List all kernel parameters
  sysctl -w net.ipv4.ip_forward=1              # Enable IP forwarding
  echo "vm.swappiness=10" >> /etc/sysctl.conf  # Persistent swappiness
  sysctl -p                                    # Apply kernel changes
</code></pre>
</div>
<div class="command-block">
  <h4>âš™ï¸ Process & File Limits</h4>
  <pre><code>
  ulimit -a        # View current limits
  ulimit -n 65535  # Increase open files (session)
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“ Persistent limits</h4>
  <pre><code>
  /etc/security/limits.conf
  * soft nofile 65535
  * hard nofile 65535
</code></pre>
</div>
<div class="command-block">
  <h4>ğŸ¥ System Health One-Liners</h4>
  <pre><code>
  uptime               # Shows system running time, load average (1, 5, 15 min) â€“ quick health check
  df -h                # Displays disk usage in human-readable format
  free -h              # Displays RAM and swap usage in human-readable format (used, free, available)
  df -hT               # Shows disk usage by filesystem type (ext4, xfs, nfs) â€“ capacity & mount validation
  top -b -n1 | head    # Non-interactive snapshot of top CPU/memory-consuming processes (automation-friendly)
  ss -tulnp            # Lists listening TCP/UDP ports with associated services and PIDs (network security check)
  journalctl -p 3 -xb  # Shows high-priority (error-level) systemd logs from current boot for troubleshooting
</code></pre>
</div>
</div>
<h2 id="ha-load-balancing">âš–ï¸ 48. HIGH AVAILABILITY & LOAD BALANCING (HAProxy, Nginx LB, Pacemaker/Corosync clusters)</h2>
<div class="command-block">
  <h4>ğŸ“š Understanding High Availability Concepts</h4>
  <pre><code>
  # High Availability (HA):
  # - System continues operating despite component failures
  # - Minimizes downtime and data loss
  # - Measured in "nines" of uptime

  # Availability Levels:
  # 99%       = 3.65 days downtime/year
  # 99.9%     = 8.77 hours downtime/year
  # 99.99%    = 52.6 minutes downtime/year
  # 99.999%   = 5.26 minutes downtime/year (five nines)
  # 99.9999%  = 31.5 seconds downtime/year (six nines)

  # HA Architecture Components:
  #
  # Load Balancer:
  # - Distributes traffic across multiple servers
  # - Health checks to detect failures
  # - Examples: HAProxy, Nginx, AWS ELB
  #
  # Cluster:
  # - Group of servers working together
  # - Shared resources (IP, storage)
  # - Automatic failover
  # - Examples: Pacemaker, Corosync, Keepalived
  #
  # Redundancy:
  # - Multiple copies of critical components
  # - Active-Active: All nodes handle traffic
  # - Active-Passive: Standby nodes wait for failure

  # Load Balancing Algorithms:
  #
  # Round Robin:
  # - Requests distributed evenly (server1, server2, server3, repeat)
  # - Simple, fair distribution
  # - Ignores server load
  #
  # Least Connections:
  # - Send to server with fewest active connections
  # - Better for long-lived connections
  # - Adapts to varying server load
  #
  # IP Hash:
  # - Client IP determines server
  # - Session persistence (sticky sessions)
  # - Same client always goes to same server
  #
  # Weighted:
  # - Servers assigned weights based on capacity
  # - More powerful servers get more traffic

  # HAProxy vs Nginx Load Balancing:
  #
  # HAProxy:
  # - Dedicated load balancer (L4 + L7)
  # - Advanced health checks
  # - SSL termination
  # - Better for complex routing rules
  #
  # Nginx:
  # - Web server + load balancer
  # - Simpler configuration
  # - Better for HTTP/HTTPS
  # - Can serve static content + load balance

  # Pacemaker/Corosync Cluster:
  #
  # Pacemaker:
  # - Cluster Resource Manager
  # - Manages services (resources)
  # - Decides where resources run
  # - Handles failover
  #
  # Corosync:
  # - Cluster Communication System
  # - Heartbeat between nodes
  # - Detects node failures
  # - Quorum management
  #
  # Resources:
  # - Services managed by cluster (IP, database, web server)
  # - Can be moved between nodes
  # - Start/stop managed automatically

  # Split-Brain Problem:
  # - Cluster nodes can't communicate
  # - Each thinks others are down
  # - Both try to become active (data corruption risk)
  #
  # Solutions:
  # - Quorum: Require majority of nodes to operate
  # - Fencing: Shut down suspected failed nodes
  # - STONITH: "Shoot The Other Node In The Head"

  # Virtual IP (VIP):
  # - Floating IP address
  # - Moves between cluster nodes
  # - Clients always connect to same IP
  # - Automatic failover transparent to clients

  # Health Checks:
  # - TCP check: Port is open
  # - HTTP check: Returns 200 OK
  # - Custom script: Application-specific test
  # - Interval: How often to check (e.g., every 5 sec)
  # - Timeout: Max wait time for response
  # - Failures: How many failures before marked down

  # Session Persistence:
  # - Sticky sessions (same client â†’ same server)
  # - Needed for stateful applications
  # - Methods:
  #   - Cookie-based (load balancer inserts cookie)
  #   - IP-based (hash client IP)
  #   - Session sharing (external session store like Redis)

  # HA Best Practices:
  # 1. No single point of failure
  # 2. Automate failover (don't rely on manual intervention)
  # 3. Test failover regularly
  # 4. Monitor all components
  # 5. Plan for split-brain scenarios
  # 6. Document recovery procedures
  # 7. Use automation (Ansible, Terraform)
</code></pre>
</div>

<h3>âš–ï¸ HAProxy Load Balancer</h3>
<div class="command-block">
  <h4>âš™ï¸ Basic Commands</h4>
  <pre><code>
  systemctl start haproxy
  systemctl enable haproxy
  haproxy -c -f /etc/haproxy/haproxy.cfg  # Check config
  haproxy -f /etc/haproxy/haproxy.cfg -d  # Start in debug mode
</code></pre>
</div>

<h3>ğŸŒ Nginx Load Balancing</h3>
<div class="command-block">
  <h4>âš™ï¸ Configuration Example</h4>
  <pre><code>
  upstream backend {
  server web1.example.com;
  server web2.example.com;
  }

  server {
  listen 80;
  location / {
  proxy_pass http://backend;
  }
  }
  systemctl restart nginx
</code></pre>
</div>

<h3>ğŸ—ï¸ Pacemaker / Corosync Clusters</h3>
<div class="command-block">
  <h4>ğŸ”§ Cluster Commands</h4>
  <pre><code>
  pcs cluster setup --name mycluster node1 node2
  pcs cluster start --all
  pcs status   # Check cluster status
  pcs resource create VirtualIP ocf:heartbeat:IPaddr ip=192.168.1.100 cidr_netmask=24
  pcs resource enable VirtualIP
</code></pre>
</div>
<h3>ğŸ›¡ï¸ Keepalived (VRRP for IP Failover)</h3>

<div class="command-block">
  <h4>âš™ï¸ Keepalived service</h4>
  <pre><code>
  systemctl start keepalived
  systemctl enable keepalived
  systemctl status keepalived
  systemctl restart keepalived
</code></pre>
</div>

<div class="command-block">
  <h4>âš™ï¸ Configuration</h4>
  <pre><code>
  /etc/keepalived/keepalived.conf                   # Main configuration file
  # Test configuration
  keepalived -t -f /etc/keepalived/keepalived.conf  # Check config syntax
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ‘ï¸ Monitor VRRP status</h4>
  <pre><code>
  ip addr show                 # Check if VIP is assigned
  journalctl -u keepalived -f  # Follow keepalived logs
  tcpdump -i eth0 vrrp         # Monitor VRRP packets
</code></pre>
</div>

<h2 id="advanced-kubernetes">ğŸ­ 49. ADVANCED KUBERNETES PRODUCTION PATTERNS</h2>

<div class="command-block">
  <h4>ğŸ­ Production Cluster Setup</h4>
  <pre><code>
  # Multi-master cluster setup
  kubeadm init --pod-network-cidr=10.244.0.0/16 --control-plane-endpoint="load-balancer-ip:6443"
  kubeadm join load-balancer-ip:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>

  # High availability etcd cluster
  etcdctl member list
  etcdctl endpoint health
  etcdctl snapshot save backup.db
  etcdctl snapshot status backup.db

  # Cluster backup and restore
  kubeadm config upload from-flags --pod-network-cidr=10.244.0.0/16
  kubectl get configmap kubeadm-config -n kube-system -o yaml
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸŒ Advanced Networking</h4>
  <pre><code>
  # Network policies enforcement
  kubectl apply -f network-policy.yaml
  kubectl get networkpolicies
  kubectl describe networkpolicy <policy-name>

  # Service mesh with Istio
  istioctl install --set profile=demo
  kubectl label namespace default istio-injection=enabled
  kubectl apply -f <app>.yaml
  istioctl proxy-status

  # Ingress controllers
  kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml
  kubectl get ingress
  kubectl describe ingress <ingress-name>
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ’¾ Storage Management</h4>
  <pre><code>
  # Persistent volume claims
  kubectl get pvc
  kubectl describe pvc <pvc-name>
  kubectl get pv

  # Storage classes
  kubectl get storageclass
  kubectl describe storageclass <class-name>

  # CSI drivers
  kubectl get csidrivers
  kubectl get csinodes
  kubectl get csistoragecapacities
</code></pre>
</div>

<h2 id="kubernetes-security">ğŸ” 50. KUBERNETES SECURITY & RBAC</h2>

<div class="command-block">
  <h4>ğŸ” RBAC Configuration</h4>
  <pre><code>
  # Create service account
  kubectl create serviceaccount <sa-name>
  kubectl get serviceaccount <sa-name>

  # Create role and role binding
  kubectl create role <role-name> --verb=get,list,watch --resource=pods
  kubectl create rolebinding <binding-name> --role=<role-name> --user=<username>

  # Cluster roles
  kubectl create clusterrole <cluster-role-name> --verb=* --resource=*
  kubectl create clusterrolebinding <binding-name> --clusterrole=<cluster-role-name> --serviceaccount=default:<sa-name>

  # Check permissions
  kubectl auth can-i get pods --as=<username>
  kubectl auth can-i create deployments --as=system:serviceaccount:<namespace>:<sa-name>
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ›¡ï¸ Pod Security Policies</h4>
  <pre><code>
  # Security contexts
  kubectl get podsecuritypolicies
  kubectl describe podsecuritypolicy <policy-name>

  # Admission controllers
  kubectl get validatingwebhookconfigurations
  kubectl get mutatingwebhookconfigurations

  # Network policies
  kubectl get networkpolicy
  kubectl describe networkpolicy <policy-name>
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ” Security Scanning</h4>
  <pre><code>
  # Image scanning with Trivy
  trivy image <image-name>
  trivy image --severity HIGH,CRITICAL <image-name>

  # Falco security monitoring
  falco -o /etc/falco/falco_rules.local.yaml
  kubectl logs -n falco -l app=falco

  # Kubernetes security audit
  kubectl get events --field-selector type=Warning
  kubectl audit-proxy
</code></pre>
</div>

<h2 id="kubernetes-monitoring">ğŸ“Š 51. KUBERNETES MONITORING & OBSERVABILITY</h2>

<div class="command-block">
  <h4>ğŸ“Š Prometheus Stack</h4>
  <pre><code>
  # Install Prometheus Operator
  kubectl create namespace monitoring
  helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  helm install prometheus prometheus-community/kube-prometheus-stack -n monitoring

  # Check monitoring components
  kubectl get pods -n monitoring
  kubectl get svc -n monitoring
  kubectl get prometheus -n monitoring

  # Custom metrics
  kubectl get servicemonitors
  kubectl describe servicemonitor <monitor-name>
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ”— Distributed Tracing</h4>
  <pre><code>
  # Jaeger installation
  kubectl create namespace observability
  kubectl apply -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.41.0/jaeger-operator.yaml
  kubectl apply -f - <<EOF
  apiVersion: jaegertracing.io/v1
  kind: Jaeger
  metadata:
  name: simplest
  EOF

  # OpenTelemetry
  kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/latest/download/opentelemetry-operator.yaml
  kubectl get otel-collector
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“ Logging and Analysis</h4>
  <pre><code>
  # Fluent Bit/Fluentd
  helm repo add fluent https://fluent.github.io/helm-charts
  helm install fluent-bit fluent/fluent-bit -n logging

  # Elasticsearch and Kibana
  helm repo add elastic https://helm.elastic.co
  helm install elasticsearch elastic/elasticsearch -n logging
  helm install kibana elastic/kibana -n logging

  # Log aggregation
  kubectl logs -f -n logging deployment/fluent-bit
  kubectl get pods -n logging
</code></pre>
</div>

<h2 id="enterprise-cicd">ğŸ—ï¸ 52. ENTERPRISE CI/CD PIPELINES</h2>

<div class="command-block">
  <h4>ğŸ—ï¸ Jenkins Enterprise Setup</h4>
  <pre><code>
  # Jenkins master-slave configuration
  docker run -d -p 8080:8080 -p 50000:50000 --name jenkins-master jenkins/jenkins:lts
  docker exec -it jenkins-master bash
  # Configure slaves via Jenkins UI or CLI

  # Pipeline as Code
  pipeline {
  agent any
  stages {
  stage('Build') {
  steps {
  sh 'mvn clean package'
  }
  }
  stage('Test') {
  steps {
  sh 'mvn test'
  }
  }
  stage('Deploy') {
  steps {
  sh 'kubectl apply -f k8s/'
  }
  }
  }
  }

  # Blue-green deployment
  stage('Blue-Green Deploy') {
  steps {
  script {
  def active = sh(script: 'kubectl get service app -o jsonpath="{.spec.selector.color}"', returnStdout: true).trim()
  def newColor = active == 'blue' ? 'green' : 'blue'
  sh "kubectl set image deployment/app app=myapp:${BUILD_NUMBER} --record"
  sh "kubectl patch service app -p '{\"spec\":{\"selector\":{\"color\":\"${newColor}\"}}}'"
  }
  }
  }
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸš€ GitOps with ArgoCD</h4>
  <pre><code>
  # Install ArgoCD
  kubectl create namespace argocd
  kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

  # Deploy application
  argocd app create myapp --repo https://github.com/user/myapp.git --path k8s --dest-server https://kubernetes.default.svc --dest-namespace default
  argocd app sync myapp
  argocd app get myapp

  # Sync strategies
  argocd app set myapp --sync-policy automated --auto-prune
  argocd app set myapp --sync-option PrunePropagationPolicy=foreground
  argocd app set myapp --sync-option RespectIgnoreDifferences=true
</code></pre>
</div>

<div class="command-block">
  <h4>âš¡ Advanced Pipeline Features</h4>
  <pre><code>
  # Multi-branch pipelines
  # Jenkinsfile with branch-specific logic
  if (env.BRANCH_NAME == 'main') {
  // Production deployment
  } else if (env.BRANCH_NAME.startsWith('feature/')) {
  // Feature branch testing
  }

  # Container-based builds
  podTemplate {
  node('pod') {
  container('maven') {
  sh 'mvn clean package'
  }
  container('docker') {
  sh 'docker build -t myapp:${BUILD_NUMBER} .'
  }
  }
  }

  # Parallel testing
  parallel(
  'unit-tests': { sh 'mvn test' },
  'integration-tests': { sh 'mvn verify -P integration' },
  'security-scan': { sh 'trivy fs .' }
  )
</code></pre>
</div>

<h2 id="gitops-advanced">ğŸŒ€ 53. ADVANCED GITOPS & AUTOMATION</h2>

<div class="command-block">
  <h4>ğŸŒ€ Flux CD GitOps</h4>
  <pre><code>
  # Install Flux
  flux bootstrap github --owner=<username> --repository=<repo> --branch=main --path=./clusters/my-cluster

  # Create GitRepository
  flux create source git myapp --url=https://github.com/user/myapp.git --branch=main --interval=5m

  # Create Kustomization
  flux create kustomization myapp --source=myapp --path=./k8s --prune=true --interval=10m

  # Check reconciliation
  flux get kustomizations
  flux get sources git
  flux reconcile kustomization myapp --with-source
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ—ï¸ Infrastructure as Code</h4>
  <pre><code>
  # Crossplane for cloud resources
  kubectl apply -f https://github.com/crossplane/crossplane/releases/latest/download/crossplane.yaml
  kubectl apply -f - <<EOF
  apiVersion: pkg.crossplane.io/v1
  kind: Provider
  metadata:
  name: provider-aws
  spec:
  package: crossplane/provider-aws:v0.32.0
  EOF

  # Create cloud resources via Kubernetes
  kubectl apply -f - <<EOF
  apiVersion: s3.aws.crossplane.io/v1beta1
  kind: Bucket
  metadata:
  name: my-bucket
  spec:
  forProvider:
  region: us-east-1
  providerConfigRef:
  name: aws-provider
  EOF
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“‹ Policy as Code</h4>
  <pre><code>
  # Open Policy Agent (OPA)
  kubectl apply -f https://github.com/open-policy-agent/gatekeeper/releases/latest/download/gatekeeper.yaml

  # Create constraint template
  kubectl apply -f - <<EOF
  apiVersion: templates.gatekeeper.sh/v1beta1
  kind: ConstraintTemplate
  metadata:
  name: k8srequiredlabels
  spec:
  crd:
  spec:
  names:
  kind: K8sRequiredLabels
  targets:
  - target: admission.k8s.gatekeeper.sh
  rego: |
  package k8srequiredlabels
  violation[{"msg": msg}] {
  required := input.parameters.labels
  provided := input.review.object.metadata.labels
  missing := {x | x := required[_] not in provided}
  count(missing) > 0
  msg := sprintf("missing required labels: %v", [missing])
  }
  EOF
</code></pre>
</div>

<h2 id="cloud-architecture">â˜ï¸ 54. CLOUD ARCHITECTURE PATTERNS</h2>

<div class="command-block">
  <h4>â˜ï¸ Multi-Cloud Strategy</h4>
  <pre><code>
  # AWS CLI advanced operations
  aws ec2 describe-instances --filters Name=tag:Environment,Values=Production
  aws s3 ls --recursive s3://my-bucket/
  aws cloudformation deploy --template-file template.yaml --stack-name my-stack --capabilities CAPABILITY_IAM

  # Azure CLI
  az vm list --resource-group my-rg
  az storage account list --output table
  az group deployment create --resource-group my-rg --template-file template.json

  # GCP CLI
  gcloud compute instances list --filter="labels.env=prod"
  gcloud storage ls gs://my-bucket/
  gcloud deployment-manager deployments create my-deployment --template=template.jinja
</code></pre>
</div>

<div class="command-block">
  <h4>âš¡ Serverless Architecture</h4>
  <pre><code>
  # AWS Lambda
  aws lambda create-function --function-name my-function --runtime python3.9 --handler lambda_function.lambda_handler --zip-file fileb://function.zip
  aws lambda invoke --function-name my-function output.txt

  # Azure Functions
  az functionapp create --resource-group my-rg --consumption-plan-location eastus --runtime python --functions-version 4 --name my-function-app

  # Google Cloud Functions
  gcloud functions deploy my-function --runtime python39 --trigger-http --allow-unauthenticated
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ³ Container Orchestration in Cloud</h4>
  <pre><code>
  # EKS (Amazon EKS)
  aws eks create-cluster --name my-cluster --role-arn arn:aws:iam::account:role/EKSServiceRole --resources-vpc-config subnetIds=subnet-123,subnet-456
  aws eks update-kubeconfig --name my-cluster
  kubectl get nodes

  # AKS (Azure Kubernetes Service)
  az aks create --resource-group my-rg --name my-cluster --node-count 3 --enable-addons monitoring
  az aks get-credentials --resource-group my-rg --name my-cluster

  # GKE (Google Kubernetes Engine)
  gcloud container clusters create my-cluster --num-nodes=3 --zone us-central1-a
  gcloud container clusters get-credentials my-cluster --zone us-central1-a
</code></pre>
</div>

<h2 id="cost-optimization">ğŸ’° 55. COST OPTIMIZATION STRATEGIES</h2>

<div class="command-block">
  <h4>âš¡ Resource Optimization</h4>
  <pre><code>
  # AWS Cost Explorer
  aws ce get-cost-and-usage --time-period Start=2023-01-01,End=2023-01-31 --granularity MONTHLY --metrics BlendedCost
  aws ce get-dimension-values --time-period Start=2023-01-01,End=2023-01-31 --dimension SERVICE

  # Azure Cost Management
  az consumption usage list --start-date 2023-01-01 --end-date 2023-01-31
  az billing account list

  # GCP Cost Analysis
  gcloud billing accounts list
  gcloud logging read 'resource.type="gce_instance" AND timestamp>="2023-01-01T00:00:00Z"' --limit 1000
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“ˆ Auto-scaling Optimization</h4>
  <pre><code>
  # Kubernetes HPA
  kubectl autoscale deployment myapp --cpu-percent=50 --min=1 --max=10
  kubectl get hpa
  kubectl describe hpa myapp

  # AWS Auto Scaling
  aws autoscaling create-auto-scaling-group --auto-scaling-group-name my-asg --launch-configuration-name my-lc --min-size 1 --max-size 10 --desired-capacity 3

  # Azure Scale Sets
  az vmss create --resource-group my-rg --name my-vmss --image UbuntuLTS --upgrade-policy-mode Automatic --instance-count 3
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ’¾ Storage Optimization</h4>
  <pre><code>
  # AWS S3 lifecycle policies
  aws s3api put-bucket-lifecycle-configuration --bucket my-bucket --lifecycle-configuration file://lifecycle.json

  # Azure Storage tiering
  az storage blob set-tier --container-name my-container --name my-blob --tier Cool

  # GCP Storage classes
  gsutil lifecycle set lifecycle.json gs://my-bucket/
  gsutil defstorageclass set NEARLINE gs://my-bucket/*
</code></pre>
</div>

<h2 id="devsecops">ğŸ›¡ï¸ 56. DEVSECOPS PRACTICES</h2>

<div class="command-block">
  <h4>ğŸ” Security Scanning in CI/CD</h4>
  <pre><code>
  # SAST (Static Application Security Testing)
  sonar-scanner -Dsonar.projectKey=myproject -Dsonar.sources=src/
  bandit -r src/
  semgrep --config=auto src/

  # DAST (Dynamic Application Security Testing)
  zap-baseline.py -t http://myapp.example.com
  nuclei -u http://myapp.example.com

  # Container security scanning
  trivy image --severity HIGH,CRITICAL myapp:latest
  clairctl scan myapp:latest
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ” Secrets Management</h4>
  <pre><code>
  # HashiCorp Vault
  vault server -dev
  vault auth enable approle
  vault write auth/approle/role/myapp token_policies="myapp-policy"
  vault read auth/approle/role/myapp/role-id

  # Kubernetes secrets
  kubectl create secret generic mysecret --from-literal=password=mypassword
  kubectl get secret mysecret -o yaml
  kubectl apply -f sealed-secret.yaml

  # AWS Secrets Manager
  aws secretsmanager create-secret --name myapp-secret --secret-string '{"username":"admin","password":"mypassword"}'
  aws secretsmanager get-secret-value --secret-id myapp-secret
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“‹ Compliance Automation</h4>
  <pre><code>
  # OpenSCAP
  oscap xccdf eval --profile xccdf_org.ssgproject.content_profile_stig-rhel7-server-upstream --report report.html /usr/share/xml/scap/ssg/content/ssg-rhel7-xccdf.xml

  # CIS Benchmarks
  kube-bench run --config-dir /etc/kube-bench/cfg --benchmark cis-1.6

  # AWS Config
  aws configservice put-config-rule --config-rule file://config-rule.json
  aws configservice get-compliance-details-by-config-rule --config-rule-name my-rule
</code></pre>
</div>

<h2 id="security-automation">ğŸ¤– 57. SECURITY AUTOMATION & COMPLIANCE</h2>

<div class="command-block">
  <h4>ğŸ¤– Automated Security Testing</h4>
  <pre><code>
  # OWASP ZAP automation
  docker run -t owasp/zap2docker-stable zap-baseline.py -t http://myapp.example.com -J report.json

  # Security in pipeline
  stage('Security Scan') {
  parallel(
  'SAST': { sh 'sonar-scanner' },
  'DAST': { sh 'zap-baseline.py -t http://myapp.example.com' },
  'Dependency Scan': { sh 'safety check' }
  )
  }

  # Infrastructure security
  tfsec ./terraform/
  checkov -d ./terraform/
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“Š Compliance Monitoring</h4>
  <pre><code>
  # AWS Config rules
  aws configservice put-config-rule --config-rule file://rule.json
  aws configservice start-config-rule-evaluation --config-rule-name my-rule

  # Azure Policy
  az policy assignment create --name 'my-policy' --policy 'my-policy-definition' --scope '/subscriptions/my-subscription'

  # Continuous compliance monitoring
  aws configservice get-compliance-summary-by-config-rule
  az policy state list
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸš¨ Incident Response Automation</h4>
  <pre><code>
  # AWS GuardDuty
  aws guardduty create-detector --enable
  aws guardduty create-ip-set --detector-id my-detector --ip-set-file ip-set.json

  # Azure Sentinel
  az sentinel alert-rule create --resource-group my-rg --workspace-name my-workspace --name my-rule --logic-app-resource-id /subscriptions/my-subscription/resourceGroups/my-rg/providers/Microsoft.Logic/workflows/my-workflow

  # Automated response
  aws lambda create-function --function-name incident-response --runtime python3.9 --handler incident_response.lambda_handler --zip-file fileb://function.zip
</code></pre>
</div>

<h2 id="interview-prep">ğŸ¯ 58. INTERVIEW PREPARATION - SYSTEM DESIGN</h2>

<div class="command-block">
  <h4>ğŸ—ï¸ Common System Design Questions</h4>
  <pre><code>
  # Design a URL Shortener
  # Components: Load Balancer, Web Servers, Cache, Database
  # Technologies: Nginx, Redis, PostgreSQL, Kubernetes

  # Design a Chat Application
  # Components: WebSocket Server, Message Queue, Database, Push Notifications
  # Technologies: Socket.io, RabbitMQ, MongoDB, Firebase

  # Design a Video Streaming Platform
  # Components: CDN, Transcoding Service, Storage, Analytics
  # Technologies: CloudFront, FFmpeg, S3, Kinesis

  # Design a Social Media Platform
  # Components: Feed Service, User Service, Notification Service, Analytics
  # Technologies: Microservices, Kafka, Elasticsearch, Prometheus
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ¯ Technical Interview Questions</h4>
  <pre><code>
  # Linux/DevOps Questions
  Q: How do you troubleshoot a high CPU usage issue?
  A:
  1. top/htop to identify process
  2. ps aux | grep <process> for details
  3. strace -p <PID> to trace system calls
  4. perf top for performance analysis
  5. Check logs in /var/log/

  Q: How do you secure a Kubernetes cluster?
  A:
  1. Enable RBAC and network policies
  2. Use pod security policies
  3. Encrypt secrets at rest
  4. Regular security scanning
  5. Monitor with Falco
  6. Use private image registry

  Q: Design a CI/CD pipeline for microservices
  A:
  1. Code commit triggers webhook
  2. Build and test in parallel
  3. Security scanning
  4. Create Docker image
  5. Deploy to staging
  6. Integration tests
  7. Deploy to production with blue-green
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ”§ Practical Scenarios</h4>
  <pre><code>
  # Troubleshooting Scenarios
  # 1. Website is down
  - Check load balancer health
  - Verify web server status
  - Check DNS resolution
  - Review application logs
  - Test database connectivity

  # 2. Database performance issues
  - Check slow query log
  - Analyze with EXPLAIN
  - Monitor connection pool
  - Check disk I/O
  - Review indexing strategy

  # 3. Container deployment failures
  - Check image pull errors
  - Verify resource limits
  - Review pod logs
  - Check network policies
  - Validate configuration
</code></pre>
</div>

<h2 id="career-advancement">ğŸ“ˆ 59. CAREER ADVANCEMENT & CERTIFICATIONS</h2>

<div class="command-block">
  <h4>ğŸ† Industry Certifications</h4>
  <pre><code>
  # Red Hat Certifications
  - RHCSA (Red Hat Certified System Administrator)
  - RHCE (Red Hat Certified Engineer)
  - RHCA (Red Hat Certified Architect)

  # Cloud Certifications
  - AWS Solutions Architect (Associate/Professional)
  - Azure Administrator (Associate/Expert)
  - Google Cloud Professional Architect

  # DevOps Certifications
  - Docker Certified Associate
  - Kubernetes Administrator (CKA/CKAD)
  - Terraform Associate

  # Security Certifications
  - CompTIA Security+
  - CISSP (Certified Information Systems Security Professional)
  - CEH (Certified Ethical Hacker)
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“ˆ Career Progression Path</h4>
  <pre><code>
  # Junior Level (0-2 years)
  - System Administrator
  - Junior DevOps Engineer
  - Cloud Support Engineer
  - Technical Support Engineer

  # Mid-Level (2-5 years)
  - DevOps Engineer
  - Site Reliability Engineer
  - Cloud Engineer
  - Infrastructure Engineer

  # Senior Level (5+ years)
  - Senior DevOps Engineer
  - Principal SRE
  - DevOps Architect
  - Platform Engineer

  # Leadership Roles
  - DevOps Manager
  - Head of Infrastructure
  - CTO/VP Engineering
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ› ï¸ Skills Development Roadmap</h4>
  <pre><code>
  # Technical Skills Priority
  1. Container Orchestration (Kubernetes)
  2. Infrastructure as Code (Terraform)
  3. CI/CD Pipelines (Jenkins/GitHub Actions)
  4. Cloud Platforms (AWS/Azure/GCP)
  5. Monitoring & Observability
  6. Security & Compliance
  7. Automation & Scripting

  # Soft Skills Development
  1. Communication & Collaboration
  2. Problem Solving & Troubleshooting
  3. Project Management
  4. Leadership & Mentoring
  5. Documentation & Knowledge Sharing

  # Learning Resources
  - Official documentation
  - Online courses (Coursera, Udemy)
  - Hands-on labs (Katacoda, Qwiklabs)
  - Community involvement (meetups, conferences)
  - Open source contributions
</code></pre>
</div>

<h2 id="performance-optimization">âš¡ 60. PERFORMANCE OPTIMIZATION & TUNING</h2>

<div class="command-block">
  <h4>âš¡ System Performance Tuning</h4>
  <pre><code>
  # CPU optimization
  echo 'performance' > /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
  tuned-adm profile throughput-performance
  cpupower frequency-set -g performance

  # Memory optimization
  echo 3 > /proc/sys/vm/drop_caches
  sysctl -w vm.swappiness=10
  sysctl -w vm.dirty_ratio=15
  sysctl -w vm.dirty_background_ratio=5

  # Disk I/O optimization
  echo deadline > /sys/block/sda/queue/scheduler
  echo 1024 > /sys/block/sda/queue/read_ahead_kb
  tune2fs -o journal_data_writeback /dev/sda1
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸŒ Network Performance</h4>
  <pre><code>
  # TCP tuning
  sysctl -w net.core.rmem_max=16777216
  sysctl -w net.core.wmem_max=16777216
  sysctl -w net.ipv4.tcp_rmem='4096 87380 16777216'
  sysctl -w net.ipv4.tcp_wmem='4096 65536 16777216'
  sysctl -w net.ipv4.tcp_congestion_control=bbr

  # Network interface optimization
  ethtool -G eth0 rx 4096 tx 4096
  ethtool -K eth0 gro on lro on tso on gso on
  tc qdisc add dev eth0 root fq_codel

  # Load balancing optimization
  nginx -t && nginx -s reload
  haproxy -f /etc/haproxy/haproxy.cfg -D
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸš€ Application Performance</h4>
  <pre><code>
  # JVM tuning
  export JAVA_OPTS="-Xms4g -Xmx8g -XX:+UseG1GC -XX:MaxGCPauseMillis=200"
  jstat -gc -t <pid> 5s
  jmap -histo <pid>

  # Database optimization
  mysql -e "SHOW VARIABLES LIKE 'innodb_buffer_pool_size';"
  postgresql -c "SELECT * FROM pg_settings WHERE name = 'shared_buffers';"
  redis-cli CONFIG SET maxmemory 2gb

  # Container performance
  docker stats --no-stream
  kubectl top nodes
  kubectl top pods
</code></pre>
</div>

<h2 id="advanced-troubleshooting">ğŸ”§ 61. ADVANCED TROUBLESHOOTING METHODOLOGIES</h2>

<div class="command-block">
  <h4>ğŸ”§ System Troubleshooting Framework</h4>
  <pre><code>
  # 1. Information gathering
  dmesg | tail -50
  journalctl -p err -n 100
  top -b -n 1 | head -20
  free -m
  df -h
  netstat -tuln

  # 2. Process analysis
  ps aux --sort=-%cpu | head -10
  ps aux --sort=-%mem | head -10
  strace -p <PID> -c
  lsof -p <PID>

  # 3. Network troubleshooting
  ping -c 4 8.8.8.8
  traceroute google.com
  nslookup google.com
  ss -tuln
  tcpdump -i eth0 -c 10

  # 4. Disk troubleshooting
  iostat -x 1 5
  iotop -o
  df -i
  du -sh /* | sort -rh | head -10
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ› Application Troubleshooting</h4>
  <pre><code>
  # Web server issues
  curl -I http://localhost
  nginx -t
  apache2ctl configtest
  systemctl status nginx

  # Database issues
  mysql -e "SHOW PROCESSLIST;"
  pg_stat_activity
  redis-cli INFO stats

  # Container issues
  docker logs <container>
  kubectl logs <pod>
  kubectl describe pod <pod>
  kubectl get events --sort-by=.metadata.creationTimestamp
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“Š Performance Bottleneck Analysis</h4>
  <pre><code>
  # CPU bottleneck analysis
  perf top -p <PID>
  perf record -p <PID> -g
  perf report

  # Memory analysis
  valgrind --tool=memcheck ./program
  pmap <PID>
  cat /proc/<PID>/smaps

  # I/O analysis
  iostat -x 1
  blktrace /dev/sda
  btrace /dev/sda
</code></pre>
</div>

<h2 id="real-projects">ğŸŒ 62. REAL-WORLD PROJECT EXAMPLES</h2>

<div class="command-block">
  <h4>ğŸ›’ E-commerce Platform Infrastructure</h4>
  <pre><code>
  # Project: Scalable E-commerce Platform
  # Technologies: Kubernetes, AWS, Redis, PostgreSQL, React

  # Infrastructure setup
  terraform init
  terraform plan -var-file=prod.tfvars
  terraform apply -var-file=prod.tfvars

  # Kubernetes deployment
  kubectl apply -f k8s/namespaces/
  kubectl apply -f k8s/configmaps/
  kubectl apply -f k8s/secrets/
  kubectl apply -f k8s/deployments/
  kubectl apply -f k8s/services/
  kubectl apply -f k8s/ingress/

  # Database setup
  kubectl apply -f k8s/postgres/
  kubectl exec -it postgres-0 -- psql -U admin -c "CREATE DATABASE ecommerce;"
  kubectl exec -it postgres-0 -- psql -U admin -d ecommerce -f schema.sql

  # Redis cache
  kubectl apply -f k8s/redis/
  kubectl exec -it redis-0 -- redis-cli FLUSHALL

  # Monitoring setup
  helm install prometheus prometheus-community/kube-prometheus-stack -n monitoring
  kubectl apply -f k8s/monitoring/grafana-dashboards/
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ”„ Microservices CI/CD Pipeline</h4>
  <pre><code>
  # Project: Microservices with Automated Deployment
  # Technologies: Jenkins, Docker, Kubernetes, Helm

  # Jenkins pipeline
  pipeline {
  agent any
  stages {
  stage('Build') {
  steps {
  sh 'docker build -t myapp:${BUILD_NUMBER} .'
  sh 'docker tag myapp:${BUILD_NUMBER} registry.com/myapp:${BUILD_NUMBER}'
  }
  }
  stage('Test') {
  steps {
  sh 'docker run --rm myapp:${BUILD_NUMBER} npm test'
  sh 'docker run --rm myapp:${BUILD_NUMBER} npm run integration-test'
  }
  }
  stage('Security Scan') {
  steps {
  sh 'trivy image --severity HIGH,CRITICAL myapp:${BUILD_NUMBER}'
  sh 'sonar-scanner'
  }
  }
  stage('Deploy') {
  steps {
  sh 'helm upgrade --install myapp ./helm-chart --set image.tag=${BUILD_NUMBER}'
  }
  }
  }
  }

  # Helm chart structure
  helm create myapp
  # Chart.yaml, values.yaml, templates/
  helm template myapp ./helm-chart --set image.tag=v1.0.0
  helm lint ./helm-chart
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“Š Log Analytics Platform</h4>
  <pre><code>
  # Project: Centralized Log Analysis
  # Technologies: ELK Stack, Filebeat, Logstash, Kibana

  # Elasticsearch setup
  docker-compose up -d elasticsearch
  curl -X GET "localhost:9200/_cluster/health?pretty"

  # Logstash configuration
  input {
  beats {
  port => 5044
  }
  }
  filter {
  if [fields][service] == "nginx" {
  grok {
  match => { "message" => "%{NGINX_ACCESS}" }
  }
  }
  }
  output {
  elasticsearch {
  hosts => ["elasticsearch:9200"]
  index => "logs-%{+YYYY.MM.dd}"
  }
  }

  # Kibana dashboards
  curl -X POST "localhost:5601/api/saved_objects/_import" -H "kbn-xsrf: true" --form file=@dashboard.ndjson
</code></pre>
</div>

<h2 id="project-templates">ğŸ“ 63. PROJECT TEMPLATES & STARTERS</h2>

<div class="command-block">
  <h4>â˜¸ï¸ Kubernetes Application Template</h4>
  <pre><code>
  # Standard K8s app structure
  mkdir -p myapp/{k8s,scripts,docker,helm}
  cd myapp

  # Dockerfile template
  cat > Dockerfile <<EOF
  FROM node:16-alpine
  WORKDIR /app
  COPY package*.json ./
  RUN npm ci --only=production
  COPY . .
  EXPOSE 3000
  CMD ["npm", "start"]
  EOF

  # Deployment template
  cat > k8s/deployment.yaml <<EOF
  apiVersion: apps/v1
  kind: Deployment
  metadata:
  name: myapp
  spec:
  replicas: 3
  selector:
  matchLabels:
  app: myapp
  template:
  metadata:
  labels:
  app: myapp
  spec:
  containers:
  - name: myapp
  image: myapp:latest
  ports:
  - containerPort: 3000
  resources:
  requests:
  memory: "256Mi"
  cpu: "250m"
  limits:
  memory: "512Mi"
  cpu: "500m"
  EOF

  # Service template
  cat > k8s/service.yaml <<EOF
  apiVersion: v1
  kind: Service
  metadata:
  name: myapp-service
  spec:
  selector:
  app: myapp
  ports:
  - port: 80
  targetPort: 3000
  type: LoadBalancer
  EOF
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ—ï¸ Terraform Module Template</h4>
  <pre><code>
  # Terraform module structure
  mkdir -p terraform/modules/{vpc,ec2,rds}
  cd terraform

  # VPC module
  cat > modules/vpc/main.tf <<EOF
  resource "aws_vpc" "main" {
  cidr_block           = var.vpc_cidr
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
  Name = var.vpc_name
  }
  }

  resource "aws_subnet" "public" {
  count             = length(var.public_subnet_cidrs)
  vpc_id            = aws_vpc.main.id
  cidr_block        = var.public_subnet_cidrs[count.index]
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
  Name = "${var.vpc_name}-public-${count.index + 1}"
  }
  }
  EOF

  # Variables
  cat > modules/vpc/variables.tf <<EOF
  variable "vpc_cidr" {
  description = "CIDR block for VPC"
  type        = string
  }

  variable "vpc_name" {
  description = "Name of the VPC"
  type        = string
  }

  variable "public_subnet_cidrs" {
  description = "CIDR blocks for public subnets"
  type        = list(string)
  }
  EOF
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸš€ CI/CD Pipeline Template</h4>
  <pre><code>
  # GitHub Actions workflow
  mkdir -p .github/workflows
  cat > .github/workflows/ci-cd.yml <<EOF
  name: CI/CD Pipeline

  on:
  push:
  branches: [ main, develop ]
  pull_request:
  branches: [ main ]

  jobs:
  test:
  runs-on: ubuntu-latest
  steps:
  - uses: actions/checkout@v3
  - name: Setup Node.js
  uses: actions/setup-node@v3
  with:
  node-version: '16'
  cache: 'npm'
  - name: Install dependencies
  run: npm ci
  - name: Run tests
  run: npm test
  - name: Run linting
  run: npm run lint

  build:
  needs: test
  runs-on: ubuntu-latest
  if: github.ref == 'refs/heads/main'
  steps:
  - uses: actions/checkout@v3
  - name: Build Docker image
  run: docker build -t myapp:${{ github.sha }} .
  - name: Push to registry
  run: |
  echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
  docker push myapp:${{ github.sha }}

  deploy:
  needs: build
  runs-on: ubuntu-latest
  if: github.ref == 'refs/heads/main'
  steps:
  - name: Deploy to Kubernetes
  run: |
  echo "${{ secrets.KUBECONFIG }}" | base64 -d > kubeconfig
  export KUBECONFIG=kubeconfig
  kubectl set image deployment/myapp myapp=myapp:${{ github.sha }}
  kubectl rollout status deployment/myapp
  EOF
</code></pre>
</div>

<h2 id="certification-guides">ğŸ“ 64. CERTIFICATION PREPARATION GUIDES</h2>

<div class="command-block">
  <h4>ğŸ“ RHCSA Exam Preparation</h4>
  <pre><code>
  # RHCSA (EX200) Key Topics
  # 1. Filesystems and Storage
  fdisk /dev/sdb
  mkfs.ext4 /dev/sdb1
  mount /dev/sdb1 /mnt/data
  echo "/dev/sdb1 /mnt/data ext4 defaults 0 2" >> /etc/fstab

  # 2. User and Group Management
  useradd -m -s /bin/bash john
  groupadd developers
  usermod -aG developers john
  passwd john

  # 3. Permissions and Ownership
  chmod 755 /home/john
  chown john:developers /home/john/project
  setfacl -m u:alice:rw /home/john/project

  # 4. Process Management
  ps aux | grep httpd
  kill -9 1234
  systemctl start httpd
  systemctl enable httpd

  # 5. Package Management
  dnf install httpd
  dnf update
  dnf remove httpd
  rpm -qa | grep httpd

  # 6. Network Configuration
  nmcli connection modify eth0 ipv4.addresses 192.168.1.100/24
  nmcli connection modify eth0 ipv4.gateway 192.168.1.1
  nmcli connection up eth0

  # 7. Firewall Management
  firewall-cmd --add-service=http --permanent
  firewall-cmd --add-port=8080/tcp --permanent
  firewall-cmd --reload

  # 8. SELinux
  getenforce
  setenforce 0
  semanage fcontext -a -t httpd_sys_content_t "/var/www/html(/.*)?"
  restorecon -Rv /var/www/html

  # Practice Commands
  find / -name "*.conf" -type f 2>/dev/null
  grep -r "error" /var/log/ 2>/dev/null
  tar -czf backup.tar.gz /home/
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ† RHCE Exam Preparation</h4>
  <pre><code>
  # RHCE (EX294) Key Topics
  # 1. Advanced Networking
  nmcli connection add type ethernet con-name eth1 ifname eth1
  nmcli connection modify eth1 ipv4.method manual ipv4.addresses 10.0.0.1/24
  nmcli connection modify eth1 ipv4.dns 8.8.8.8
  nmcli connection up eth1

  # 2. Firewall Advanced Rules
  firewall-cmd --permanent --add-rich-rule='rule family="ipv4" source address="192.168.1.0/24" service name="http" accept'
  firewall-cmd --permanent --add-port=5432/tcp --source=10.0.0.0/24
  firewall-cmd --reload

  # 3. Web Server Configuration
  yum install httpd mod_ssl
  mkdir -p /var/www/html/vhost1
  echo "Welcome to vhost1" > /var/www/html/vhost1/index.html

  # Virtual host configuration
  cat > /etc/httpd/conf.d/vhost1.conf <<EOF
  <VirtualHost *:80>
  ServerName vhost1.example.com
  DocumentRoot /var/www/html/vhost1
</VirtualHost>
EOF

# 4. Database Setup
yum install mariadb-server
systemctl enable --now mariadb
mysql_secure_installation
mysql -u root -p -e "CREATE USER 'webapp'@'localhost' IDENTIFIED BY 'password';"
mysql -u root -p -e "GRANT ALL PRIVILEGES ON webapp.* TO 'webapp'@'localhost';"

# 5. Scripting and Automation
cat > /usr/local/bin/backup.sh <<EOF
#!/bin/bash
tar -czf /backup/$(date +%Y%m%d)_backup.tar.gz /home/
EOF
chmod +x /usr/local/bin/backup.sh

# Cron job configuration
echo "0 2 * * * /usr/local/bin/backup.sh" | crontab -

# 6. Container Management
yum install podman
podman pull nginx
podman run -d -p 8080:80 --name webserver nginx
podman generate systemd --name webserver --files
</code></pre>
</div>

<div class="command-block">
  <h4>â˜¸ï¸ CKA (Certified Kubernetes Administrator)</h4>
  <pre><code>
  # CKA Exam Practice
  # 1. Cluster Architecture
  kubectl cluster-info
  kubectl get nodes
  kubectl describe node node1

  # 2. Installation, Configuration & Validation
  kubeadm init --pod-network-cidr=10.244.0.0/16
  kubeadm join <control-plane-ip>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>

  # 3. Workloads & Scheduling
  kubectl run nginx --image=nginx --port=80
  kubectl expose pod nginx --port=80 --target-port=80 --type=LoadBalancer
  kubectl scale deployment nginx --replicas=3

  # 4. Services & Networking
  kubectl create service clusterip mysvc --tcp=80:80
  kubectl get endpoints
  kubectl apply -f network-policy.yaml

  # 5. Storage
  kubectl create -f pvc.yaml
  kubectl get pv,pvc
  kubectl apply -f storage-class.yaml

  # 6. Troubleshooting
  kubectl get pods -o wide
  kubectl describe pod <pod-name>
  kubectl logs <pod-name>
  kubectl exec -it <pod-name> -- /bin/bash

  # 7. Cluster Maintenance
  kubectl cordon node1
  kubectl drain node1 --ignore-daemonsets
  kubectl uncordon node1

  # 8. Security & RBAC
  kubectl create serviceaccount mysa
  kubectl create role myrole --verb=get,list,watch --resource=pods
  kubectl create rolebinding mybinding --role=myrole --serviceaccount=default:mysa
</code></pre>
</div>

<h2 id="exam-strategies">ğŸ“š 65. EXAM STRATEGIES & CAREER DEVELOPMENT</h2>

<div class="command-block">
  <h4>ğŸ“š Exam Preparation Strategies</h4>
  <pre><code>
  # Study Plan Template
  # Week 1-2: Foundation
  - Review official exam objectives
  - Set up practice environment
  - Complete basic labs

  # Week 3-4: Core Topics
  - Focus on high-weight topics
  - Practice with hands-on labs
  - Take practice exams

  # Week 5-6: Advanced Topics
  - Complex scenarios
  - Performance optimization
  - Troubleshooting techniques

  # Week 7-8: Final Preparation
  - Full practice exams
  - Time management practice
  - Review weak areas

  # Practice Environment Setup
  # RHCSA/RHCE
  yum install -y @development
  yum install -y httpd mariadb-server
  systemctl set-default multi-user.target

  # Kubernetes
  minikube start
  kubectl cluster-info
  helm init

  # AWS
  aws configure
  aws ec2 describe-instances
  aws s3 ls
</code></pre>
</div>

<div class="command-block">
  <h4>ğŸ“ˆ Career Development Path</h4>
  <pre><code>
  # Skill Assessment Matrix
  # Technical Skills (Weight: 70%)
  1. Linux Administration (20%)
  2. Container Orchestration (15%)
  3. Cloud Platforms (15%)
  4. CI/CD & Automation (10%)
  5. Monitoring & Security (10%)

  # Soft Skills (Weight: 30%)
  1. Problem Solving (10%)
  2. Communication (8%)
  3. Leadership (7%)
  4. Documentation (5%)

  # Learning Roadmap
  # Year 1: Foundation
  - Master Linux fundamentals
  - Learn container basics
  - Get first cloud certification
  - Build automation skills

  # Year 2: Specialization
  - Kubernetes expertise
  - Multi-cloud skills
  - Advanced CI/CD
  - Security fundamentals

  # Year 3: Leadership
  - Architecture design
  - Team leadership
  - Cost optimization
  - DevOps transformation

  # Portfolio Projects
  1. Personal blog on Kubernetes
  2. CI/CD pipeline for microservices
  3. Multi-cloud deployment script
  4. Monitoring dashboard
  5. Infrastructure as Code templates
</code></pre>
</div>

<div class="command-block">
  <h4>âœ… Interview Preparation Checklist</h4>
  <pre><code>
  # Technical Questions to Master
  1. System Design
  - Scalable architecture patterns
  - High availability strategies
  - Performance optimization

  2. Troubleshooting
  - Network issues
  - Performance bottlenecks
  - Service failures

  3. Security
  - Best practices
  - Common vulnerabilities
  - Compliance requirements

  4. Automation
  - Scripting languages
  - Infrastructure as Code
  - CI/CD pipelines

  # Practical Exercises
  # 1. Build a complete deployment pipeline
  # 2. Design a microservices architecture
  # 3. Troubleshoot a production issue
  # 4. Optimize system performance
  # 5. Implement security measures

  # Resume Keywords
  - Kubernetes, Docker, Containers
  - AWS, Azure, GCP, Multi-cloud
  - Jenkins, GitLab CI, GitHub Actions
  - Terraform, Ansible, Infrastructure as Code
  - Prometheus, Grafana, ELK Stack
  - Python, Bash, Go, Automation
  - Security, Compliance, DevSecOps
  - Performance, Scalability, Reliability
</code></pre>
</div>

<h2 id="final-resources">ğŸ“š 66. FINAL RESOURCES & REFERENCE</h2>

<div class="command-block">
  <h4>ğŸ“š Essential DevOps Resources</h4>
  <pre><code>
  # Official Documentation
  # Kubernetes: https://kubernetes.io/docs/
  # Docker: https://docs.docker.com/
  # AWS: https://docs.aws.amazon.com/
  # Terraform: https://www.terraform.io/docs/
  # Ansible: https://docs.ansible.com/

  # Learning Platforms
  # Katacoda: https://katacoda.com/
  # Qwiklabs: https://qwiklabs.com/
  # A Cloud Guru: https://acloud.guru/
  # Linux Academy: https://linuxacademy.com/

  # Community Resources
  # GitHub: https://github.com/
  # Stack Overflow: https://stackoverflow.com/
  # Reddit: r/devops, r/sysadmin
  # DevOps subreddits

  # Certification Resources
  # Red Hat: https://www.redhat.com/en/services/training-and-certification
  # AWS: https://aws.amazon.com/certification/
  # Kubernetes: https://www.cncf.io/certification/
  # Docker: https://www.docker.com/certified-associate/

  # Tools & Utilities
  # Visual Studio Code: https://code.visualstudio.com/
  # Postman: https://www.postman.com/
  # DBeaver: https://dbeaver.io/
  # GitKraken: https://www.gitkraken.com/

  # Monitoring & Observability
  # Prometheus: https://prometheus.io/
  # Grafana: https://grafana.com/
  # Jaeger: https://www.jaegertracing.io/
  # OpenTelemetry: https://opentelemetry.io/

  # Security Tools
  # OWASP ZAP: https://www.zaproxy.org/
  # Trivy: https://github.com/aquasecurity/trivy
  # Falco: https://falco.org/
  # OPA: https://www.openpolicyagent.org/

  # Container Registries
  # Docker Hub: https://hub.docker.com/
  # Quay.io: https://quay.io/
  # GitHub Container Registry: https://ghcr.io/
  # AWS ECR: https://aws.amazon.com/ecr/

  # CI/CD Platforms
  # GitHub Actions: https://github.com/features/actions
  # GitLab CI: https://docs.gitlab.com/ee/ci/
  # Jenkins: https://www.jenkins.io/
  # CircleCI: https://circleci.com/
  # Travis CI: https://www.travisci.com/

  # Infrastructure as Code
  # Terraform Registry: https://registry.terraform.io/
  # Pulumi: https://www.pulumi.com/
  # AWS CloudFormation: https://aws.amazon.com/cloudformation/
  # Azure ARM Templates: https://docs.microsoft.com/en-us/azure/azure-resource-manager/

  # Configuration Management
  # Ansible Galaxy: https://galaxy.ansible.com/
  # Chef Supermarket: https://supermarket.chef.io/
  # Puppet Forge: https://forge.puppet.com/
  # SaltStack: https://repo.saltstack.com/

  # Container Orchestration
  # Docker Swarm: https://docs.docker.com/engine/swarm/
  # Kubernetes: https://kubernetes.io/
  # OpenShift: https://www.openshift.com/
  # Rancher: https://rancher.com/

  # Service Mesh
  # Istio: https://istio.io/
  # Linkerd: https://linkerd.io/
  # Consul Connect: https://www.consul.io/
  # AWS App Mesh: https://aws.amazon.com/app-mesh/

  # Serverless Platforms
  # AWS Lambda: https://aws.amazon.com/lambda/
  # Azure Functions: https://azure.microsoft.com/en-us/services/functions
  # Google Cloud Functions: https://cloud.google.com/functions
  # Knative: https://knative.dev/

  # Database Technologies
  # PostgreSQL: https://www.postgresql.org/
  # MySQL: https://www.mysql.com/
  # MongoDB: https://www.mongodb.com/
  # Redis: https://redis.io/
  # Elasticsearch: https://www.elastic.co/

  # Message Queues
  # RabbitMQ: https://www.rabbitmq.com/
  # Apache Kafka: https://kafka.apache.org/
  # AWS SQS: https://aws.amazon.com/sqs/
  # Azure Service Bus: https://azure.microsoft.com/en-us/services/service-bus

  # Load Balancing
  # NGINX: https://www.nginx.com/
  # HAProxy: https://www.haproxy.org/
  # AWS ALB/NLB: https://aws.amazon.com/elasticloadbalancing/
  # Azure Load Balancer: https://azure.microsoft.com/en-us/services/load-balancer

  # DNS Services
  # AWS Route 53: https://aws.amazon.com/route53/
  # Cloudflare: https://www.cloudflare.com/
  # Google Cloud DNS: https://cloud.google.com/dns

  # CDN Services
  # AWS CloudFront: https://aws.amazon.com/cloudfront/
  # Azure CDN: https://azure.microsoft.com/en-us/services/cdn
  # Cloudflare CDN: https://www.cloudflare.com/cdn

  # Storage Services
  # AWS S3: https://aws.amazon.com/s3/
  # Azure Blob Storage: https://azure.microsoft.com/en-us/services/storage/blobs
  # Google Cloud Storage: https://cloud.google.com/storage

  # Network Services
  # AWS VPC: https://aws.amazon.com/vpc/
  # Azure VNet: https://azure.microsoft.com/en-us/services/virtual-network
  # Google Cloud VPC: https://cloud.google.com/vpc

  # Identity & Access Management
  # AWS IAM: https://aws.amazon.com/iam/
  # Azure AD: https://azure.microsoft.com/en-us/services/active-directory
  # Google Cloud IAM: https://cloud.google.com/iam

  # Backup & Disaster Recovery
  # AWS Backup: https://aws.amazon.com/backup/
  # Azure Backup: https://azure.microsoft.com/en-us/services/backup
  # Veeam: https://www.veeam.com/
</code></pre>
</div>

<h2 id="summary">ğŸ“‹ 67. SUMMARY</h2>

<div class="info">
  <p>
  This comprehensive command reference covers 62 major categories with
  3000+ commands organized for rapid recall and muscle memory
  reinforcement:
</p>

<br />

<hr />

<br />

<p>
<strong>System Information & Navigation</strong> - Basic Linux
exploration
</p>
<p>
<strong>File Operations</strong> - Creating, copying, moving, finding
files
</p>
<p><strong>Text Editors</strong> - Vim and Nano</p>
<p>
<strong>File Permissions</strong> - chmod, chown, ACLs, attributes
</p>
<p>
<strong>User & Group Management</strong> - Creating, modifying
users/groups
</p>
<p>
<strong>Process Management</strong> - Viewing, controlling,
prioritizing processes
</p>
<p>
<strong>systemd Service Management</strong> - Managing services and
system state
</p>
<p><strong>Package Management</strong> - DNF/YUM and RPM</p>
<p><strong>System Logging</strong> - rsyslog, journalctl, auditd</p>
<p>
<strong>Disk Management</strong> - Partitioning, LVM, filesystems,
mounting
</p>
<p>
<strong>Networking Basics</strong> - IP configuration, routing,
testing
</p>
<p><strong>Firewall Management</strong> - firewalld and iptables</p>
<p><strong>SSH</strong> - Secure shell, keys, tunneling, SCP, SFTP</p>
<p>
<strong>Text Processing</strong> - grep, sed, awk, cut, sort, etc.
</p>
<p>
<strong>Bash Scripting</strong> - Variables, loops, functions, arrays
</p>
<p><strong>Python for DevOps</strong> - pip, virtual environments</p>
<p><strong>Git Version Control</strong> - All Git operations</p>
<p><strong>Docker</strong> - Containers, images, volumes, networks</p>
<p><strong>Kubernetes</strong> - kubectl for container orchestration</p>
<p>
<strong>Ansible</strong> - Configuration management and automation
</p>
<p><strong>Terraform</strong> - Infrastructure as Code</p>
<p><strong>AWS CLI</strong> - Cloud resource management</p>
<p><strong>Monitoring</strong> - Prometheus, Grafana, ELK stack</p>
<p>
<strong>Security</strong> - fail2ban, SELinux, OpenSSL, GPG, scanning
</p>
<p>
<strong>Performance Monitoring</strong> - iostat, vmstat, sar, tuning
</p>
<p>
<strong>Backup & Recovery</strong> - tar, rsync, dd, database backups
</p>
<p><strong>Automation</strong> - cron, at, systemd timers</p>
<p>
<strong>Miscellaneous</strong> - Compression, system info,
calculators, multiplexers
</p>
<p>
<strong>Useful One-Liners</strong> - Quick reference for common tasks
</p>

<br />

<p>
Perfect for DevOps engineers who need rapid command lookup without
project context or theoretical explanations.
</p>
</div>

<div class="download-section">
  <h3>ğŸ“– Complete Reference Guide</h3>
  <p>
  This is Part 3 of 3. For the complete Linux/DevOps command reference:
</p>
<a href="system-admin.html" class="download-btn"
>Part 1: System Administration</a
>
<a href="devops-tools.html" class="download-btn"
>Part 2: DevOps Tools</a
>
<a href="monitoring-security.html" class="download-btn btn-current"
>Part 3: Monitoring & Security</a>

</div>

<div class="warning">
  <p>
  <strong>Note:</strong> This reference guide is optimized for quick
  lookup. Commands are organized by function rather than complexity.
  Always test commands in a safe environment before using in production.
</p>
</div>
<div class="info-box" style="border-left-color: #ff6b9d; margin-top: 50px;">
  <h3 style="color: #ff6b9d">ğŸ¯ Section Complete?</h3>
  <ul>
    <li><strong>ğŸ“š Next Steps:</strong> Continue to <a href="index.html" style="color: #ff6b9d;">main page</a> or explore other sections</li>
      <li><strong>ğŸ” Quick Review:</strong> Use Ctrl+F to find specific commands</li>
        <li><strong>ğŸ’¼ Career Focus:</strong> Practice these commands for senior-level positions</li>
          <li><strong>ğŸ“– Documentation:</strong> Check <a href="README.md">README.md</a> for full overview</li>
          </ul>
        </div>
</div>


<script>
  // Auto-generate TOC based on <h2> headings
  const tocList = document.getElementById("toc-list");
  const headings = document.querySelectorAll("h2");

  headings.forEach((heading, index) => {
  const li = document.createElement("li");
  const a = document.createElement("a");
  a.href = `#${heading.id}`; // link to section
  a.textContent = `${heading.textContent}`; // same text
  li.appendChild(a);
  tocList.appendChild(li);
  });
</script>

<script>
  function downloadAsFile(filename) {
  // Get the entire HTML content
  const htmlContent = document.documentElement.outerHTML;

  // Create a blob with the HTML content
  const blob = new Blob([htmlContent], { type: "text/html" });

  // Create a temporary link element
  const link = document.createElement("a");
  link.href = URL.createObjectURL(blob);
  link.download = filename || "linux-devops-monitoring-security.html";

  // Append to body, click, and remove
  document.body.appendChild(link);
  link.click();
  document.body.removeChild(link);
  }

  function printPage() {
  window.print();
  }

  // Add smooth scrolling for navigation links
  document.querySelectorAll('a[href^="#"]').forEach((anchor) => {
  anchor.addEventListener("click", function (e) {
  e.preventDefault();

  const targetId = this.getAttribute("href");
  if (targetId === "#") return;

  const targetElement = document.querySelector(targetId);
  if (targetElement) {
  window.scrollTo({
  top: targetElement.offsetTop - 20,
  behavior: "smooth",
  });
  }
  });
  });

  function performSearch(searchTerm, resultsContainer) {
  // Get all pre elements (command blocks)
  const preElements = document.querySelectorAll("pre");
  const results = [];

  preElements.forEach((pre) => {
  const text = pre.textContent.toLowerCase();
  if (text.includes(searchTerm)) {
  // Get the heading for context
  let heading = "Unknown Section";
  let currentElement = pre.previousElementSibling;

  // Find the nearest heading
  while (currentElement) {
  if (currentElement.tagName.match(/^H[2-4]$/)) {
  heading = currentElement.textContent;
  break;
  }
  currentElement = currentElement.previousElementSibling;
  }

  // Extract matching lines
  const lines = pre.textContent.split("\n");
  const matchingLines = lines
  .filter((line) => line.toLowerCase().includes(searchTerm))
  .slice(0, 3); // Show first 3 matching lines

  if (matchingLines.length > 0) {
  results.push({
  heading: heading,
  lines: matchingLines,
  element: pre,
  });
  }
  }
  });

  // Display results
  if (results.length > 0) {
  let html = `<h3>Found ${results.length} results for "${searchTerm}"</h3>`;

  results.forEach((result, index) => {
  html += `
  <div class="search-result-item">
    <h4>${result.heading}</h4>
    <pre>${result.lines.join(
    "\n"
    )}</pre>
    <button onclick="scrollToResult(${index})"
    class="search-result-button">
    Go to Command
  </button>
</div>
`;
});

resultsContainer.innerHTML = html;
resultsContainer.style.display = "block";

// Store results for scrolling
window.searchResults = results;
} else {
resultsContainer.innerHTML = `<p class="no-results">No results found for "${searchTerm}"</p>`;
resultsContainer.style.display = "block";
}
}

function scrollToResult(index) {
if (window.searchResults && window.searchResults[index]) {
const element = window.searchResults[index].element;
window.scrollTo({
top: element.offsetTop - 20,
behavior: "smooth",
});

// Highlight the element temporarily
element.style.backgroundColor = "#fff3cd";
element.style.borderColor = "#ffc107";

setTimeout(() => {
element.style.backgroundColor = "";
element.style.borderColor = "";
}, 2000);
}
}

function clearSearch() {
const searchInput = document.getElementById("commandSearch");
if (searchInput) {
searchInput.value = "";
}
const resultsContainer = document.getElementById("searchResults");
if (resultsContainer) {
resultsContainer.style.display = "none";
resultsContainer.innerHTML = "";
}
}

</script>

<style>

  /* Professional Footer Styles */
  .info-box.section-complete {
  border-left-color: #ff6b9d;
  margin-top: 50px;
  }

  .info-box.section-complete h3 {
  color: #ff6b9d;
  }

  .info-box.section-complete a {
  color: #ff6b9d;
  }

  .next-level-section {
  text-align: center;
  margin-top: 30px;
  padding: 20px;
  background: #0d1117;
  border-radius: 10px;
  border: 2px solid #30363d;
  }

  .next-level-section h4 {
  color: #00d9ff;
  margin-top: 0;
  }

  .next-level-section p {
  color: #c9d1d9;
  font-size: 1em;
  margin-bottom: 15px;
  }

  .nav-buttons {
  margin-top: 15px;
  }

  .nav-buttons a {
  display: inline-block;
  padding: 10px 20px;
  border-radius: 5px;
  text-decoration: none;
  font-weight: bold;
  margin: 5px;
  transition: all 0.3s ease;
  }

  .nav-buttons a:hover {
  transform: translateY(-2px);
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
  }

  .nav-buttons .btn-home {
  background: #00d9ff;
  color: #0d1117;
  }

  .nav-buttons .btn-tools {
  background: #28a745;
  color: white;
  }

  .nav-buttons .btn-security {
  background: #ff6b9d;
  color: white;
  }

  /* Search Results Styles */
  .search-result-item {
  background-color: #f8f9fa;
  padding: 15px;
  margin: 10px 0;
  border-left: 4px solid #3498db;
  }

  .search-result-item h4 {
  margin-top: 0;
  }

  .search-result-item pre {
  margin: 10px 0;
  }

  .search-result-button {
  padding: 5px 10px;
  background-color: #2ecc71;
  color: white;
  border: none;
  border-radius: 3px;
  cursor: pointer;
  transition: all 0.3s ease;
  }

  .search-result-button:hover {
  background-color: #27ae60;
  transform: translateY(-1px);
  }

  .no-results {
  color: #e74c3c;
  }

  .btn-current {
  background-color: #3498db !important;
  }
</style>

        <div class="next-level-section">
          <h4>ğŸ† Ready for Next Level?</h4>
          <p>
          Master this section and advance your DevOps career
        </p>
        <div class="nav-buttons">
          <a href="index.html" class="btn-home">ğŸ  Back to Home</a>
          <a href="system-admin.html" class="btn-security">ğŸ’» System Admin</a>
          <a href="devops-tools.html" class="btn-tools">ğŸ› ï¸ DevOps Tools</a>
        </div>
      </div>

      <!-- SCRIPT SECTION -->
      <div id="script">
        <script>
          // Dark Mode Toggle
          function initDarkMode() {
          const theme = localStorage.getItem("theme") || "light";
          if (theme === "dark") {
          document.body.classList.add("dark-mode");
          }

          const themeToggle = document.getElementById("themeToggle");
          if (themeToggle) {
          themeToggle.addEventListener("click", toggleTheme);
          }
          }

          function toggleTheme() {
          document.body.classList.toggle("dark-mode");
          const theme = document.body.classList.contains("dark-mode")
          ? "dark"
          : "light";
          localStorage.setItem("theme", theme);
          }


          // Comment Formatter - Fixed version
          function formatComments() {
          document.querySelectorAll("pre code").forEach((codeBlock) => {
          let text = codeBlock.innerText;

          // Remove all leading spaces from all lines first
          let lines = text.split("\n");
          lines = lines.map(line => line.replace(/^\s+/, ""));

          // Find the longest command length for proper alignment
          let maxCmdLen = 0;
          lines.forEach(line => {
          const idx = line.indexOf("#");
          if (idx > 0 && !line.trim().startsWith("#")) {
          const cmd = line.slice(0, idx).trimEnd();
          maxCmdLen = Math.max(maxCmdLen, cmd.length);
          }
          });

          // Format each line
          const MIN_GAP = 13;
          const MAX_LINE_LENGTH = 113;
          const baseColumn = Math.max(maxCmdLen + 2, MIN_GAP);
          const result = [];

          lines.forEach(line => {
          const idx = line.indexOf("#");

          // Skip pure comments and lines without comments
          if (idx === -1 || line.trim().startsWith("#")) {
          result.push(line);
          return;
          }

          const cmd = line.slice(0, idx).trimEnd();
          const commentText = line.slice(idx).replace(/^#\s*/, "# "); // Ensure single space after #

          // Calculate proper spacing
          const spacesNeeded = Math.max(2, baseColumn - cmd.length);
          const paddedLine = cmd + " ".repeat(spacesNeeded) + commentText;

          // If too long, move comment to next line
          if (paddedLine.length > MAX_LINE_LENGTH) {
          result.push(cmd);
          result.push("# " + commentText.slice(2)); // Remove # and add it back properly
          } else {
          result.push(paddedLine);
          }
          });

          codeBlock.innerText = result.join("\n");
          });
          }

          // Clean Code Blocks - Remove leading spaces
          function cleanCodeBlocks() {
          document.querySelectorAll("pre code").forEach((block) => {
          const lines = block.innerText.split("\n");
          const cleanedLines = lines.map((line) => line.replace(/^\s+/, ""));
          block.innerText = cleanedLines.join("\n");
          });
          }

          // Accessibility Improvements
          function improveAccessibility() {
          // Add ARIA labels to command blocks
          document.querySelectorAll(".command-block pre").forEach((block) => {
          if (!block.getAttribute("aria-label")) {
          block.setAttribute("aria-label", "Command block with code examples");
          }
          });

          // Add keyboard navigation
          document.querySelectorAll("a[href]").forEach((link) => {
          if (!link.getAttribute("role")) {
          link.setAttribute("role", "navigation");
          }
          });
          }

          // Table of Contents Generator
          function generateTOC() {
          const tocList = document.getElementById('toc-list');
          if (!tocList) return;

          const headings = document.querySelectorAll('h2, h3');
          const tocItems = [];

          headings.forEach((heading, index) => {
          if (heading.id) {
          const level = parseInt(heading.tagName.charAt(1));
          const title = heading.textContent.trim().replace(/\b\w/g, char => char.toUpperCase());

          const li = document.createElement('li');
          li.className = `toc-level-${level}`;

          const a = document.createElement('a');
          a.href = `#${heading.id}`;
          a.textContent = title;

          li.appendChild(a);
          tocItems.push(li);
          }
          });

          tocList.innerHTML = '';
          tocItems.forEach(item => tocList.appendChild(item));
          }

          // Initialize everything on page load
          window.addEventListener("DOMContentLoaded", function () {
          initDarkMode();
          initBackToTop();
          formatComments();
          cleanCodeBlocks();
          generateTOC();

          // Add smooth scrolling for TOC links
          document.querySelectorAll('a[href^="#"]').forEach(anchor => {
          anchor.addEventListener('click', function (e) {
          e.preventDefault();
          const targetId = this.getAttribute('href');
          if (targetId === '#') return;

          const targetElement = document.querySelector(targetId);
          if (targetElement) {
          targetElement.scrollIntoView({
          behavior: 'smooth',
          block: 'start'
          });
          }
          });
          });
          });

          // Initialize accessibility on load
          window.addEventListener("load", improveAccessibility);
        </script>
      </div>
    </div>
  </body>
</html>
