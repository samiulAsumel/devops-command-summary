<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>🛠️ Linux/DevOps Command Reference - DevOps Tools (Top 5% Industry Standard)</title>
      <link rel="icon" type="image/png" href="assets/favicon.png" />
      <link rel="stylesheet" href="assets/styles.css" />
      <script src="assets/scripts.js" defer></script>
    </head>
    <body class="index-page">
      <!-- CONTENT SECTION -->
      <div id="content">
        <div class="container">
          <div class="part-links">
            <strong>📚 Navigate:</strong>
            <a href="index.html">🏠 HOME</a>
            <a href="system-admin.html">💻 Part 1: System Administration</a>
            <a href="devops-tools.html" class="active-part">🛠️ Part 2: DevOps Tools</a>
            <a href="monitoring-security.html">🔒 Part 3: Monitoring & Security</a>
          </div>
          <div class="hero-section">
            <h1>DevOps Tools</h1>
            <p class="hero-subtitle">🏆 Top 5% Industry Standard - Section 2 of 3</p>
            <p style="font-size: 0.95em; opacity: 0.9">
            Comprehensive reference covering enterprise-grade commands and best practices
          </p>
        </div>
        <div class="content">

          <h1>📚 Linux/DevOps Command Reference Guide</h1>

          <div class="section-index">
            <h3>📋 Table of Contents</h3>
            <ul id="toc-list">
              <!-- JS will populate this automatically -->
            </ul>
          </div>

          <h2 id="aws-from-scratch">25. AMAZON WEB SERVICES (AWS) - FROM SCRATCH</h2>

          <div>
            <h3>☁️ AWS Cloud Platform - From Scratch to Hero</h3>

            <div class="command-block">
              <h4>📚 Chapter 1: AWS Fundamentals (Absolute Beginner)</h4>
              <pre><code>
              # What is AWS?
              # AWS = Amazon Web Services
              # Created by Amazon (2006)
              # Cloud computing platform

              # Why Learn AWS for DevOps?
              # 1. Cloud Infrastructure: No physical servers
              # 2. Scalability: Auto-scale resources
              # 3. Global: Data centers worldwide
              # 4. Pay-as-you-go: Only pay for what you use
              # 5. Managed Services: Database, storage, compute

              # AWS Core Services for DevOps:
              # - EC2: Virtual servers
              # - S3: Object storage
              # - RDS: Managed databases
              # - Lambda: Serverless computing
              # - CloudFormation: Infrastructure as Code
              # - IAM: Identity and access management

              # Prerequisites:
              # 1. Basic networking knowledge
              # 2. Linux command line
              # 3. Credit card for AWS account
              # 4. AWS CLI installed
              # 5. Practice mindset! 🎯
            </code></pre>
          </div>

          <div class="command-block">
            <h4>🚀 Chapter 2: AWS CLI Installation and Setup</h4>
            <pre><code>
            # Step 1: Install AWS CLI
            # On Linux/Mac:
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip awscliv2.zip
            sudo ./aws/install

            # On Windows:
            # Download and run installer from aws.amazon.com/cli

            # Step 2: Verify Installation
            aws --version
            aws --version | head -1

            # Step 3: Configure AWS Credentials
            aws configure
            # Enter your AWS Access Key ID
            # Enter your AWS Secret Access Key
            # Enter default region (us-east-1)
            # Enter default output format (json)

            # Step 4: Test Configuration
            aws s3 ls
            aws ec2 describe-instances --max-items 1

            # Step 5: Create IAM User (Best Practice)
            # 1. Go to AWS Console -> IAM -> Users -> Create user
            # 2. Attach policies for required permissions
            # 3. Generate access keys
            # 4. Use these keys with aws configure

            # Step 6: Set Up Profiles (Multiple Accounts)
            aws configure --profile dev
            aws configure --profile prod
            aws configure --profile staging

            # Use specific profile:
            aws s3 ls --profile dev
            aws ec2 describe-instances --profile prod

            # Step 7: Environment Variables
            export AWS_ACCESS_KEY_ID=your_access_key
            export AWS_SECRET_ACCESS_KEY=your_secret_key
            export AWS_DEFAULT_REGION=us-east-1
            export AWS_DEFAULT_OUTPUT=json
          </code></pre>
        </div>

        <div class="command-block">
          <h4>🖥️ Chapter 3: EC2 (Virtual Servers)</h4>
          <pre><code>
          # EC2 = Elastic Compute Cloud
          # Virtual servers in the cloud

          # 1. List Available Instances
          aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,State.Name,InstanceType,PublicIpAddress]'
          aws ec2 describe-instances --output table

          # 2. Launch New Instance
          aws ec2 run-instances \
          --image-id ami-0abcdef12345678 \
          --instance-type t2.micro \
          --key-name my-key-pair \
          --security-group-ids sg-903004f8example \
          --subnet-id subnet-6e7f829example \
          --user-data file://user-data.sh \
          --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=MyInstance},{Key=Environment,Value=Dev}]'

          # 3. Stop Instance
          aws ec2 stop-instances --instance-ids i-1234567890abcdef0

          # 4. Start Instance
          aws ec2 start-instances --instance-ids i-1234567890abcdef0

          # 5. Reboot Instance
          aws ec2 reboot-instances --instance-ids i-1234567890abcdef0

          # 6. Terminate Instance
          aws ec2 terminate-instances --instance-ids i-1234567890abcdef0

          # 7. Create Key Pair
          aws ec2 create-key-pair \
          --key-name my-key-pair \
          --query 'KeyMaterial' \
          --output text > my-key-pair.pem

          # 8. List Key Pairs
          aws ec2 describe-key-pairs

          # 9. Create Security Group
          aws ec2 create-security-group \
          --group-name my-security-group \
          --description "Security group for web server"

          # 10. Add Security Group Rules
          aws ec2 authorize-security-group-ingress \
          --group-id sg-903004f8example \
          --protocol tcp \
          --port 80 \
          --cidr 0.0.0.0/0

          aws ec2 authorize-security-group-ingress \
          --group-id sg-903004f8example \
          --protocol tcp \
          --port 22 \
          --cidr 0.0.0.0/0

          # 11. Monitor Instance Status
          aws ec2 describe-instance-status --instance-ids i-1234567890abcdef0
        </code></pre>
      </div>

      <div class="command-block">
        <h4>📦 Chapter 4: S3 (Object Storage)</h4>
        <pre><code>
        # S3 = Simple Storage Service
        # Object storage for files and data

        # 1. List Buckets
        aws s3 ls
        aws s3api list-buckets

        # 2. Create Bucket
        aws s3 mb s3://my-unique-bucket-name
        aws s3 mb s3://my-bucket --region us-west-2

        # 3. Upload Files to S3
        aws s3 cp myfile.txt s3://my-bucket/myfile.txt
        aws s3 cp ./directory/ s3://my-bucket/directory/ --recursive

        # 4. Download Files from S3
        aws s3 cp s3://my-bucket/myfile.txt ./myfile.txt
        aws s3 cp s3://my-bucket/directory/ ./directory --recursive

        # 5. Sync Directories
        aws s3 sync ./local-directory/ s3://my-bucket/remote-directory/
        aws s3 sync s3://my-bucket/remote-directory/ ./local-directory/

        # 6. List Bucket Contents
        aws s3 ls s3://my-bucket
        aws s3 ls s3://my-bucket --recursive
        aws s3 ls s3://my-bucket --human-readable --summarize

        # 7. Delete Files
        aws s3 rm s3://my-bucket/myfile.txt
        aws s3 rm s3://my-bucket/directory/ --recursive

        # 8. Delete Bucket (must be empty)
        aws s3 rb s3://my-bucket
        aws s3 rb s3://my-bucket --force

        # 9. Set Bucket Permissions
        aws s3api put-bucket-acl \
        --bucket my-bucket \
        --acl public-read

        # 10. Enable Versioning
        aws s3api put-bucket-versioning \
        --bucket my-bucket \
        --versioning-configuration Status=Enabled

        # 11. Configure Lifecycle Rules
        aws s3api put-bucket-lifecycle-configuration \
        --bucket my-bucket \
        --lifecycle-configuration file://lifecycle.json

        # lifecycle.json:
        {
        "Rules": [
        {
        "ID": "DeleteOldFiles",
        "Status": "Enabled",
        "Filter": {"Prefix": "logs/"},
        "Transitions": [
        {
        "Days": 30,
        "StorageClass": "STANDARD_IA"
        },
        {
        "Days": 90,
        "StorageClass": "GLACIER"
        }
        ],
        "Expiration": {"Days": 365}
        }
        ]
        }
      </code></pre>
    </div>

    </div>

    <h2 id="docker-from-scratch">26. DOCKER CONTAINERIZATION - FROM SCRATCH</h2>

    <div>
      <h3>🐳 Docker Containers - From Scratch to Hero</h3>

      <div class="command-block">
        <h4>📚 Chapter 1: Docker Fundamentals (Absolute Beginner)</h4>
        <pre><code>
        # What is Docker?
        # Docker = Containerization platform
        # Created by Solomon Hykes (2013)
        # Acquired by Microsoft (2020)

        # Why Learn Docker for DevOps?
        # 1. Consistency: Same environment everywhere
        # 2. Portability: Run anywhere Docker runs
        # 3. Efficiency: Better resource utilization
        # 4. Isolation: Separate application dependencies
        # 5. Microservices: Modern application architecture

        # Docker in DevOps Ecosystem:
        # - Application Deployment: Containerized apps
        # - CI/CD: Build and test in containers
        # - Microservices: Service-oriented architecture
        # - Development: Consistent dev environments
        # - Production: Scalable deployments

        # Prerequisites:
        # 1. Basic Linux commands
        # 2. Understanding of applications
        # 3. System administration basics
        # 4. Text editor skills
        # 5. Practice mindset! 🎯
      </code></pre>
    </div>

    <div class="command-block">
      <h4>🚀 Chapter 2: Docker Installation and Setup</h4>
      <pre><code>
      # Step 1: Install Docker
      # On Ubuntu/Debian:
      curl -fsSL https://get.docker.com -o get-docker.sh
      sudo sh get-docker.sh

      # On CentOS/RHEL/Fedora:
      sudo dnf install docker
      sudo systemctl start docker
      sudo systemctl enable docker

      # On Arch Linux:
      sudo pacman -S docker

      # Step 2: Add User to Docker Group
      sudo usermod -aG docker $USER
      # Logout and login again for changes to take effect

      # Step 3: Verify Installation
      docker --version
      docker info

      # Step 4: Test Docker
      docker run hello-world

      # Step 5: Install Docker Compose
      # Latest version:
      sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
      sudo chmod +x /usr/local/bin/docker-compose

      # Using pip:
      pip install docker-compose

      # Step 6: Verify Docker Compose
      docker-compose --version

      # Step 7: Configure Docker Daemon
      sudo mkdir -p /etc/docker
      sudo tee /etc/docker/daemon.json > /dev/null <<EOF
      {
      "registry-mirrors": ["https://mirror.gcr.io"],
      "insecure-registries": [],
      "debug": false,
      "experimental": false,
      "storage-driver": "overlay2"
      }
      EOF

      sudo systemctl restart docker
    </code></pre>
  </div>

  <div class="command-block">
    <h4>🏗️ Chapter 3: Docker Images and Containers</h4>
    <pre><code>
    # Docker Images = Templates for containers
    # Docker Containers = Running instances of images

    # 1. Search for Images
    docker search nginx
    docker search python
    docker search --filter is-official=true ubuntu

    # 2. Pull Images
    docker pull nginx:latest
    docker pull ubuntu:20.04
    docker pull python:3.9-slim

    # 3. List Local Images
    docker images
    docker image ls
    docker images --format "table {{.Repository}}\t{{.Tag}}\t{{.Size}}\t{{.CreatedAt}}"

    # 4. Run Containers
    # Basic run:
    docker run nginx
    docker run ubuntu:20.04 echo "Hello from container"

    # Run with name:
    docker run --name my-nginx nginx

    # Run in background (detached):
    docker run -d nginx

    # Run with port mapping:
    docker run -d -p 8080:80 nginx

    # Run with volume mount:
    docker run -d -p 8080:80 -v /host/path:/container/path nginx

    # Run with environment variables:
    docker run -d -p 8080:80 -e ENV=production nginx

    # 5. List Running Containers
    docker ps
    docker ps -a  # Show all containers including stopped

    # 6. Stop Containers
    docker stop container_name
    docker stop container_id

    # 7. Start Containers
    docker start container_name
    docker start container_id

    # 8. Remove Containers
    docker rm container_name
    docker rm container_id
    docker rm $(docker ps -a -q)  # Remove all stopped containers

    # 9. Remove Images
    docker rmi image_name:tag
    docker rmi image_id
    docker rmi $(docker images -q)  # Remove all images

    # 10. Execute Commands in Running Container
    docker exec -it container_name /bin/bash
    docker exec container_name ls -la /app

    # 11. View Container Logs
    docker logs container_name
    docker logs -f container_name  # Follow logs

    # 12. Copy Files to/from Container
    docker cp container_name:/path/to/file ./local_file
    docker cp ./local_file container_name:/path/to/file
  </code></pre>
</div>

<div class="command-block">
  <h4>📝 Chapter 4: Dockerfile (Build Custom Images)</h4>
  <pre><code>
  # Dockerfile = Text file with instructions to build image
  # Automates image creation process

  # Basic Dockerfile Structure:
  # FROM          # Base image
  # LABEL         # Metadata
  # RUN           # Execute commands
  # COPY/ADD     # Copy files
  # WORKDIR       # Set working directory
  # CMD/ENTRYPOINT # Default command

  # Example 1: Simple Python App
  FROM python:3.9-slim

  LABEL maintainer="your-email@example.com"
  LABEL version="1.0"
  LABEL description="Simple Python web application"

  WORKDIR /app

  COPY requirements.txt .
  RUN pip install -r requirements.txt

  COPY . .

  EXPOSE 8080

  CMD ["python", "app.py"]

  # Example 2: Multi-stage Build
  # Build stage
  FROM node:16-alpine AS builder

  WORKDIR /app
  COPY package*.json ./
  RUN npm install
  COPY . .
  RUN npm run build

  # Production stage
  FROM nginx:alpine

  COPY --from=builder /app/dist /usr/share/nginx/html
  COPY nginx.conf /etc/nginx/nginx.conf

  EXPOSE 80

  CMD ["nginx", "-g", "daemon off;"]

  # Example 3: Custom Base Image
  FROM ubuntu:20.04

  LABEL maintainer="devops@example.com"

  # Install system dependencies
  RUN apt-get update && apt-get install -y \
  curl \
  wget \
  git \
  vim \
  && rm -rf /var/lib/apt/lists/*

  # Create non-root user
  RUN useradd -m -s /bin/bash developer
  USER developer

  WORKDIR /home/developer

  # Set default command
  CMD ["/bin/bash"]

  # Build Commands:
  # Build image with tag:
  docker build -t myapp:1.0 .

  # Build without cache:
  docker build --no-cache -t myapp:1.0 .

  # Build with build arguments:
  docker build --build-arg VERSION=1.0 -t myapp:$VERSION .

  # Use build-arg in Dockerfile:
  ARG VERSION
  ENV APP_VERSION=$VERSION

  # Push to Registry:
  docker tag myapp:1.0 your-registry/myapp:1.0
  docker push your-registry/myapp:1.0
</code></pre>
</div>

</div>

<h2 id="terraform-from-scratch">27. TERRAFORM IAC - FROM SCRATCH</h2>

<div>
  <h3>🏗️ Terraform Infrastructure as Code - From Scratch to Hero</h3>

  <div class="command-block">
    <h4>📚 Chapter 1: Terraform Fundamentals (Absolute Beginner)</h4>
    <pre><code>
    # What is Terraform?
    # Terraform = Infrastructure as Code tool
    # Created by HashiCorp (2014)
    # Declarative configuration language

    # Why Learn Terraform for DevOps?
    # 1. Infrastructure as Code: Version control infrastructure
    # 2. Multi-Cloud: Works with AWS, Azure, GCP, etc.
    # 3. State Management: Tracks infrastructure changes
    # 4. Reusability: Modules for common patterns
    # 5. Collaboration: Team-based infrastructure management

    # Terraform in DevOps Ecosystem:
    # - Cloud Provisioning: AWS, Azure, GCP resources
    # - Multi-Cloud: Single tool for multiple providers
    # - CI/CD Integration: Automate infrastructure deployment
    # - Configuration Management: Consistent environments
    # - Cost Management: Track and optimize spending

    # Prerequisites:
    # 1. Basic cloud concepts
    # 2. Understanding of infrastructure components
    # 3. JSON/YAML knowledge
    # 4. Command line experience
    # 5. Practice mindset! 🎯
  </code></pre>
</div>

<div class="command-block">
  <h4>🚀 Chapter 2: Terraform Installation and Setup</h4>
  <pre><code>
  # Step 1: Install Terraform
  # On Linux:
  curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -
  sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"
  sudo apt-get update && sudo apt-get install terraform

  # On macOS:
  brew install terraform

  # On Windows:
  # Download from terraform.io/downloads.html

  # Step 2: Verify Installation
  terraform --version
  terraform version

  # Step 3: Configure Cloud Provider
  # Create provider configuration file
  # Method 1: Environment variables
  export AWS_ACCESS_KEY_ID="your_access_key"
  export AWS_SECRET_ACCESS_KEY="your_secret_key"
  export AWS_DEFAULT_REGION="us-east-1"

  # Method 2: Shared credentials file
  mkdir -p ~/.aws
  cat > ~/.aws/credentials <<EOF
  [default]
  aws_access_key_id = your_access_key
  aws_secret_access_key = your_secret_key

  [dev]
  aws_access_key_id = dev_access_key
  aws_secret_access_key = dev_secret_key
  EOF

  # Method 3: Terraform cloud configuration
  cat > ~/.terraformrc <<EOF
  credentials "app.terraform.io" {
  token = "your_terraform_cloud_token"
  }
  EOF

  # Step 4: Initialize Terraform Project
  mkdir my-project
  cd my-project
  terraform init

  # Step 5: Configure Terraform Backend
  # Local backend (default):
  terraform {
  backend "local" {
  path = "terraform.tfstate"
  }
  }

  # S3 backend (recommended for teams):
  terraform {
  backend "s3" {
  bucket = "my-terraform-state"
  key    = "project/terraform.tfstate"
  region = "us-east-1"
  encrypt = true
  }
  }

  # Step 6: Create Workspace
  terraform workspace new development
  terraform workspace select development
  terraform workspace list
</code></pre>
</div>

<div class="command-block">
  <h4>📝 Chapter 3: Terraform Configuration Language (HCL)</h4>
  <pre><code>
  # HCL = HashiCorp Configuration Language
  # Human-readable configuration language

  # Basic HCL Structure:
  # provider    # Cloud provider configuration
  # resource    # Infrastructure resources
  # variable    # Input variables
  # output     # Output values
  # module     # Reusable components
  # data        # Query existing resources

  # Example 1: Basic AWS EC2 Instance
  provider "aws" {
  region = "us-east-1"
  }

  variable "instance_type" {
  description = "EC2 instance type"
  type        = string
  default     = "t2.micro"
  }

  resource "aws_instance" "web" {
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = var.instance_type
  tags = {
  Name        = "WebServer"
  Environment = "Development"
  }
  }

  output "instance_ip" {
  value = aws_instance.web.public_ip
  }

  # Example 2: Multiple Resources
  resource "aws_instance" "web_servers" {
  count         = 3
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = "t2.micro"

  tags = {
  Name = "WebServer-${count.index}"
  }
  }

  # Example 3: Variables and Interpolation
  variable "app_name" {
  description = "Application name"
  type        = string
  }

  variable "environment" {
  description = "Environment name"
  type        = string
  default     = "development"
  }

  resource "aws_s3_bucket" "app_bucket" {
  bucket = "${var.app_name}-${var.environment}-bucket"
  tags = {
  Name        = "${var.app_name} bucket"
  Environment = var.environment
  }
  }

  # Example 4: Data Sources
  data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"]
  name_filter {
  name   = "ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server"
  }
  }

  resource "aws_instance" "web" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = "t2.micro"
  }

  # Example 5: Conditional Resources
  variable "create_database" {
  description = "Whether to create database"
  type        = bool
  default     = false
  }

  resource "aws_db_instance" "database" {
  count = var.create_database ? 1 : 0

  identifier = "my-database"
  engine     = "mysql"
  instance_class = "db.t2.micro"
  }

  # Example 6: Functions and Built-in Functions
  locals {
  common_tags = {
  Environment = var.environment
  Project     = var.app_name
  ManagedBy   = "Terraform"
  }

  bucket_name = "${var.app_name}-${var.environment}-storage"
  }

  resource "aws_s3_bucket" "storage" {
  bucket = locals.bucket_name
  tags   = locals.common_tags
  }

  # String functions:
  resource "aws_s3_bucket" "logs" {
  bucket = "${lower(var.app_name)}-logs"
  }

  # Numeric functions:
  variable "instance_count" {
  default = 2
  }

  resource "aws_instance" "servers" {
  count = var.instance_count
  # ...
  }
</code></pre>
</div>

<h2 id="python">28. PYTHON FOR DEVOPS - BASICS</h2>

<div>
  <h3>🐍 Python Programming - From Scratch to Hero (Complete Tutorial)</h3>

  <div class="command-block">
    <h4>📚 Chapter 1: What is Python? (Absolute Beginner)</h4>
    <pre><code>
    # What is Python?
    # Python = High-level programming language
    # Created by Guido van Rossum (1991)
    # Named after Monty Python's Flying Circus

    # Why Learn Python for DevOps?
    # 1. Automation: Script repetitive tasks
    # 2. Cloud SDKs: AWS, Azure, GCP use Python
    # 3. Configuration Management: Ansible, Salt use Python
    # 4. Monitoring: Prometheus, Grafana use Python
    # 5. CI/CD: Jenkins, GitHub Actions use Python
    # 6. Data Processing: Log analysis, metrics

    # Python in DevOps Ecosystem:
    # - Infrastructure as Code (Terraform providers)
    # - Container Orchestration (Kubernetes clients)
    # - Configuration Management (Ansible modules)
    # - Monitoring & Alerting (Prometheus exporters)
    # - CI/CD Pipelines (GitHub Actions, Jenkins)
    # - Cloud Automation (Boto3, Azure SDK)

    # Prerequisites:
    # 1. Basic programming concepts (variables, loops, functions)
    # 2. Linux terminal knowledge
    # 3. Text editor (VS Code, PyCharm, vim)
    # 4. Python 3.6+ installed
    # 5. Practice mindset! 🎯
  </code></pre>
</div>

<div class="command-block">
  <h4>🚀 Chapter 2: Your First Python Program (Step-by-Step)</h4>
  <pre><code>
  # Step 1: Create your first Python file
  # Open terminal and type:
  nano hello.py

  # Step 2: Write your first Python program
  #!/usr/bin/env python3
  # This is my first Python program
  print("Hello, World! I'm learning Python!")
  print(f"Today is: {datetime.now().strftime('%Y-%m-%d')}")
  print(f"Current user: {os.getenv('USER', 'Unknown')}")

  # Step 3: Save and exit
  # In nano: Press Ctrl+X, then Y, then Enter

  # Step 4: Run your Python program
  python3 hello.py
  # OR make executable first:
  chmod +x hello.py
  ./hello.py

  # Expected Output:
  # Hello, World! I'm learning Python!
  # Today is: 2025-01-13
  # Current user: your-username

  # Understanding Each Line:
  # #!/usr/bin/env python3 -> Portable shebang
  # import statements           -> Import modules/libraries
  # print()                  -> Display output
  # f-string                  -> Modern string formatting
  # os.getenv()              -> Get environment variables
</code></pre>
</div>

<div class="command-block">
  <h4>📦 Chapter 3: Variables and Data Types</h4>
  <pre><code>
  # Variables store data for later use
  # Python has dynamic typing (no need to declare types)

  # Basic Data Types:
  name = "John"              # String (text)
  age = 25                   # Integer (whole numbers)
  height = 5.8               # Float (decimal)
  is_admin = True             # Boolean (True/False)
  fruits = ["apple", "banana"] # List (array)
  person = {"name": "Alice", "age": 30}  # Dictionary (key-value)

  # Variable Naming Rules:
  # 1. Start with letter or underscore
  # 2. Use letters, numbers, underscores
  # 3. Case-sensitive (myvar != MyVar)
  # 4. No reserved words (if, for, def, class)

  # Good Variable Names:
  user_name = "Bob"
  server_count = 5
  is_connected = True
  api_key = "secret123"
  first_name = "Alice"

  # Bad Variable Names:
  2user = "invalid"        # Starts with number
  user-name = "invalid"      # Contains hyphen
  class = "invalid"         # Reserved keyword
  def = "invalid"           # Reserved keyword

  # Type Conversion (Casting):
  age_str = str(25)          # Integer to string
  height_int = int(5.8)        # Float to integer
  age_float = float("25")        # String to float
  is_admin_str = str(True)      # Boolean to string

  # Dynamic Typing Example:
  data = "123"               # Initially string
  data = int(data)            # Now integer (123)
  data = str(data)            # Back to string ("123")

  # f-string Formatting (Modern Python):
  name = "Alice"
  age = 30
  print(f"Name: {name}, Age: {age}")
  print(f"{name} is {age} years old")

  # Output:
  # Name: Alice, Age: 30
  # Alice is 30 years old
</code></pre>
</div>

<div class="command-block">
  <h4>🔤 Chapter 4: User Input and Interactive Programs</h4>
  <pre><code>
  # Making Python programs interactive

  # Method 1: Basic input()
  name = input("What is your name? ")
  print(f"Hello, {name}!")

  # Method 2: input with prompt and default
  city = input("Enter your city [Dhaka]: ") or "Dhaka"
  print(f"You live in: {city}")

  # Method 3: input with type conversion
  age_str = input("Enter your age: ")
  age = int(age_str)  # Convert string to integer
  print(f"You are {age} years old")

  # Method 4: Multiple inputs
  name = input("Name: ")
  age = input("Age: ")
  city = input("City: ")
  print(f"Name: {name}, Age: {age}, City: {city}")

  # Method 5: Password input (hidden)
  import getpass
  password = getpass.getpass("Enter password: ")
  print("Password received (hidden for security)")

  # Real Example: User Registration
  #!/usr/bin/env python3
  import getpass

  print("=== User Registration ===")
  username = input("Username: ")
  password = getpass.getpass("Password: ")
  email = input("Email: ")

  print("\n=== Registration Summary ===")
  print(f"Username: {username}")
  print(f"Password: {'*' * len(password)}")
  print(f"Email: {email}")
  print("Registration complete!")

  # Input Validation Example:
  def get_age():
  while True:
  try:
  age = int(input("Enter your age (18-100): "))
  if 18 <= age <= 100:
  return age
  else:
  print("Age must be between 18 and 100")
  except ValueError:
  print("Please enter a valid number")

  user_age = get_age()
  print(f"Valid age entered: {user_age}")
</code></pre>
</div>

<div class="command-block">
  <h4>🔀 Chapter 5: Control Flow (Making Decisions)</h4>
  <pre><code>
  # Control flow helps programs make decisions

  # if-else statements:
  age = 20
  if age >= 18:
  print("You are eligible to vote!")
  else:
  print("You are not eligible to vote yet")

  # if-elif-else (Multiple conditions):
  score = 85
  if score >= 90:
  grade = "A+ (Excellent!)"
  elif score >= 80:
  grade = "A (Very Good)"
  elif score >= 70:
  grade = "B (Good)"
  elif score >= 60:
  grade = "C (Average)"
  else:
  grade = "F (Need Improvement)"

  print(f"Grade: {grade}")

  # Comparison Operators:
  # ==  -> Equal to
  # !=  -> Not equal to
  # >   -> Greater than
  # <   -> Less than
  # >=  -> Greater or equal to
  # <=  -> Less or equal to

  # Logical Operators:
  # and -> Both conditions must be True
  # or  -> At least one condition must be True
  # not -> Reverse the condition

  # Example with AND:
  age = 25
  has_license = True
  if age >= 18 and has_license:
  print("Eligible for driving license")
  else:
  print("Not eligible for driving license")

  # Example with OR:
  day = "Saturday"
  if day == "Saturday" or day == "Sunday":
  print("It's weekend! Enjoy!")
  else:
  print("It's weekday. Work hard!")

  # Ternary Operator (Compact if-else):
  age = 20
  status = "Adult" if age >= 18 else "Minor"
  print(f"Status: {status}")

  # Nested if statements:
  age = 25
  has_id = True
  has_license = True

  if age >= 18:
  print("Age check passed")
  if has_id and has_license:
  print("All documents verified")
  print("License approved!")
  else:
  print("Missing documents")
  else:
  print("Too young for license")
</code></pre>
</div>

<div class="command-block">
  <h4>🔄 Chapter 6: Loops (Repeating Actions)</h4>
  <pre><code>
  # Loops help repeat actions efficiently

  # For Loop (Basic):
  print("Counting to 5:")
  for i in range(1, 6):  # range(1, 6) = 1, 2, 3, 4, 5
  print(f"Number: {i}")

  # For Loop with Range:
  print("Counting to 10:")
  for i in range(1, 11):  # 1 to 10
  print(f"Count: {i}")

  # For Loop with List:
  fruits = ["apple", "banana", "orange", "mango"]
  print("My favorite fruits:")
  for fruit in fruits:
  print(f"Fruit: {fruit}")

  # For Loop with Index:
  fruits = ["apple", "banana", "orange"]
  print("Fruits with index:")
  for index, fruit in enumerate(fruits):
  print(f"Index {index}: {fruit}")

  # While Loop (Condition-based):
  print("Countdown from 10:")
  count = 10
  while count > 0:
  print(f"Countdown: {count}")
  count -= 1
  import time
  time.sleep(1)  # Wait 1 second
  print("Blast off! 🚀")

  # While Loop with User Input:
  print("Simple calculator:")
  while True:
  expression = input("Enter expression (or 'quit'): ")
  if expression.lower() == 'quit':
  print("Goodbye!")
  break
  try:
  result = eval(expression)  # Be careful with eval in production!
  print(f"Result: {result}")
  except:
  print("Invalid expression!")

  # Loop Control:
  # break  -> Exit loop immediately
  # continue -> Skip to next iteration

  # Example with continue:
  print("Even numbers from 1 to 10:")
  for i in range(1, 11):
  if i % 2 == 1:  # Skip odd numbers
  continue
  print(f"Even: {i}")

  # Nested Loops:
  print("Multiplication table (1-5):")
  for i in range(1, 6):
  print(f"Table of {i}:")
  for j in range(1, 6):
  product = i * j
  print(f"{i} x {j} = {product}")
  print("---")
</code></pre>
</div>

<div class="command-block">
  <h4>📋 Chapter 7: Functions (Reusable Code)</h4>
  <pre><code>
  # Functions are reusable blocks of code
  # Define once, use many times

  # Basic Function Definition:
  def say_hello():
  print("Hello, World!")

  # Function Call:
  say_hello()

  # Function with Parameters:
  def greet(name, age):
  print(f"Hello, {name}! You are {age} years old.")

  # Function Call with Arguments:
  greet("Alice", 25)
  greet("Bob", 30)

  # Function with Return Value:
  def add_numbers(a, b):
  result = a + b
  return result

  # Using Function Return:
  sum_result = add_numbers(10, 20)
  print(f"Sum of 10 and 20 is: {sum_result}")

  # Function with Default Parameters:
  def backup_file(filename="default.txt", backup_dir="/backup"):
  print(f"Backing up {filename} to {backup_dir}")
  # Backup logic here

  # Function Calls:
  backup_file()                    # Uses defaults
  backup_file("important.txt", "/tmp")  # Custom values

  # Function with Variable Arguments:
  def summarize(*args):
  print(f"Received {len(args)} arguments:")
  for i, arg in enumerate(args):
  print(f"  Arg {i+1}: {arg}")

  summarize("apple", "banana", "orange")

  # Function with Keyword Arguments:
  def create_user(**kwargs):
  print("Creating user with:")
  for key, value in kwargs.items():
  print(f"  {key}: {value}")

  create_user(name="Alice", age=25, city="Dhaka")

  # Advanced Function Examples:

  # 1. File Reader Function:
  def read_file_lines(filename):
  """Read file and return lines"""
  try:
  with open(filename, 'r') as file:
  lines = file.readlines()
  print(f"✅ Read {len(lines)} lines from {filename}")
  return lines
  except FileNotFoundError:
  print(f"❌ File not found: {filename}")
  return []
  except Exception as e:
  print(f"❌ Error reading {filename}: {e}")
  return []

  # 2. JSON Processor Function:
  import json
  def process_json_config(config_file):
  """Process JSON configuration file"""
  try:
  with open(config_file, 'r') as file:
  config = json.load(file)
  print(f"✅ Loaded config with {len(config)} settings")
  return config
  except json.JSONDecodeError as e:
  print(f"❌ Invalid JSON in {config_file}: {e}")
  return {}
  except Exception as e:
  print(f"❌ Error reading {config_file}: {e}")
  return {}

  # 3. System Info Function:
  import platform
  import os

  def get_system_info():
  """Get comprehensive system information"""
  info = {
  'hostname': platform.node(),
  'platform': platform.system(),
  'python_version': platform.python_version(),
  'user': os.getenv('USER', 'Unknown'),
  'current_dir': os.getcwd(),
  'cpu_count': os.cpu_count()
  }

  print("=== System Information ===")
  for key, value in info.items():
  print(f"{key}: {value}")

  return info

  # Usage Examples:
  lines = read_file_lines("/etc/passwd")
  config = process_json_config("config.json")
  system_info = get_system_info()
</code></pre>
</div>

<div class="command-block">
  <h4>🗂️ Chapter 8: File Operations (Reading & Writing)</h4>
  <pre><code>
  # File operations are essential for DevOps automation

  # Writing to Files:
  # Method 1: Basic write
  with open("output.txt", "w") as file:
  file.write("This is line 1\n")
  file.write("This is line 2\n")

  # Method 2: Append to file
  with open("output.txt", "a") as file:
  file.write("This is appended line\n")

  # Method 3: Write multiple lines
  lines = ["Line 1", "Line 2", "Line 3"]
  with open("output.txt", "w") as file:
  file.writelines(line + "\n" for line in lines)

  # Reading Files:
  # Method 1: Read entire file
  with open("input.txt", "r") as file:
  content = file.read()
  print(f"File content: {content}")

  # Method 2: Read line by line
  with open("input.txt", "r") as file:
  for line_num, line in enumerate(file, 1):
  print(f"Line {line_num}: {line.strip()}")

  # Method 3: Read all lines to list
  with open("input.txt", "r") as file:
  lines = file.readlines()
  print(f"Total lines: {len(lines)}")

  # File Operations for DevOps:

  # 1. Configuration File Reader:
  import json
  def read_config(config_file):
  """Read JSON configuration file"""
  try:
  with open(config_file, 'r') as file:
  config = json.load(file)
  print(f"✅ Config loaded from {config_file}")
  return config
  except Exception as e:
  print(f"❌ Error loading config: {e}")
  return {}

  # 2. Log File Writer:
  import datetime
  def write_log(message, log_file="app.log", level="INFO"):
  """Write message to log file with timestamp"""
  timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
  log_entry = f"[{timestamp}] [{level}] {message}\n"

  with open(log_file, "a") as file:
  file.write(log_entry)

  # 3. CSV Processor:
  import csv
  def process_csv_file(csv_file):
  """Process CSV file and return data"""
  data = []
  try:
  with open(csv_file, 'r') as file:
  reader = csv.DictReader(file)
  for row in reader:
  data.append(row)
  print(f"✅ Processed {len(data)} rows from {csv_file}")
  return data
  except Exception as e:
  print(f"❌ Error processing {csv_file}: {e}")
  return []

  # 4. Backup Script:
  import shutil
  import datetime

  def backup_files(source_dir, backup_dir):
  """Backup all files from source to backup directory"""
  timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
  backup_name = f"backup_{timestamp}"
  backup_path = f"{backup_dir}/{backup_name}"

  try:
  shutil.make_archive(backup_path, source_dir, 'zip')
  print(f"✅ Backup created: {backup_path}")
  return backup_path
  except Exception as e:
  print(f"❌ Backup failed: {e}")
  return None

  # Usage Examples:
  config = read_config("config.json")
  write_log("Application started", "app.log", "INFO")
  data = process_csv_file("users.csv")
  backup_path = backup_files("/home/user/documents", "/backup")
</code></pre>
</div>

<div class="command-block">
  <h4>🌐 Chapter 9: Modules and Libraries (Importing Code)</h4>
  <pre><code>
  # Modules extend Python's functionality
  # Import statements make external code available

  # Basic Import:
  import os          # Operating system interface
  import sys         # System-specific parameters
  import datetime     # Date and time functions
  import json         # JSON data handling
  import csv          # CSV file operations

  # Import with Alias:
  import numpy as np      # Common convention
  import pandas as pd     # Data analysis
  import matplotlib.pyplot as plt  # Plotting

  # From Import (Specific items):
  from datetime import datetime, timedelta
  from pathlib import Path
  from collections import defaultdict, Counter

  # Standard Library Examples:

  # 1. OS Module (File Operations):
  import os
  print(f"Current directory: {os.getcwd()}")
  print(f"Files in current dir: {os.listdir()}")

  if not os.path.exists("backup"):
  os.makedirs("backup")
  print("Created backup directory")

  # 2. Sys Module (System Info):
  import sys
  print(f"Python version: {sys.version}")
  print(f"Platform: {sys.platform}")
  print(f"Command line args: {sys.argv}")

  # Exit with error code:
  if len(sys.argv) < 2:
  print("Usage: script.py <filename>")
  sys.exit(1)

  # 3. JSON Module (Configuration):
  import json

  config = {
  "database": {
  "host": "localhost",
  "port": 5432,
  "name": "mydb"
  },
  "api_keys": {
  "service1": "key123",
  "service2": "key456"
  }
  }

  # Write config to file:
  with open("config.json", "w") as file:
  json.dump(config, file, indent=2)

  # Read config from file:
  with open("config.json", "r") as file:
  loaded_config = json.load(file)
  print(f"Database host: {loaded_config['database']['host']}")

  # 4. Datetime Module (Time Operations):
  from datetime import datetime, timedelta

  now = datetime.now()
  print(f"Current time: {now}")
  print(f"Formatted: {now.strftime('%Y-%m-%d %H:%M:%S')}")

  # Time calculations:
  future = now + timedelta(days=30)
  print(f"30 days later: {future.strftime('%Y-%m-%d')}")

  # DevOps Module Examples:

  # 1. Pathlib (Modern File Paths):
  from pathlib import Path

  # Create Path objects:
  config_path = Path("/etc/myapp/config.json")
  log_path = Path("/var/log/myapp.log")
  backup_path = Path("/backup/myapp")

  # Path operations:
  print(f"Config exists: {config_path.exists()}")
  print(f"Parent dir: {config_path.parent}")
  print(f"File extension: {config_path.suffix}")

  # Create directories:
  backup_path.mkdir(parents=True, exist_ok=True)

  # 2. Requests Module (HTTP Client):
  import requests

  def check_website(url):
  """Check if website is accessible"""
  try:
  response = requests.get(url, timeout=5)
  if response.status_code == 200:
  print(f"✅ {url} is accessible")
  return True
  else:
  print(f"❌ {url} returned {response.status_code}")
  return False
  except requests.RequestException as e:
  print(f"❌ Error accessing {url}: {e}")
  return False

  # 3. Subprocess Module (System Commands):
  import subprocess

  def run_command(command):
  """Execute system command and return output"""
  try:
  result = subprocess.run(
  command,
  shell=True,
  capture_output=True,
  text=True,
  timeout=30
  )
  print(f"✅ Command executed: {command}")
  print(f"Return code: {result.returncode}")
  print(f"Output: {result.stdout}")
  return result
  except subprocess.TimeoutExpired:
  print(f"❌ Command timed out: {command}")
  return None
  except Exception as e:
  print(f"❌ Command failed: {e}")
  return None

  # Usage Examples:
  check_website("https://www.google.com")
  run_command("ls -la /home")
  run_command("docker ps")
</code></pre>
</div>

<div class="command-block">
  <h4>🐍 Chapter 10: Error Handling and Debugging</h4>
  <pre><code>
  # Error handling makes programs robust
  # Debugging helps find and fix issues

  # Try-Except Blocks:
  try:
  result = 10 / 0
  print(f"Result: {result}")
  except ZeroDivisionError:
  print("❌ Cannot divide by zero!")
  except Exception as e:
  print(f"❌ Unexpected error: {e}")
  finally:
  print("Cleanup code runs here")

  # Multiple Exception Types:
  try:
  # File operations
  with open("nonexistent.txt", "r") as file:
  content = file.read()
  except FileNotFoundError:
  print("❌ File not found")
  except PermissionError:
  print("❌ Permission denied")
  except Exception as e:
  print(f"❌ General error: {e}")

  # Custom Exception:
  class CustomError(Exception):
  """Custom exception for specific errors"""
  pass

  def validate_age(age):
  if age < 0:
  raise CustomError("Age cannot be negative")
  if age > 150:
  raise CustomError("Age seems unrealistic")
  return True

  try:
  validate_age(-5)
  except CustomError as e:
  print(f"❌ Validation error: {e}")

  # Logging for Debugging:
  import logging

  # Configure logging:
  logging.basicConfig(
  level=logging.INFO,
  format='%(asctime)s - %(levelname)s - %(message)s',
  handlers=[
  logging.FileHandler('app.log'),
  logging.StreamHandler()
  ]
  )

  # Logging usage:
  logging.info("Application started")
  logging.warning("This is a warning")
  logging.error("Something went wrong!")
  logging.debug("Debug information")

  # Debugging Techniques:

  # 1. Print Statements:
  def calculate_total(items):
  print(f"DEBUG: Input items: {items}")
  total = sum(items)
  print(f"DEBUG: Calculated total: {total}")
  return total

  # 2. Assert Statements:
  def divide(a, b):
  assert b != 0, "Divisor cannot be zero"
  result = a / b
  return result

  # 3. pdb Debugger:
  import pdb

  def complex_function(data):
  pdb.set_trace()  # Start debugging here
  processed = []
  for item in data:
  processed_item = item.upper()
  processed.append(processed_item)
  return processed

  # Usage:
  # Run with: python -m pdb script.py
  # OR add: pdb.set_trace() in code

  # 4. Exception Chaining:
  try:
  with open("config.json", "r") as file:
  config = json.load(file)
  except FileNotFoundError as e:
  raise ValueError(f"Config file missing: {e}") from e
  except json.JSONDecodeError as e:
  raise ValueError(f"Invalid JSON format: {e}") from e

  # DevOps Error Handling Example:

  # Robust File Operations:
  def safe_file_read(filename):
  """Safely read file with comprehensive error handling"""
  try:
  with open(filename, 'r') as file:
  content = file.read()
  print(f"✅ Successfully read {filename}")
  return content
  except FileNotFoundError:
  logging.error(f"File not found: {filename}")
  return None
  except PermissionError:
  logging.error(f"Permission denied: {filename}")
  return None
  except Exception as e:
  logging.error(f"Unexpected error reading {filename}: {e}")
  return None

  # Usage:
  content = safe_file_read("important.txt")
  if content:
  print("File processed successfully")
  else:
  print("Failed to read file")
</code></pre>
</div>

<h3>Python Execution</h3>

<div class="command-block">
  <h4>Run Python</h4>
  <pre><code>
  # Python Execution (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Basic Python Operations
  python3 script.py                                               # Execute script
  python3 -c "print('Hello')"                                     # Execute command
  python3 -m module                                               # Run module as script
  python3 -i script.py                                            # Interactive mode after script</code></pre>
</div>

<div class="command-block">
  <h4>Shebang for Python scripts</h4>
  <pre><code>
  # Python Shebang (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Portable Shebang
  #!/usr/bin/env python3           # Portable shebang</code></pre>
</div>

<h3>pip (Package Manager)</h3>

<div class="command-block">
  <h4>pip operations - Enterprise Package Management</h4>
  <pre><code>
  # Enterprise Python Package Management Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Package Installation (Enterprise Best Practices)
  pip3 --version                                                  # Verify pip version (Security compliance)
  pip3 install --user package                                      # User-level install (No sudo required)
  pip3 install package==1.2.3                                     # Pin specific version (Stability)
  pip3 install 'package>=1.2,<2.0'                                 # Version constraints (Security)
  pip3 install -r requirements.txt                                # Install from requirements file (Reproducibility)
  pip3 install --upgrade package                                  # Upgrade with dependency resolution
  pip3 uninstall package -y                                       # Uninstall without confirmation (Automation)

  # Package Auditing and Security
  pip3 list --format=columns                                     # Formatted package listing
  pip3 list --outdated                                            # Identify outdated packages (Security)
  pip3 audit requirements.txt                                    # Security vulnerability scan
  pip3 show package                                               # Package metadata and dependencies
  pip3 freeze --exclude-package package                           # Exclude specific packages
  pip3 freeze > requirements.txt                                  # Export requirements (Portability)
  pip3 cache purge                                                # Clear cache (Security compliance)

  # Enterprise Search and Discovery
  pip3 search package --verbose                                   # Detailed search (Deprecated in newer versions)
  pip index versions package                                       # Check available versions</code></pre>
</div>
<h3>Poetry (Modern dependency management)</h3>

<div class="command-block">
  <h4>Poetry commands - Enterprise Dependency Management</h4>
  <pre><code>
  # Poetry: Modern Python Dependency Management (Enterprise Standard)
  # Supports: Red Hat, Debian, Ubuntu, openSUSE, Arch Linux

  # Installation (Enterprise Security)
  curl -sSL https://install.python-poetry.org | python3 -        # Official installer (Verify checksum)
  # Alternative: pip install poetry (Package manager)

  # Project Management (Enterprise Standards)
  poetry new myproject --src                                       # Create project with src layout
  poetry init --no-interaction                                     # Non-interactive initialization
  poetry config virtualenvs.create false                           # Use system Python (Container environments)

  # Dependency Management (Enterprise Compliance)
  poetry add package                                              # Add production dependency
  poetry add --group dev pytest                                    # Add development dependency
  poetry add package@^1.2.0                                       # Version constraint (Caret range)
  poetry add package --optional                                   # Optional dependency
  poetry remove package                                           # Remove dependency cleanly

  # Environment Management (Enterprise)
  poetry env info                                                 # Show virtual environment info
  poetry env use python3.9                                        # Use specific Python version
  poetry env remove python3.9                                     # Remove environment
  poetry shell                                                    # Activate virtual environment
  poetry run python script.py                                     # Execute in virtual environment
  poetry run -- python script.py --arg                            # Pass arguments

  # Project Operations (Enterprise)
  poetry install --no-dev                                         # Install production only
  poetry install --with dev                                       # Install with dev dependencies
  poetry update                                                   # Update all dependencies
  poetry update package                                           # Update specific package
  poetry lock --no-update                                         # Generate lockfile without updates

  # Export and Integration (Enterprise)
  poetry export -f requirements.txt --output requirements.txt     # Export to pip format
  poetry export -f requirements.txt --with dev                    # Include dev dependencies
  poetry build                                                    # Build wheel and sdist
  poetry publish --username token --password __token__          # Publish to PyPI

  # Enterprise Diagnostics
  poetry show --tree                                              # Dependency tree visualization
  poetry check                                                    # Validate pyproject.toml
  poetry version                                                 # Show current version
  poetry version patch                                            # Bump patch version</code></pre>
</div>

<h3>pipenv (Alternative dependency management)</h3>

<div class="command-block">
  <h4>pipenv commands - Enterprise Alternative</h4>
  <pre><code>
  # Pipenv: Python Virtual Environment Management (Enterprise Alternative)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Installation and Setup (Enterprise)
  pip install --user pipenv                                       # User-level installation
  pipenv --version                                                # Verify installation

  # Environment Management (Enterprise Standards)
  pipenv install                                                  # Install from Pipfile
  pipenv install --deploy                                         # Deploy mode (Exact versions)
  pipenv install --dev                                          # Include development dependencies
  pipenv install package                                          # Add package to Pipfile
  pipenv install --dev pytest                                     # Add development dependency
  pipenv uninstall package                                        # Remove package and clean up
  pipenv uninstall --all                                         # Remove all packages

  # Virtual Environment Operations (Enterprise)
  pipenv shell                                                    # Activate virtual shell
  pipenv run python script.py                                     # Execute command in environment
  pipenv run -- python script.py --arg                            # Pass arguments
  pipenv --rm                                                     # Remove virtual environment

  # Dependency Management (Enterprise Compliance)
  pipenv lock                                                     # Generate Pipfile.lock
  pipenv lock --requirements                                      # Export to requirements.txt
  pipenv lock --clear                                             # Clear cache before locking
  pipenv graph                                                    # Show dependency graph
  pipenv check                                                    # Check for security vulnerabilities
  pipenv sync                                                     # Sync Pipfile.lock

  # Enterprise Diagnostics
  pipenv --where                                                  # Show project path
  pipenv --venv                                                    # Show virtual environment path
  pipenv --py                                                     # Show Python interpreter path</code></pre>
</div>

<h3>Virtual Environments</h3>
<div class="command-block">
  <h4>Why use virtual environments?</h4>
  <pre><code>
  # Virtual Environments (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Benefits of Virtual Environments
  # Virtual environments isolate project dependencies
  # Benefits:
  # - Prevents conflicts between different project requirements
  # - Each project can have its own package versions
  # - Clean separation of development environments
  # - Easy to replicate environments across systems</code></pre>
</div>

<div class="command-block">
  <h4>venv (built-in) - Enterprise Virtual Environment</h4>
  <pre><code>
  # Python Virtual Environment (Enterprise Standard)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Environment Creation (Enterprise Best Practices)
  python3 -m venv myenv                                           # Create virtual environment
  python3 -m venv --system-site-packages myenv                  # Include system packages
  python3 -m venv --copies myenv                                 # Copy Python binaries (Container compatibility)
  python3 -m venv --clear myenv                                 # Clear existing environment

  # Environment Activation (Multi-Platform)
  source myenv/bin/activate                                       # Linux/Mac/WSL activation
  myenv\Scripts\activate                                         # Windows CMD activation
  myenv\Scripts\Activate.ps1                                     # Windows PowerShell activation

  # Environment Management
  deactivate                                                     # Deactivate current environment
  which python                                                   # Verify Python path in environment
  python --version                                               # Check Python version in environment

  # Enterprise Environment Configuration
  python3 -m venv --prompt="myproject" myenv                    # Custom prompt
  python3 -m venv --upgrade-deps myenv                          # Upgrade pip and setuptools</code></pre>
</div>

<div class="command-block">
  <h4>Check if in virtual environment - Enterprise Verification</h4>
  <pre><code>
  # Virtual Environment Verification (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Environment Check
  echo $VIRTUAL_ENV                                               # Show virtual environment path
  echo $CONDA_DEFAULT_ENV                                         # Show Conda environment
  which python                                                   # Verify Python executable path
  python -c "import sys; print(sys.prefix)"                    # Show Python installation prefix
  python -c "import site; print(site.getsitepackages())"        # Show site packages location

  # Environment Diagnostics (Enterprise)
  python -c "import venv; print(venv.__file__)"                  # Verify venv module
  python -m pip list --format=json | jq '.[] | .name'            # List packages in JSON format
  python -c "import sys; print('Virtual:' if sys.prefix != sys.base_prefix else 'System')"  # Check if virtual</code></pre>
</div>

<div class="command-block">
  <h4>virtualenv (alternative) - Enterprise Enhanced</h4>
  <pre><code>
  # Virtualenv: Enhanced Virtual Environment (Enterprise Alternative)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Installation and Setup (Enterprise)
  pip3 install --user virtualenv                                 # User-level installation
  virtualenv --version                                            # Verify installation

  # Environment Creation (Enterprise Features)
  virtualenv myenv                                                # Create with current Python
  virtualenv -p python3.9 myenv                                   # Specific Python version
  virtualenv -p /usr/bin/python3.8 myenv                          # Full Python path
  virtualenv --system-site-packages myenv                        # Include system packages
  virtualenv --no-site-packages myenv                            # Isolated environment
  virtualenv --copies myenv                                       # Copy Python (Container compatibility)
  virtualenv --clear myenv                                        # Clear existing environment

  # Advanced Configuration (Enterprise)
  virtualenv --prompt="myproject" myenv                          # Custom prompt
  virtualenv --python=python3.9 --copies myenv                   # Specific version with copies
  virtualenv --seed myenv                                         # Seed with pip and setuptools

  # Environment Management (Enterprise)
  virtualenv --relocatable myenv                                 # Make relocatable (Docker)
  virtualenv --no-wheel myenv                                    # Skip wheel installation
  virtualenv --setuptools myenv                                  # Use setuptools instead of pip</code></pre>
</div>

<h3>Python Interactive</h3>

<div class="command-block">
  <h4>Python REPL - Enterprise Interactive Environment</h4>
  <pre><code>
  # Python Interactive Shell (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Basic REPL Operations (Enterprise)
  python3                                                         # Start interactive Python
  exit()                                                          # Exit REPL gracefully
  quit()                                                          # Alternative exit method
  Ctrl+D                                                          # Exit REPL (EOF signal - Linux/Mac)
  Ctrl+Z                                                          # Suspend process (Linux/Mac)

  # Enterprise REPL Features
  help()                                                          # Interactive help system
  dir()                                                           # List current namespace
  dir(__builtins__)                                                # List built-in functions
  vars()                                                          # Return __dict__ of current module
  import readline; readline.get_history_size()                    # Check history size
  import readline; readline.get_history_item(1)                   # Get specific history item
  Ctrl+P                                                          # Previous history (Linux/Mac)
  Ctrl+N                                                          # Next history (Linux/Mac)

  # Enterprise Environment Inspection
  import sys; sys.version                                         # Python version
  import sys; sys.platform                                        # Platform information
  import sys; sys.path                                           # Python path
  import os; os.getcwd()                                          # Current working directory

  # REPL Debugging (Enterprise)
  import pdb; pdb.set_trace()                                     # Start debugger
  import traceback; traceback.print_exc()                         # Print last exception
  import inspect; inspect.getsource(object)                       # View source code

  # Enterprise REPL Configuration
  # ~/.pythonrc                                                    # REPL startup file
  # export PYTHONSTARTUP=~/.pythonrc                              # Auto-load startup file

  # Enterprise IPython Features
  # Tab completion for commands and filenames
  # Magic commands: %time, %debug, %history
  # Object introspection: dir(), help(), type()
  # Command history with search functionality
  # Syntax highlighting and error highlighting
  # Integration with Jupyter notebooks
  # Customizable prompts and color schemes
  # Extension system for additional functionality

  # Enterprise Best Practices
  # Use virtual environments for project isolation
  # Configure IPython for enterprise workflows
  # Set up proper PYTHONPATH for module discovery
  # Use IPython profiles for different project configurations
  # Enable auto-reload for development efficiency
  # Configure magic commands for productivity enhancement
  # Use IPython's embedded shell integration for system commands
  # Set up logging for debugging and audit trails
  # Configure secure execution policies for production environments

  # Enterprise Script Execution
  # Execute Python scripts with proper error handling
  # Use subprocess module for system command integration
  # Implement proper logging and monitoring
  # Configure execution environments for security compliance
  # Use context managers for resource management
  # Implement proper exception handling and recovery

  # Enterprise Configuration Management
  # Use IPython configuration files for persistent settings
  # Configure profiles for different development environments
  # Set up automatic module reloading for development
  # Configure custom magic commands for productivity
  # Implement proper logging and audit trails

  # Enterprise Security Considerations
  # Validate input before execution in production
  # Use restricted execution policies for security
  # Implement proper sandboxing for untrusted code
  # Configure secure file system access controls
  # Set up monitoring and alerting for security events
  # Use proper authentication and authorization mechanisms
  # Implement code signing and verification processes</code></pre>
</div>

<div class="command-block">
  <h4>IPython (enhanced REPL) - Enterprise Interactive Python</h4>
  <pre><code>
  # IPython: Enhanced Interactive Python (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Installation and Setup (Enterprise)
  pip3 install --user ipython                                    # User-level installation
  pip3 install --user ipython[notebook]                         # Include Jupyter support
  pip3 install --user ipython[all]                              # All optional dependencies

  # Basic IPython Operations (Enterprise)
  ipython                                                         # Start IPython
  ipython --profile=enterprise                                  # Use specific profile
  ipython --quick                                                 # Fast startup (no banner)
  ipython --matplotlib inline                                    # Enable matplotlib integration

  # IPython Magic Commands (Enterprise)
  %lsmagic                                                       # List all magic commands
  %timeit expression                                            # Time execution
  %%timeit                                                        # Time cell execution
  %who                                                           # List variables
  %whos                                                          # List variables with details
  %history                                                       # Command history
  %edit filename                                                # Edit file in editor
  %run script.py                                                # Run Python script
  %load filename                                                 # Load file into cell
  %paste                                                         # Paste clipboard content

  # Enterprise IPython Configuration
  ipython profile create enterprise                              # Create profile
  ipython locate profile enterprise                             # Find profile location
  # ~/.ipython/profile_enterprise/ipython_config.py              # Profile configuration

  # IPython Extensions (Enterprise)
  %load_ext autoreload                                           # Auto-reload modules
  %autoreload 2                                                  # Reload all modules
  %load_ext storemagic                                           # Store variables between sessions
  %store variable_name                                          # Save variable
  %store -r variable_name                                       # Restore variable

  # Enterprise Debugging and Inspection
  %debug                                                         # Enter post-mortem debugger
  %pdb                                                           # Enable pdb on exception
  %xmode verbose                                                # Detailed exception traces
  %precision 3                                                   # Set floating point precision

  # IPython Notebook Integration (Enterprise)
  jupyter notebook                                              # Start Jupyter notebook
  jupyter lab                                                    # Start JupyterLab
  jupyter --version                                             # Check Jupyter version</code></pre>
</div>

<div class="command-block">
  <h4>Run Python script from REPL - Enterprise Script Execution</h4>
  <pre><code>
  # Python Script Execution from REPL (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Basic Script Execution (Enterprise)
  exec(open('script.py').read())                                 # Execute script in current namespace
  exec(open('script.py').read(), globals())                      # Execute with global namespace
  exec(open('script.py').read(), locals())                       # Execute with local namespace

  # Advanced Script Execution (Enterprise)
  with open('script.py', 'r') as f: exec(f.read())               # Safe file handling with context manager
  exec(open('script.py').read(), {'__name__': '__main__'})       # Set execution context

  # Script Execution with Error Handling (Enterprise)
  try:
  exec(open('script.py').read())
  except Exception as e:
  print(f"Script execution failed: {e}")
  import traceback; traceback.print_exc()

  # Enterprise Module Import and Execution
  import importlib.util
  spec = importlib.util.spec_from_file_location("module_name", "script.py")
  module = importlib.util.module_from_spec(spec)
  spec.loader.exec_module(module)                                 # Execute as module

  # Dynamic Script Execution (Enterprise)
  code = compile(open('script.py').read(), 'script.py', 'exec')
  exec(code)                                                      # Execute compiled code

  # Enterprise Security Considerations
  # WARNING: exec() can execute arbitrary code - use with caution
  # Alternative: subprocess.run(['python3', 'script.py']) for isolation
  # Alternative: import script for module execution

  # Script Execution with Namespace Control (Enterprise)
  script_globals = {}
  with open('script.py', 'r') as f:
  exec(f.read(), script_globals)
  # Access script variables: script_globals['variable_name']

  # Enterprise Debugging
  import pdb; pdb.run('exec(open("script.py").read())')         # Debug script execution</code></pre>
</div>
</div>

<h2 id="git">29. GIT VERSION CONTROL</h2>

<div>
  <h3>Git Configuration</h3>

  <div class="command-block">
    <h4>User configuration</h4>
    <pre><code>
    # Git User Configuration (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Basic User Configuration
    git config --global user.name "Your Name"
    git config --global user.email "your@email.com"
    git config --global core.editor vim
    git config --global init.defaultBranch main</code></pre>
  </div>

  <div class="command-block">
    <h4>View configuration</h4>
    <pre><code>
    # Git Configuration Viewing (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Configuration Display
    git config --list                                               # List all config
    git config --global --list                                      # Global config only
    git config user.name                                            # Show specific value
    git config --get user.name                                      # Alternative</code></pre>
  </div>

  <div class="command-block">
    <h4>Configuration levels</h4>
    <pre><code>
    # Git Configuration Levels (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Configuration Scope
    git config --system                                             # System-wide
    git config --global                                             # User-specific
    git config --local                                              # Repository-specific</code></pre>
  </div>

  <div class="command-block">
    <h4>Configuration file locations</h4>
    <pre><code>
    # Git Configuration Files (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Configuration File Locations
    ~/.gitconfig                                                    # Global config
    .git/config                                                     # Local repo config</code></pre>
  </div>

  <h3>Repository Initialization</h3>

  <div class="command-block">
    <h4>Create repository</h4>
    <pre><code>
    # Git Repository Initialization (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Repository Creation and Cloning
    git init                                                        # Initialize new repo
    git init project-name                                           # Initialize with directory name
    git clone url                                                   # Clone remote repository
    git clone url dirname                                           # Clone to specific directory
    git clone --depth 1 url                                         # Shallow clone (last commit only)
    git clone -b branch url                                         # Clone specific branch</code></pre>
  </div>

  <h3>Basic Git Workflow</h3>

  <div class="command-block">
    <h4>Check status</h4>
    <pre><code>
    # Git Status Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Status Checking
    git status                                                      # Show working tree status
    git status -s                                                   # Short status
    git status -sb                                                  # Short status with branch</code></pre>
  </div>

  <div class="command-block">
    <h4>Add files</h4>
    <pre><code>
    # Git Add Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # File Staging
    git add file.txt                                                # Add specific file
    git add .                                                       # Add all changes in current dir
    git add -A                                                      # Add all changes in repo
    git add *.txt                                                   # Add all .txt files
    git add -p                                                      # Interactive staging
    git add -u                                                      # Add modified/deleted, not new</code></pre>
  </div>

  <div class="command-block">
    <h4>Commit</h4>
    <pre><code>
    # Git Commit Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Commit Operations
    git commit                                                      # Commit (opens editor)
    git commit -m "message"                                        # Commit with message
    git commit -am "message"                                       # Add and commit
    git commit --amend                                            # Amend last commit
    git commit --amend --no-edit                                   # Amend without changing message</code></pre>
  </div>

  <div class="command-block">
    <h4>View changes</h4>
    <pre><code>
    # Git Diff Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Change Comparison
    git diff                                                        # Unstaged changes
    git diff --staged                                               # Staged changes
    git diff HEAD                                                   # All changes since last commit
    git diff branch1 branch2                                        # Compare branches
    git diff commit1 commit2                                        # Compare commits
    git diff --stat                                                 # Summary of changes</code></pre>
  </div>

  <div class="command-block">
    <h4>Remove files</h4>
    <pre><code>
    # Git Remove Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # File Removal
    git rm file.txt                                                 # Remove file and stage
    git rm --cached file.txt                                        # Untrack file, keep in working dir
    git rm -r directory                                             # Remove directory</code></pre>
  </div>

  <div class="command-block">
    <h4>Move/rename</h4>
    <pre><code>
    # Git Move/Rename Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # File Movement
    git mv oldname newname                                          # Move or rename file</code></pre>
  </div>

  <h3>Viewing History</h3>

  <div class="command-block">
    <h4>Log</h4>
    <pre><code>
    # Git Log Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Log Viewing
    git log                                                         # Show commit history
    git log --oneline                                               # Compact format
    git log --graph                                                 # Graph view
    git log --all --graph --decorate --oneline                      # Full graph view
    git log -n 5                                                    # Last 5 commits
    git log --since="2024-01-01"                                    # Commits since date
    git log --until="2024-12-31"                                    # Commits until date
    git log --author="name"                                         # By author
    git log --grep="pattern"                                        # Search commit messages
    git log file.txt                                                # Commits affecting file
    git log -p                                                      # Show patches (diffs)
    git log --stat                                                  # Show file statistics</code></pre>
  </div>

  <div class="command-block">
    <h4>Show commit</h4>
    <pre><code>
    # Git Show Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Commit Details
    git show commit_hash                                            # Show commit details
    git show HEAD                                                   # Show latest commit
    git show HEAD~1                                                 # Show previous commit
    git show branch:file.txt                                        # Show file from branch</code></pre>
  </div>

  <div class="command-block">
    <h4>Blame</h4>
    <pre><code>
    # Git Blame Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Line History
    git blame file.txt                                              # Show who changed each line
    git blame -L 10,20 file.txt                                     # Lines 10-20 only</code></pre>
  </div>

  <h3>Branching</h3>

  <div class="command-block">
    <h4>List branches</h4>
    <pre><code>
    # Git Branch Listing (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Branch Display
    git branch                                                      # List local branches
    git branch -a                                                   # List all branches (local + remote)
    git branch -r                                                   # List remote branches
    git branch -v                                                   # Verbose (show last commit)</code></pre>
  </div>

  <div class="command-block">
    <h4>Create branch</h4>
    <pre><code>
    # Git Branch Creation (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Branch Creation
    git branch branch-name                                          # Create branch
    git checkout -b branch-name                                     # Create and switch to branch
    git switch -c branch-name                                       # Create and switch (modern)</code></pre>
  </div>

  <div class="command-block">
    <h4>Switch branches</h4>
    <pre><code>
    # Git Branch Switching (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Branch Navigation
    git checkout branch-name                                        # Switch to branch
    git switch branch-name                                          # Switch to branch (modern)
    git checkout -                                                  # Switch to previous branch</code></pre>
  </div>

  <div class="command-block">
    <h4>Delete branch</h4>
    <pre><code>
    # Git Branch Deletion (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Branch Removal
    git branch -d branch-name                                       # Delete branch (safe)
    git branch -D branch-name                                       # Force delete branch
    git push origin --delete branch-name                            # Delete remote branch</code></pre>
  </div>

  <div class="command-block">
    <h4>Rename branch</h4>
    <pre><code>
    # Git Branch Renaming (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Branch Renaming
    git branch -m new-name                                          # Rename current branch
    git branch -m old new                                           # Rename specific branch</code></pre>
  </div>

  <h3>Merging</h3>

  <div class="command-block">
    <h4>Merge branches</h4>
    <pre><code>
    # Git Merge Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Branch Merging
    git merge branch-name                                           # Merge branch into current
    git merge --no-ff branch-name                                   # No fast-forward merge
    git merge --squash branch-name                                  # Squash commits</code></pre>
  </div>

  <div class="command-block">
    <h4>Abort merge</h4>
    <pre><code>
    # Git Merge Abort (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Merge Cancellation
    git merge --abort                                               # Cancel merge in progress</code></pre>
  </div>

  <div class="command-block">
    <h4>Merge tools</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    it mergetool                                                   # Launch merge tool</code></pre>
  </div>

  <h3>Remote Repositories</h3>

  <div class="command-block">
    <h4>View remotes</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    it remote                                                      # List remotes
    git remote -v                                                   # List with URLs
    git remote show origin                                          # Show remote details</code></pre>
  </div>

  <div class="command-block">
    <h4>Add remote</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    it remote add origin url                                       # Add remote
    git remote add upstream url                                     # Add upstream remote</code></pre>
  </div>

  <div class="command-block">
    <h4>Remove/rename remote</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    it remote remove origin                                        # Remove remote
    git remote rename old new                                       # Rename remote</code></pre>
  </div>

  <div class="command-block">
    <h4>Fetch and pull</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    it fetch                                                       # Fetch from default remote
    git fetch origin                                                # Fetch from specific remote
    git fetch --all                                                 # Fetch from all remotes
    git pull                                                        # Fetch and merge
    git pull --rebase                                               # Fetch and rebase
    git pull origin main                                            # Pull specific branch</code></pre>
  </div>

  <div class="command-block">
    <h4>Push</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    it push                                                        # Push to default remote/branch
    git push origin main                                            # Push to specific remote/branch
    git push -u origin main                                         # Push and set upstream
    git push --all                                                  # Push all branches
    git push --tags                                                 # Push all tags
    git push --force                                                # Force push (DANGEROUS)
    git push --force-with-lease                                     # Safer force push</code></pre>
  </div>

  <h3>Stashing</h3>
  <div class="command-block">
    <h4>When to use git stash</h4>
    <pre><code># Common use cases:
    # 1. Save work-in-progress when switching branches
    # 2. Temporarily save uncommitted changes
    # 3. Pull latest changes without committing incomplete work
    # 4. Apply saved changes later when ready
    #
    # Example workflow:
    # Working on feature → Need to fix urgent bug → Stash current work
    # → Switch to main → Fix bug → Return to feature → Apply stash

    # Stash commands:
    git stash                                                       # Stash changes
    git stash save "message"                                        # Stash with message
    git stash -u                                                    # Include untracked files
    git stash -a                                                    # Include all files (even ignored)
  </code></pre>
</div>

<div class="command-block">
  <h4>List stashes</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # List all stashes:
  git stash list                                                    # Show all stashes
  git stash list --oneline                                        # Compact format
  git stash list --date=local                                      # Local date format

  # Show specific stash:
  git stash show stash@{0}                                        # Show latest stash
  git stash show stash@{1} --stat                                 # Show file changes only

  # Apply stashes:
  git stash apply                                                   # Apply latest stash
  git stash apply stash@{2}                                       # Apply specific stash
  git stash pop                                                     # Apply and remove latest stash

  # Clear stashes:
  git stash clear                                                  # Remove all stashes
  git stash drop stash@{0}                                        # Remove specific stash
</code></pre>
</div>

<div class="command-block">
  <h4>Apply stash</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  git stash apply                                                 # Apply latest stash
  git stash apply stash@{2}                                       # Apply specific stash
  git stash pop                                                   # Apply and remove latest stash
  git stash pop stash@{2}                                         # Apply and remove specific stash</code></pre>
</div>

<div class="command-block">
  <h4>Manage stashes</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  git stash show                                                  # Show stash changes
  git stash show -p                                               # Show stash diff
  git stash drop                                                  # Delete latest stash
  git stash drop stash@{2}                                        # Delete specific stash
  git stash clear                                                 # Delete all stashes</code></pre>
</div>

<h3>Undoing Changes</h3>

<div class="command-block">
  <h4>Discard changes</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  it restore file.txt                                            # Discard changes in file (modern)
  git restore --staged file.txt                                   # Unstage file (modern)
  git checkout -- file.txt                                        # Discard changes (legacy)
  git reset HEAD file.txt                                         # Unstage file (legacy)</code></pre>
</div>

<div class="command-block">
  <h4>Reset commits</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  it reset --soft HEAD~1                                         # Undo commit, keep changes staged
  git reset --mixed HEAD~1                                        # Undo commit, keep changes unstaged (default)
  git reset --hard HEAD~1                                         # Undo commit, discard changes (DANGEROUS)
  git reset --hard origin/main                                    # Reset to remote state (DANGEROUS)</code></pre>
</div>

<div class="command-block">
  <h4>Revert (create new commit that undoes)</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  it revert commit_hash                                          # Revert specific commit
  git revert HEAD                                                 # Revert last commit
  git revert --no-commit HEAD                                     # Revert without committing</code></pre>
</div>

<div class="command-block">
  <h4>Clean untracked files</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  it clean -n                                                    # Dry run (show what would be deleted)
  git clean -f                                                    # Delete untracked files
  git clean -fd                                                   # Delete untracked files and directories
  git clean -fX                                                   # Delete only ignored files
  git clean -fx                                                   # Delete untracked and ignored</code></pre>
</div>

<h3>Tagging</h3>

<div class="command-block">
  <h4>List tags</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  it tag                                                         # List all tags
  git tag -l "v1.*"                                               # List tags matching pattern</code></pre>
</div>

<div class="command-block">
  <h4>Create tags</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  it tag v1.0.0                                                  # Lightweight tag
  git tag -a v1.0.0 -m "message"                                  # Annotated tag
  git tag -a v1.0.0 commit_hash                                   # Tag specific commit</code></pre>
</div>

<div class="command-block">
  <h4>Show tag</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  it show v1.0.0                                                 # Show tag details</code></pre>
</div>

<div class="command-block">
  <h4>Push tags</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  it push origin v1.0.0                                          # Push specific tag
  git push origin --tags                                          # Push all tags
  git push --follow-tags                                          # Push commit and associated tags</code></pre>
</div>

<div class="command-block">
  <h4>Delete tags</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  it tag -d v1.0.0                                               # Delete local tag
  git push origin --delete v1.0.0                                 # Delete remote tag</code></pre>
</div>

<h3>Rebasing</h3>

<div class="command-block">
  <h4>Rebase</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  it rebase main                                                 # Rebase current branch onto main
  git rebase -i HEAD~3                                            # Interactive rebase last 3 commits
  git rebase --continue                                           # Continue after resolving conflicts
  git rebase --abort                                              # Abort rebase
  git rebase --skip                                               # Skip current commit</code></pre>
</div>

<h3>Git Extras</h3>

<div class="command-block">
  <h4>Submodules</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  it submodule add url path                                      # Add submodule
  git submodule init                                              # Initialize submodules
  git submodule update                                            # Update submodules
  git submodule update --remote                                   # Update to latest
  git clone --recursive url                                       # Clone with submodules</code></pre>
</div>

<div class="command-block">
  <h4>Cherry-pick</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  it cherry-pick commit_hash                                     # Apply specific commit
  git cherry-pick --continue                                      # Continue after conflict
  git cherry-pick --abort                                         # Abort cherry-pick</code></pre>
</div>

<div class="command-block">
  <h4>Bisect (find bad commit)</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  it bisect start                                                # Start bisect
  git bisect bad                                                  # Mark current as bad
  git bisect good commit_hash                                     # Mark commit as good
  git bisect reset                                                # End bisect</code></pre>
</div>

<div class="command-block">
  <h4>Reflog (recover lost commits)</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  it reflog                                                      # Show reflog
  git reflog show branch-name                                     # Reflog for branch
  git reset --hard HEAD@{2}                                       # Restore to reflog entry</code></pre>
</div>

<div class="command-block">
  <h4>Archive</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  it archive --format=zip HEAD > archive.zip                     # Create archive
  git archive --format=tar HEAD | gzip > archive.tar.gz           # Tar.gz archive</code></pre>
</div>
<h3>Advanced Git Workflow & Best Practices</h3>

<div class="command-block">
  <h4>Understanding Advanced Git Concepts</h4>
  <pre><code>
  # Git Internal Architecture:
  # Working Directory → Staging Area (Index) → Local Repository → Remote Repository
  # Local Repository = Working Directory + Index
  # Three Trees in Git:
  # 1. HEAD - Last commit snapshot (current branch pointer)
  # 2. Index - Proposed next commit (staging area)
  # 3. Working Directory - Sandbox (actual files)

  # Git Objects:
  # Blob - File content
  # Tree - Directory structure
  # Commit - Snapshot with metadata
  # Tag - Named reference to commit

  # Refs (References):
  # HEAD - Current branch pointer
  # Branch - Movable pointer to commit
  # Tag - Fixed pointer to commit
  # Remote - Pointer to remote repository state</code></pre>
</div>

<div class="command-block">
  <h4>Rebase vs Merge - Critical Differences</h4>
  <pre><code>
  # MERGE - Preserves history, creates merge commit
  git checkout main
  git merge feature-branch
  # Result: Creates new merge commit, maintains both branch histories
  # Pros:
  #   - Safe (non-destructive)
  #   - Preserves complete history
  #   - Shows collaboration timeline
  # Cons:
  #   - Pollutes history with merge commits
  #   - Harder to read git log
  # Use when: Working on public branches, team collaboration

  # REBASE - Rewrites history, linear timeline
  git checkout feature-branch
  git rebase main
  # Result: Moves entire feature branch to tip of main
  # Pros:
  #   - Clean, linear history
  #   - Easier to understand project progression
  #   - No merge commits
  # Cons:
  #   - Rewrites commit history (dangerous for shared branches)
  #   - Can cause conflicts for team members
  # Use when: Local branches, cleaning up before merge

  # Golden Rule of Rebasing:
  # ⚠️ NEVER rebase commits that have been pushed to public/shared branches

  # Interactive Rebase (Squash, Edit, Reorder commits)
  git rebase -i HEAD~5                                            # Last 5 commits
  # Inside editor:
  # pick abc1234 First commit
  # squash def5678 Second commit (will merge into first)
  # reword ghi9012 Third commit (will edit message)
  # edit jkl3456 Fourth commit (will stop for changes)
  # drop mno7890 Fifth commit (will remove)

  # Rebase Workflow Example:
  git checkout feature-branch
  git fetch origin
  git rebase origin/main                                          # Rebase on latest main
  # If conflicts occur:
  # 1. Fix conflicts in files
  # 2. git add &lt;resolved-files&gt;
  # 3. git rebase --continue
  # To abort: git rebase --abort</code></pre>
</div>


<div class="command-block">
  <h4>Reset vs Revert - Understanding Undo Operations</h4>
  <pre><code>
  # RESET - Moves branch pointer backward (rewrites history)
  # --mixed: Moves HEAD, unstages changes
  # --soft: Moves HEAD, keeps changes staged
  git reset --soft HEAD~1
  # Use case: Undo commit, keep all changes ready to recommit
  # Example: Wrong commit message, want to recommit with better message

  # --mixed (default): Moves HEAD, unstages changes
  git reset HEAD~1                                                # Same as --mixed
  git reset --mixed HEAD~1
  # Use case: Undo commit and staging, keep files modified
  # Example: Committed too many files, want to split into multiple commits

  # --hard: Moves HEAD, discards all changes (DANGEROUS)
  git reset --hard HEAD~1
  # Use case: Completely discard commits and changes
  # Example: Experimental changes didn't work, start fresh
  # ⚠️ WARNING: Cannot undo after --hard reset

  # Reset to specific commit
  git reset --hard abc1234                                        # Reset to commit abc1234
  git reset --hard origin/main                                    # Reset to match remote

  # REVERT - Creates new commit that undoes changes (safe)
  git revert HEAD                                                 # Revert last commit
  git revert abc1234                                              # Revert specific commit
  git revert HEAD~3..HEAD                                         # Revert multiple commits
  # Use case: Undo changes on public/shared branches
  # Result: History preserved, safe for collaboration
  # Example: Bug introduced in production, need to undo safely

  # Comparison Table:
  # Operation  | Changes History | Safe for Public | Use Case
  # -----------|-----------------|-----------------|---------------------------
  # reset      | YES (rewrites)  | NO              | Local cleanup, undo commits
  # revert     | NO (adds new)   | YES             | Undo on shared branches

  # Rule of Thumb:
  # - Local branch → Use reset
  # - Public/shared branch → Use revert</code></pre>
</div>

<div class="command-block">
  <h4>Advanced Conflict Resolution</h4>
  <pre><code>
  # Understanding Conflict Markers:
  # <<<<<<< HEAD
  # Your changes (current branch)
  # =======
  # Their changes (incoming branch)
  # >>>>>>> feature-branch

  # Conflict Resolution Strategies:

  # 1. Manual Resolution (recommended for complex conflicts)
  git merge feature-branch
  # Conflicts appear in files
  # Edit files manually, choose correct code
  git add <resolved-files>
  git commit                                                      # Complete merge

  # 2. Accept Ours (keep current branch changes)
  git merge feature-branch
  git checkout --ours <conflicted-file>                                            # Keep our version
  git add <conflicted-file>
  git commit

  # 3. Accept Theirs (keep incoming branch changes)
  git merge feature-branch
  git checkout --theirs <conflicted-file>                                          # Keep their version
  git add <conflicted-file>
  git commit

  # 4. Merge Tool (visual conflict resolution)
  git mergetool                                                   # Launch merge tool
  # Common tools: vimdiff, kdiff3, meld, Beyond Compare

  # Configure merge tool
  git config --global merge.tool vimdiff
  git config --global mergetool.prompt false

  # Rebase Conflict Resolution:
  git rebase main
  # If conflicts:
  git status                                                      # See conflicted files
  # Fix conflicts
  git add <resolved-files>
  git rebase --continue                                           # Continue rebase
  # OR
  git rebase --skip                                               # Skip current commit
  # OR
  git rebase --abort                                              # Cancel rebase

  # Show Conflicts:
  git diff                                                        # Show conflicts
  git diff --name-only --diff-filter=U                            # List conflicted files
  git diff --check                                                # Check for whitespace issues

  # Prevent Conflicts:
  # 1. Pull frequently (git pull --rebase)
  # 2. Small, focused commits
  # 3. Communicate with team about file changes
  # 4. Use feature branches for isolation</code></pre>
</div>

<div class="command-block">
  <h4>GitFlow Workflow - Professional Branching Model</h4>
  <pre><code>
  # GitFlow Branch Structure:
  #
  # main (production)          ← Production-ready code
  #   ↑
  # release/v1.2               ← Release preparation
  #   ↑
  # develop (integration)      ← Latest development
  #   ↑
  # feature/login              ← Feature development
  # feature/payment
  #
  # hotfix/critical-bug        → Urgent production fixes (branches from main)

  # Branch Types:

  # 1. main/master - Production branch
  #    - Always deployable
  #    - Tagged with version numbers
  #    - No direct commits (only merges)

  # 2. develop - Integration branch
  #    - Latest development code
  #    - Base for feature branches
  #    - Continuous integration target

  # 3. feature/* - Feature development
  #    - Branch from: develop
  #    - Merge back to: develop
  #    - Naming: feature/feature-name
  git checkout develop
  git checkout -b feature/user-authentication
  # Work on feature
  git checkout develop
  git merge --no-ff feature/user-authentication                   # --no-ff preserves branch history
  git branch -d feature/user-authentication

  # 4. release/* - Release preparation
  #    - Branch from: develop
  #    - Merge to: main and develop
  #    - Naming: release/v1.2.0
  git checkout develop
  git checkout -b release/v1.2.0
  # Bug fixes, version bumps

  # Git Checkout Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Branch Switching
  git checkout main
  git merge --no-ff release/v1.2.0
  git tag -a v1.2.0 -m "Release version 1.2.0"
  git checkout develop
  git merge --no-ff release/v1.2.0
  git branch -d release/v1.2.0

  # 5. hotfix/* - Emergency production fixes
  #    - Branch from: main
  #    - Merge to: main and develop
  #    - Naming: hotfix/critical-bug

  # Git Checkout Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Branch Switching
  git checkout main
  git checkout -b hotfix/security-patch
  # Fix critical bug

  # Git Checkout Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Branch Switching
  git checkout main
  git merge --no-ff hotfix/security-patch
  git tag -a v1.2.1 -m "Hotfix security patch"
  git checkout develop
  git merge --no-ff hotfix/security-patch
  git branch -d hotfix/security-patch

  # GitFlow Commands (with git-flow extension):
  git flow init                                                   # Initialize GitFlow
  git flow feature start feature-name                             # Start feature
  git flow feature finish feature-name                            # Finish feature
  git flow release start 1.2.0                                    # Start release
  git flow release finish 1.2.0                                   # Finish release
  git flow hotfix start critical-bug                              # Start hotfix
  git flow hotfix finish critical-bug                             # Finish hotfix

  # Alternative: Trunk-Based Development (Modern approach)
  # - Single main/trunk branch
  # - Short-lived feature branches (< 1 day)
  # - Feature flags for incomplete features
  # - Continuous integration/deployment
  # Better for: Small teams, rapid deployment</code></pre>
</div>

<div class="command-block">
  <h4>Advanced Git Techniques</h4>
  <pre><code>
  # Cherry-Pick - Apply specific commits from another branch
  git cherry-pick abc1234                                                # Apply single commit
  git cherry-pick abc1234 def5678                                     # Multiple commits
  git cherry-pick abc1234..ghi9012                                # Range of commits
  # Use case: Apply bug fix from develop to main without merging all changes

  # Reflog - Recover lost commits (Git's safety net)
  git reflog                                                      # Show reference log
  git reflog show HEAD                                            # HEAD history
  git reflog show feature-branch                                  # Branch history
  # Recover deleted branch:
  git checkout -b recovered-branch HEAD@{5}                       # Restore to specific reflog entry
  # Undo reset --hard:
  git reset --hard HEAD@{1}                                       # Go back one step in reflog

  # Bisect - Find bug-introducing commit (binary search)
  git bisect start                                                # Start bisect
  git bisect bad                                                  # Current commit is bad
  git bisect good abc1234                                         # Known good commit
  # Git will checkout middle commit, test it:
  git bisect good                                                 # If works
  git bisect bad                                                  # If broken
  # Repeat until bug-introducing commit found
  git bisect reset                                                # End bisect session

  # Worktree - Multiple working directories from same repo
  git worktree add ../feature-hotfix hotfix/critical-bug          # Create new worktree
  git worktree list                                               # List all worktrees
  git worktree remove ../feature-hotfix                           # Remove worktree
  # Use case: Work on hotfix while keeping feature branch intact

  # Submodules - Embed repositories within repositories
  git submodule add https://github.com/user/repo.git path/to/submodule
  git submodule init                                              # Initialize submodules
  git submodule update                                            # Update submodules
  git clone --recursive https://github.com/user/repo.git          # Clone with submodules
  # Update all submodules:
  git submodule update --remote --merge

  # Clean - Remove untracked files
  git clean -n                                                    # Dry run (show what would be deleted)
  git clean -f                                                    # Delete untracked files
  git clean -fd                                                   # Delete untracked files and directories
  git clean -fX                                                   # Delete only ignored files
  git clean -fx                                                   # Delete untracked and ignored

  # Blame - Find who changed each line (for debugging)
  git blame file.txt                                              # Show authors of each line
  git blame -L 10,20 file.txt                                     # Specific line range
  git blame -C file.txt                                           # Detect moved/copied lines
  git blame -w file.txt                                           # Ignore whitespace changes

  # Show - Display commit details
  git show abc1234                                                # Show commit details
  git show HEAD~3                                                 # Show 3 commits ago
  git show HEAD:file.txt                                          # Show file at HEAD
  git show branch:path/file.txt                                   # Show file from branch</code></pre>
</div>

<div class="command-block">
  <h4>Git Performance Optimization</h4>
  <pre><code>
  # Garbage Collection
  git gc                                                         # Cleanup unnecessary files
  git gc --aggressive                                            # Aggressive cleanup
  git gc --prune=now                                              # Remove old objects immediately

  # Optimize Repository Size
  git count-objects -vH                                           # Show repo size
  git prune                                                       # Remove unreachable objects
  git repack -a -d --depth=250 --window=250                       # Repack objects efficiently

  # Shallow Clone (faster for large repos)
  git clone --depth 1 https://github.com/user/repo.git            # Only last commit
  git clone --depth 10 https://github.com/user/repo.git           # Last 10 commits
  git fetch --unshallow                                           # Convert to full clone

  # Sparse Checkout (only specific directories)
  git clone --filter=blob:none --sparse https://github.com/user/repo.git
  cd repo
  git sparse-checkout init --cone
  git sparse-checkout set src/ docs/                              # Only checkout these dirs

  # Large File Storage (LFS) - for binary files
  git lfs install                                                 # Install LFS
  git lfs track "*.psd"                                           # Track Photoshop files
  git lfs track "*.mp4"                                           # Track videos
  git lfs ls-files                                                # List tracked files
  git lfs pull                                                    # Download LFS files

  # Commit Signing (GPG) - Verify commit authenticity
  git config --global user.signingkey YOUR_GPG_KEY_ID
  git config --global commit.gpgsign true                         # Sign all commits
  git commit -S -m "Signed commit"                                # Sign specific commit
  git log --show-signature                                        # Verify signatures</code></pre>
</div>

<div class="command-block">
  <h4>Git Hooks - Automation & Enforcement</h4>
  <pre><code>
  # Git Hooks Location
  # Path: .git/hooks/

  # pre-commit Hook
  # Runs before commit (linting, unit tests)
  # File: .git/hooks/pre-commit

  #!/bin/bash
  npm run lint || exit 1                                           # Run linter
  npm test || exit 1                                               # Run tests

  # commit-msg Hook
  # Validates commit message format
  # File: .git/hooks/commit-msg

  #!/bin/bash

  commit_msg=$(cat "$1")                                           # Read commit message

  if ! echo "$commit_msg" | grep -qE "^(feat|fix|docs|style|refactor|test|chore):"; then
  echo "Invalid commit message format. Use: type: description"
  echo "Allowed types: feat, fix, docs, style, refactor, test, chore"
  exit 1
  fi

  # pre-push Hook
  # Runs before push (integration tests)
  # File: .git/hooks/pre-push

  #!/bin/bash
  npm run test:integration || exit 1                                # Run integration tests

  # post-merge Hook
  # Runs after merge (update dependencies)
  # File: .git/hooks/post-merge

  #!/bin/bash
  npm install                                                      # Update packages

  # Make Hooks Executable

  chmod +x .git/hooks/pre-commit                                   # Enable pre-commit hook
  chmod +x .git/hooks/commit-msg                                   # Enable commit-msg hook
  chmod +x .git/hooks/pre-push                                     # Enable pre-push hook
  chmod +x .git/hooks/post-merge                                   # Enable post-merge hook

  # Share Hooks with Team (Recommended: Husky)

  npm install --save-dev husky                                      # Install Husky
  npx husky install                                                # Initialize Husky
  npx husky add .husky/pre-commit "npm test"                       # Add pre-commit hook
  npx husky add .husky/commit-msg "npx commitlint --edit $1"       # Add commit-msg hook
</code></pre>

</div>

<div class="command-block">
  <h4>Git Best Practices - Professional Standards</h4>
  <pre><code>
  # Commit Message Standards (Conventional Commits):
  #
  # Format: <type>(<scope>): <subject>
  #
  # Types:
  # feat: New feature
  # fix: Bug fix
  # docs: Documentation changes
  # style: Code formatting (no logic change)
  # refactor: Code restructuring (no behavior change)
  # test: Add/update tests
  # chore: Build process, dependencies

  # Examples:
  # feat(auth): add JWT authentication
  # fix(api): resolve null pointer exception in user endpoint
  # docs(readme): update installation instructions
  # refactor(database): optimize query performance

  # Commit Best Practices:
  # 1. Small, focused commits (one logical change per commit)
  # 2. Descriptive messages (why, not what)
  # 3. Present tense ("Add feature" not "Added feature")
  # 4. Imperative mood ("Fix bug" not "Fixes bug")
  # 5. Reference issue numbers (fix #123)

  # Branch Naming Conventions:
  # feature/user-authentication
  # bugfix/login-validation
  # hotfix/security-patch
  # release/v1.2.0
  # docs/api-documentation

  # .gitignore Best Practices:
  # Always ignore:
  # - Dependencies (node_modules/, vendor/)
  # - Build artifacts (dist/, build/, target/)
  # - IDE files (.vscode/, .idea/, *.swp)
  # - Environment files (.env, .env.local)
  # - OS files (.DS_Store, Thumbs.db)
  # - Log files (*.log, logs/)
  # - Sensitive data (credentials, keys)

  # Security Best Practices:
  # 1. Never commit secrets (API keys, passwords)
  # 2. Use .gitignore for sensitive files
  # 3. Sign commits with GPG
  # 4. Use HTTPS with tokens (not passwords)
  # 5. Enable branch protection rules
  # 6. Require pull request reviews
  # 7. Regularly update dependencies

  # Performance Best Practices:
  # 1. Use .gitattributes for line endings
  # 2. Use shallow clones for CI/CD
  # 3. Enable Git LFS for large binaries
  # 4. Regular garbage collection
  # 5. Keep commits small (< 1000 lines)

  # Collaboration Best Practices:
  # 1. Pull before push (git pull --rebase)
  # 2. Create feature branches for all changes
  # 3. Use pull requests for code review
  # 4. Squash commits before merging
  # 5. Delete merged branches
  # 6. Communicate breaking changes
  # 7. Document in commit messages</code></pre>
</div>

<div class="command-block">
  <h4>Git Aliases - Productivity Boosters</h4>
  <pre><code>
  # Add to ~/.gitconfig or use git config --global

  [alias]
  # Status & Log
  st = status -sb                                             # Short status
  lg = log --oneline --graph --all --decorate                 # Pretty log
  last = log -1 HEAD --stat                                   # Last commit details

  # Branching
  co = checkout
  cob = checkout -b                                           # Create and checkout
  br = branch
  brd = branch -d                                             # Delete branch

  # Committing
  ci = commit
  ca = commit --amend                                         # Amend last commit
  can = commit --amend --no-edit                              # Amend without editing message

  # Pushing/Pulling
  ps = push
  pl = pull --rebase                                          # Pull with rebase
  pom = push origin main

  # Stashing
  ss = stash save
  sp = stash pop
  sl = stash list

  # Undoing
  undo = reset --soft HEAD~1                                  # Undo last commit
  unstage = reset HEAD --                                     # Unstage files
  discard = checkout --                                       # Discard changes

  # Information
  contributors = shortlog -sn                                 # List contributors
  aliases = config --get-regexp alias                         # List all aliases

  # Advanced
  squash = rebase -i HEAD~5                                   # Interactive rebase last 5
  cleanup = !git branch --merged | grep -v '\\*' | xargs -n 1 git branch -d  # Delete merged branches

  # Usage examples:
  git st                                                          # Instead of git status -sb
  git lg                                                          # Instead of git log --oneline --graph
  git cob feature/new-feature                                     # Create and checkout branch
  git can                                                         # Amend without editing message</code></pre>
</div>

<div class="command-block">
  <h4>Interview Questions - Advanced Git</h4>
  <pre><code>
  # Q1: What's the difference between git reset and git revert?
  # A: reset rewrites history (moves HEAD), revert creates new commit.
  #    Use reset for local changes, revert for public branches.

  # Q2: When would you use git rebase vs git merge?
  # A: Rebase for clean linear history on local branches,
  #    Merge for preserving collaboration history on shared branches.

  # Q3: How do you recover a deleted branch?
  # A: git reflog to find commit, then git checkout -b recovered-branch HEAD@{n}

  # Q4: Explain git cherry-pick use case.
  # A: Apply specific commit from one branch to another without full merge.
  #    Example: Hotfix in develop needs to go to main immediately.

  # Q5: What are git hooks and name 3 use cases?
  # A: Scripts that run at specific Git events.
  #    1. pre-commit: Run linter/tests before commit
  #    2. commit-msg: Validate commit message format
  #    3. pre-push: Run integration tests before push

  # Q6: How do you squash multiple commits?
  # A: git rebase -i HEAD~N (where N is number of commits),
  #    Change 'pick' to 'squash' for commits to combine.

  # Q7: What's the difference between git stash pop and git stash apply?
  # A: pop removes stash after applying, apply keeps it in stash list.

  # Q8: How do you find which commit introduced a bug?
  # A: git bisect - binary search through commit history to find bug.

  # Q9: Explain GitFlow branching model.
  # A: main (production), develop (integration), feature/* (features),
  #    release/* (release prep), hotfix/* (emergency fixes).

  # Q10: How do you handle a merge conflict?
  # A: 1. git status to see conflicts
  #    2. Edit files, remove conflict markers
  #    3. git add <resolved-files>
  #    4. git commit to complete merge</code></pre>
</div>
</div>

<h2 id="docker">30. DOCKER COMMANDS</h2>

<div>
  <h3>Docker System</h3>

  <div class="command-block">
    <h4>Docker version and info - Enterprise System Verification</h4>
    <pre><code>
    # Docker System Information (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Version and System Information (Enterprise Compliance)
    docker --version                                                # Show client version
    docker version                                                  # Detailed client/server versions
    docker info                                                     # System-wide information and configuration
    docker system info                                               # Alternative system info

    # Enterprise System Verification
    docker system df                                                 # Docker space usage
    docker system events                                             # Real-time system events
    docker system prune --dry-run                                   # Preview cleanup before execution

    # Security and Configuration (Enterprise)
    docker context ls                                              # List contexts (Multi-environment)
    docker context inspect default                                 # Inspect current context
    <pre><code>
    # Docker Build Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Build Images
    docker buildx version                                           # BuildKit version (Advanced builds)</code></pre>
  </div>

  <div class="command-block">
    <h4>Docker service (systemd) - Enterprise Service Management</h4>
    <pre><code>
    # Docker Service Management (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Service Control (Enterprise Operations)
    systemctl status docker                                         # Check Docker service status
    systemctl start docker                                          # Start Docker service
    systemctl stop docker                                           # Stop Docker service
    systemctl enable docker                                         # Enable at boot (Persistence)
    systemctl disable docker                                        # Disable at boot
    systemctl restart docker                                        # Restart Docker service
    systemctl reload docker                                         # Reload configuration

    # Enterprise Service Diagnostics
    systemctl is-active docker                                       # Check if service is active
    systemctl is-enabled docker                                      # Check if enabled at boot
    systemctl is-failed docker                                       # Check if service failed
    journalctl -u docker -f                                         # Follow Docker logs
    journalctl -u docker --since "1 hour ago"                      # Recent Docker logs

    # Docker Daemon Configuration (Enterprise)
    # /etc/docker/daemon.json                                        # Main configuration file
    # /etc/systemd/system/docker.service.d/override.conf           # Service overrides
    systemctl daemon-reload                                         # Reload systemd configuration
    systemctl restart docker                                        # Apply configuration changes</code></pre>
  </div>

  <h3>Docker Images</h3>

  <div class="command-block">
    <h4>List images - Enterprise Image Management</h4>
    <pre><code>
    # Docker Image Listing (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Basic Image Listing (Enterprise)
    docker images                                                   # List all images
    docker images -a                                                # List all images (including intermediate)
    docker images -q                                                # List only image IDs
    docker images --digests                                         # Show content digests (Security)
    docker images --format "table {{.Repository}}\t{{.Tag}}\t{{.Size}}"  # Custom format

    # Advanced Image Filtering (Enterprise)
    docker images --filter "dangling=true"                          # Dangling images (cleanup candidates)
    docker images --filter "before=ubuntu:22.04"                   # Images created before
    docker images --filter "since=ubuntu:22.04"                    # Images created after
    docker images --filter "label=environment=production"          # Filter by labels
    docker images --filter "reference=ubuntu:*"                    # Pattern matching

    # Enterprise Image Auditing
    docker images --format "{{.ID}}\t{{.Repository}}\t{{.CreatedAt}}" | head -20  # Recent images
    docker images --format "json" | jq '.[] | select(.Size | contains("GB"))'  # Large images
    docker images --format "{{.Repository}}:{{.Tag}}" | sort | uniq -c | sort -nr  # Image usage

    # Security and Compliance
    docker images --format "{{.ID}}\t{{.RepoTags}}" | xargs -I {} docker inspect {} --format='{{.Id}} {{.RepoTags}} {{.Created}}'  # Detailed audit
    docker images --format "{{.ID}}" | xargs docker history --no-trunc  # Layer analysis</code></pre>
  </div>
  <div class="command-block">
    <h4>Multi-stage builds (Production optimization)</h4>
    <pre><code>
    # Dockerfile example - Multi-stage build
    FROM node:18 AS builder
    WORKDIR /app
    COPY package*.json ./
    RUN npm ci --only=production
    COPY . .
    RUN npm run build

    FROM node:18-alpine
    WORKDIR /app
    COPY --from=builder /app/dist ./dist
    COPY --from=builder /app/node_modules ./node_modules
    EXPOSE 3000
    CMD ["node", "dist/index.js"]

    # Benefits:
    # - Smaller final image (only runtime dependencies)
    # - Faster builds (cached layers)
    # - More secure (no build tools in production image)</code></pre>
  </div>

  <div class="command-block">
    <h4>Pull images</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker pull image:tag                                           # Pull image
    <pre><code>
    # Docker Pull Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Pull Images
    docker pull ubuntu:22.04                                        # Pull specific version
    <pre><code>
    # Docker Pull Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Pull Images
    docker pull --all-tags image                                    # Pull all tags</code></pre>
  </div>

  <div class="command-block">
    <h4>Search images</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    docker search ubuntu                                            # Search Docker Hub
    docker search --limit 5 ubuntu                                  # Limit results</code></pre>
  </div>

  <div class="command-block">
    <h4>Build images</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    docker build -t name:tag .                                      # Build from Dockerfile

    # Docker Build Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Build Images
    docker build -t name:tag -f Dockerfile.prod .                   # Specify Dockerfile

    # Docker Build Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Build Images
    docker build --no-cache -t name:tag .                           # Build without cache

    # Docker Build Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Build Images
    docker build --build-arg VAR=value -t name:tag .                # Build args</code></pre>
  </div>

  <div class="command-block">
    <h4>Remove images</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker rmi image_id                                             # Remove image
    docker rmi image:tag                                            # Remove by tag
    docker rmi -f image_id                                          # Force remove
    docker image prune                                              # Remove unused images
    docker image prune -a                                           # Remove all unused images</code></pre>
  </div>

  <div class="command-block">
    <h4>Image details</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker inspect image_id                                         # Inspect image
    docker history image_id                                         # Show image layers</code></pre>
  </div>

  <div class="command-block">
    <h4>Tag images</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker tag source:tag target:tag                                # Tag image
    docker tag image_id new_name:tag                                # Tag by ID</code></pre>
  </div>

  <div class="command-block">
    <h4>Save/load images</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker save image:tag > image.tar                               # Save to tar
    docker save -o image.tar image:tag                              # Save to file
    docker load < image.tar                                         # Load from tar
    docker load -i image.tar                                        # Load from file</code></pre>
  </div>

  <div class="command-block">
    <h4>Export/import images</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker export container_id > container.tar                      # Export container
    docker import container.tar name:tag                            # Import as image</code></pre>
  </div>

  <h3>Docker Containers</h3>

  <div class="command-block">
    <h4>Run containers</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker run image                                                # Run container
    docker run -d image                                             # Run detached (background)
    docker run -it image bash                                       # Interactive with terminal
    docker run --name mycontainer image                             # Name container
    docker run -p 8080:80 image                                     # Port mapping (host:container)
    docker run -P image                                             # Map all exposed ports
    docker run -v /host:/container image                            # Volume mount
    docker run --rm image                                           # Remove container when stops
    docker run -e VAR=value image                                   # Set environment variable
    docker run --env-file .env image                                # Load env from file
    docker run --network mynet image                                # Connect to network
    docker run --restart always image                               # Restart policy
    docker run --memory 512m image                                  # Memory limit
    docker run --cpus 2 image                                       # CPU limit</code></pre>
  </div>

  <div class="command-block">
    <h4>List containers</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker ps                                                       # List running containers
    docker ps -a                                                    # List all containers
    docker ps -q                                                    # List container IDs only
    docker ps -l                                                    # List last created container
    docker ps --filter "status=exited"                              # Filter by status</code></pre>
  </div>

  <div class="command-block">
    <h4>Container management</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker start container_id                                       # Start container
    docker stop container_id                                        # Stop container (SIGTERM)
    docker stop -t 30 container_id                                  # Stop with 30s timeout
    docker kill container_id                                        # Kill container (SIGKILL)
    docker restart container_id                                     # Restart container
    docker pause container_id                                       # Pause container
    docker unpause container_id                                     # Unpause container
    docker rename old new                                           # Rename container</code></pre>
  </div>

  <div class="command-block">
    <h4>Remove containers</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker rm container_id                                          # Remove container
    docker rm -f container_id                                       # Force remove running container
    docker rm $(docker ps -aq)                                      # Remove all containers
    docker container prune                                          # Remove all stopped containers</code></pre>
  </div>

  <div class="command-block">
    <h4>Container details</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker inspect container_id                                     # Inspect container
    docker logs container_id                                        # View logs
    docker logs -f container_id                                     # Follow logs
    docker logs --tail 100 container_id                             # Last 100 lines
    docker logs --since 10m container_id                            # Last 10 minutes
    docker top container_id                                         # Running processes
    docker stats                                                    # Resource usage (live)
    docker stats --no-stream                                        # Resource usage (snapshot)
    docker port container_id                                        # Port mappings</code></pre>
  </div>

  <div class="command-block">
    <h4>Execute commands</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker exec container_id command                                # Execute command
    docker exec -it container_id bash                               # Interactive bash
    docker exec -u user container_id command                        # As specific user</code></pre>
  </div>

  <div class="command-block">
    <h4>Copy files</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker cp container_id:/path/file ./local                       # Copy from container
    docker cp ./local container_id:/path/                           # Copy to container</code></pre>
  </div>

  <div class="command-block">
    <h4>Attach to container</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker attach container_id                                      # Attach to running container</code></pre>
  </div>

  <h3>Docker Volumes</h3>
  <div class="command-block">
    <h4>Volume Types Explained</h4>
    <pre><code>
    # Named Volume (Docker-managed, recommended for production)
    docker run -v myvolume:/data image                             # Mount volume
    # - Docker manages storage location
    # - Portable across systems
    # - Survives container deletion

    # Bind Mount (Direct host path mapping)
    docker run -v /host/path:/container/path image
    # - Direct access to host filesystem
    # - Useful for development
    # - Changes reflect immediately

    # tmpfs Mount (Stored in memory, temporary)
    docker run --tmpfs /app/temp image
    # - Fast performance
    # - Data lost on container stop</code></pre>
  </div>

  <div class="command-block">
    <h4>List volumes</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker volume ls                                                # List all volumes
    docker volume ls -q                                             # List volume names only
    docker volume ls --filter "dangling=true"                       # Dangling volumes</code></pre>
  </div>

  <div class="command-block">
    <h4>Create volume</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker volume create myvolume                                   # Create named volume
    docker volume create --driver local --opt type=nfs myvolume     # With options</code></pre>
  </div>

  <div class="command-block">
    <h4>Inspect volume</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker volume inspect myvolume                                  # Volume details</code></pre>
  </div>

  <div class="command-block">
    <h4>Remove volumes</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker volume rm myvolume                                       # Remove volume
    docker volume prune                                             # Remove unused volumes</code></pre>
  </div>

  <div class="command-block">
    <h4>Use volumes</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker run -v myvolume:/data image                                  # Named volume
    docker run -v /host/path:/container/path image                      # Bind mount
    docker run --mount type=volume,source=myvolume,target=/data image   # Long syntax</code></pre>
  </div>

  <h3>Docker Networks</h3>

  <div class="command-block">
    <h4>List networks</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker network ls                                               # List all networks
    docker network ls --filter "driver=bridge"                      # Filter by driver</code></pre>
  </div>

  <div class="command-block">
    <h4>Create network</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker network create mynetwork                                 # Create bridge network
    docker network create --driver bridge mynetwork                 # Specify driver
    docker network create --subnet 172.20.0.0/16 mynetwork          # With subnet</code></pre>
  </div>

  <div class="command-block">
    <h4>Inspect network</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker network inspect mynetwork                                # Network details</code></pre>
  </div>

  <div class="command-block">
    <h4>Connect/disconnect</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker network connect mynetwork container_id                   # Connect container
    docker network disconnect mynetwork container_id                # Disconnect</code></pre>
  </div>

  <div class="command-block">
    <h4>Remove networks</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker network rm mynetwork                                     # Remove network
    docker network prune                                            # Remove unused networks</code></pre>
  </div>

  <h3>Docker Compose</h3>

  <div class="command-block">
    <h4>Docker Compose commands</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker-compose --version                                        # Show version
    docker-compose up                                               # Create and start containers
    docker-compose up -d                                            # Detached mode
    docker-compose down                                             # Stop and remove containers
    docker-compose down -v                                          # Also remove volumes
    docker-compose start                                            # Start services
    docker-compose stop                                             # Stop services
    docker-compose restart                                          # Restart services
    docker-compose ps                                               # List containers
    docker-compose logs                                             # View logs
    docker-compose logs -f                                          # Follow logs
    docker-compose logs service                                     # Logs for specific service
    docker-compose exec service command                             # Execute command
    docker-compose build                                            # Build images
    docker-compose build --no-cache                                 # Build without cache
    docker-compose pull                                             # Pull images
    docker-compose config                                           # Validate and view config
    docker-compose scale service=3                                  # Scale service</code></pre>
  </div>
  <div class="command-block">
    <h4>Docker Compose v2 (Modern syntax)</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker compose version                                          # Show Docker Compose version
    <pre><code>
    # Docker Compose Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Multi-Container
    docker compose up -d                                            # Start services in detached mode
    <pre><code>
    # Docker Compose Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Multi-Container
    docker compose down                                             # Stop and remove containers
    <pre><code>
    # Docker Compose Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Multi-Container
    docker compose ps                                               # List containers
    <pre><code>
    # Docker Compose Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Multi-Container
    docker compose logs -f                                          # Follow logs
    <pre><code>
    # Docker Compose Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Multi-Container
    docker compose exec service sh                                  # Execute command in service
    <pre><code>
    # Docker Compose Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Multi-Container
    docker compose build --no-cache                                 # Build without cache
    <pre><code>
    # Docker Compose Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Multi-Container
    docker compose pull                                             # Pull service images
    <pre><code>
    # Docker Compose Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Multi-Container
    docker compose push                                             # Push service images
    <pre><code>
    # Docker Compose Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Multi-Container
    docker compose config                                           # Validate and view config
    <pre><code>
    # Docker Compose Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Multi-Container
    docker compose top                                              # Display running processes</code></pre>
  </div>

  <h3>Docker System Management</h3>

  <div class="command-block">
    <h4>System info</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker system df                                                # Show disk usage
    docker system df -v                                             # Verbose disk usage
    docker system events                                            # Real-time events
    docker system info                                              # System information</code></pre>
  </div>

  <div class="command-block">
    <h4>Cleanup</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker system prune                                             # Remove unused data
    docker system prune -a                                          # Remove all unused data
    docker system prune --volumes                                   # Also remove volumes
    docker system prune -f                                          # Force without prompt</code></pre>
  </div>

  <h3>Docker Registry</h3>

  <div class="command-block">
    <h4>Login/logout</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    docker login                                                    # Login to Docker Hub
    docker login registry.example.com                               # Login to private registry
    docker logout                                                   # Logout</code></pre>
  </div>

  <div class="command-block">
    <h4>Push images</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    docker push image:tag                                           # Push to registry

    # Docker Push Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Push Images
    docker push registry.example.com/image:tag                      # Push to private registry</code></pre>
  </div>

  <div class="command-block">
    <h4>Tag for registry</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker tag local:tag registry.example.com/image:tag</code></pre>
  </div>
  <h3>Advanced Docker - Production Deployment Skills</h3>

  <div class="command-block">
    <h4>Docker Multi-Stage Builds - Image Optimization</h4>
    <pre><code>
    # Why Multi-Stage Builds?
    # 70–90% smaller images
    # Faster deployments
    # More secure (no build tools in production)
    # Better CI/CD performance


    # Single-Stage Build (Not Recommended – ~1.2GB image)

    FROM node:18
    WORKDIR /app
    COPY package*.json ./
    RUN npm install                                                 # Includes dev dependencies
    COPY . .
    RUN npm run build
    EXPOSE 3000
    CMD ["node", "dist/main.js"]


    # Multi-Stage Build (Recommended – ~150MB image)
    # Stage 1: Build

    FROM node:18 AS builder
    WORKDIR /app
    COPY package*.json ./
    RUN npm ci --only=production                                    # Install production deps only
    COPY . .
    RUN npm run build


    # Stage 2: Production

    FROM node:18-alpine                                             # Smaller base image
    WORKDIR /app
    COPY --from=builder /app/dist ./dist
    COPY --from=builder /app/node_modules ./node_modules
    USER node                                                       # Security: run as non-root
    EXPOSE 3000
    CMD ["node", "dist/main.js"]
    # Result: ~150MB (≈87% smaller)


    # Real Example: Go Application (Ultra-Small Image)

    FROM golang:1.21 AS builder
    WORKDIR /build
    COPY go.mod go.sum ./
    RUN go mod download
    COPY . .
    RUN CGO_ENABLED=0 GOOS=linux go build -o main .


    FROM alpine:latest
    RUN apk --no-cache add ca-certificates
    WORKDIR /root/
    COPY --from=builder /build/main .
    EXPOSE 8080
    CMD ["./main"]
    # Result: ~15MB (from ~800MB)


    # Python Flask API (Multi-Stage)

    FROM python:3.11 AS builder
    WORKDIR /app
    COPY requirements.txt .
    RUN pip install --user --no-cache-dir -r requirements.txt


    FROM python:3.11-slim
    WORKDIR /app
    COPY --from=builder /root/.local /root/.local
    COPY . .
    ENV PATH=/root/.local/bin:$PATH
    USER nobody
    CMD ["python", "app.py"]


    # Docker Build Operations (Enterprise Standards)
    # Supported OS: Red Hat, Debian, Ubuntu, openSUSE, Arch


    docker build -t myapp:v1 .                                      # Build full image

    docker build -t myapp:v1 --target builder .                     # Build specific stage only

    docker buildx build \
    --platform linux/amd64,linux/arm64 \
    -t myapp:latest \
    --push .                                                        # Multi-arch build and push
  </code></pre>

</div>

<div class="command-block">
  <h4>Docker Volumes - Production Data Management</h4>
  <pre><code>
  # Volume Types & Use Cases:

  # 1. Named Volume (Production Databases)
  docker volume create pgdata
  docker run -d \
  --name postgres \
  -v pgdata:/var/lib/postgresql/data \
  -e POSTGRES_PASSWORD=secret \
  postgres:15
  # Managed by Docker, survives container deletion

  # 2. Bind Mount (Development)
  docker run -d \
  -v $(pwd)/app:/usr/src/app \
  -v $(pwd)/config.yml:/etc/app/config.yml:ro \
  myapp
  # Direct host access, changes reflect immediately

  # 3. tmpfs (Temporary Caches)
  docker run --tmpfs /app/cache:size=512m myapp
  # RAM-based, fast, auto-cleanup

  # Production Database Setup
  docker run -d \
  --name mysql-prod \
  -e MYSQL_ROOT_PASSWORD=secret \
  -v mysql-data:/var/lib/mysql \
  -v mysql-config:/etc/mysql/conf.d:ro \
  -v mysql-backup:/backup \
  --restart unless-stopped \
  mysql:8.0

  # Backup Volume to Host
  docker run --rm \
  -v mysql-data:/data:ro \
  -v $(pwd)/backups:/backup \
  alpine tar czf /backup/mysql-$(date +%Y%m%d).tar.gz /data

  # Restore from Backup
  docker run --rm \
  -v mysql-data:/data \
  -v $(pwd)/backups:/backup \
  alpine tar xzf /backup/mysql-20250110.tar.gz -C /

  # Volume Management
  docker volume ls
  docker volume inspect mysql-data
  docker volume prune -f               # Remove unused volumes

  # NFS Volume (Shared Storage)
  docker volume create \
  --driver local \
  --opt type=nfs \
  --opt o=addr=192.168.1.100,rw \
  --opt device=:/mnt/nfs \
  nfs-volume

  # Docker Compose with Volumes
  version: '3.8'
  services:
  db:
  image: postgres:15
  volumes:
  - pgdata:/var/lib/postgresql/data
  - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro
  web:
  image: myapp
  volumes:
  - uploads:/app/uploads
  volumes:
  pgdata:
  uploads:</code></pre>
</div>

<div class="command-block">
  <h4>Docker Networks - Microservices Architecture</h4>
  <pre><code>
  # Network Types:

  # Bridge (Default) - Isolated network
  docker network create app-network
  docker run -d --name api --network app-network myapi
  docker run -d --name web --network app-network nginx
  # Containers communicate via name: http://api:8080

  # Host Network - No isolation
  docker run --network host nginx
  # Uses host IP directly, no port mapping

  # None - Completely isolated
  docker run --network none alpine

  # Real-World 3-Tier Application
  # Frontend → API → Database

  # Create networks
  docker network create frontend
  docker network create backend

  # Database (backend only)
  docker run -d \
  --name postgres \
  --network backend \
  -e POSTGRES_PASSWORD=secret \
  postgres:15

  # API (both networks)
  docker run -d \
  --name api \
  --network frontend \
  --network backend \
  -e DATABASE_URL=postgres://postgres:secret@postgres:5432/mydb \
  myapi:v1

  # Web (frontend + exposed)
  docker run -d \
  --name web \
  --network frontend \
  -p 80:80 \
  -e API_URL=http://api:8080 \
  nginx

  # Network Commands
  docker network ls
  docker network inspect app-network
  docker network connect backend redis
  docker network disconnect backend redis

  # Custom Subnet
  docker network create \
  --subnet 172.20.0.0/16 \
  --gateway 172.20.0.1 \
  custom-net

  docker run --network custom-net --ip 172.20.0.10 nginx

  # DNS Resolution Test
  docker run --rm --network app-network alpine ping -c 3 api
  # Should successfully ping 'api' container

  # Network Alias
  docker run -d \
  --network app-network \
  --network-alias database \
  --network-alias db \
  postgres
  # Accessible as: database, db, or container name

  # Docker Compose Networks
  version: '3.8'
  services:
  web:
  networks:
  - frontend
  api:
  networks:
  - frontend
  - backend
  db:
  networks:
  - backend
  networks:
  frontend:
  backend:
  internal: true    # No external access</code></pre>
</div>

<div class="command-block">
  <h4>Resource Limits - Production Stability</h4>
  <pre><code>
  # Why Resource Limits?
  # - Prevent resource exhaustion
  # - Ensure fair resource distribution
  # - Enable capacity planning
  # - Required for Kubernetes migration

  # Memory Limits
  docker run -m 512m nginx                    # Max 512MB
  docker run --memory-reservation 256m nginx  # Soft limit
  docker run -m 1g --memory-swap 2g nginx     # 1GB RAM + 1GB swap

  # CPU Limits
  docker run --cpus=1.5 nginx                 # Max 1.5 cores
  docker run --cpu-shares=512 nginx           # Relative weight
  docker run --cpuset-cpus="0,1" nginx        # Specific cores only

  # Production API Server
  docker run -d \
  --name api-prod \
  --memory 2g \
  --memory-reservation 1g \
  --cpus 4 \
  --restart unless-stopped \
  -p 8080:8080 \
  myapi:v2

  # High-Traffic Web Application
  docker run -d \
  --name web-prod \
  --memory 4g \
  --cpus 8 \
  --pids-limit 200 \
  --restart unless-stopped \
  --health-cmd="curl -f http://localhost/health || exit 1" \
  --health-interval=30s \
  --health-timeout=3s \
  --health-retries=3 \
  -p 80:80 \
  nginx

  # Disk I/O Limits
  docker run \
  --device-read-bps /dev/sda:10mb \
  --device-write-bps /dev/sda:5mb \
  myapp

  # Monitor Resource Usage
  docker stats                                # Live stats
  docker stats --no-stream                    # Snapshot
  docker stats --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.NetIO}}"

  # Update Running Container
  docker update --cpus 2 --memory 1g my-container

  # Docker Compose with Limits
  version: '3.8'
  services:
  api:
  image: myapi
  deploy:
  resources:
  limits:
  cpus: '2'
  memory: 1G
  reservations:
  cpus: '1'
  memory: 512M
  restart: unless-stopped

  # OOM (Out of Memory) Handling
  docker run -m 512m --oom-kill-disable nginx     # Don't kill (dangerous)
  docker run -m 512m --oom-score-adj=-500 nginx   # Less likely to be killed</code></pre>
</div>

<div class="command-block">
  <h4>Container Debugging - Production Troubleshooting</h4>
  <pre><code>
  # Container Won't Start - Debugging Steps:

  # 1. Check Logs (First Step Always)
  docker logs mycontainer
  docker logs --tail 100 mycontainer          # Last 100 lines
  docker logs --since 10m mycontainer         # Last 10 minutes
  docker logs -f mycontainer                  # Follow (live)
  docker logs --timestamps mycontainer        # With timestamps

  # 2. Inspect Container (Full Details)
  docker inspect mycontainer
  docker inspect mycontainer | grep -i status
  docker inspect mycontainer | jq '.[0].State'
  docker inspect --format='{{.State.Status}}' mycontainer
  docker inspect --format='{{.NetworkSettings.IPAddress}}' mycontainer

  # 3. Container Status
  docker ps -a                                            # All containers
  docker ps --filter "status=exited"                      # Failed containers
  docker ps --filter "status=exited" --filter "exited=1"  # Exit code 1

  # 4. Execute Commands in Container
  docker exec -it mycontainer bash            # Interactive shell
  docker exec mycontainer ps aux              # List processes
  docker exec mycontainer netstat -tlnp       # Network connections
  docker exec mycontainer df -h               # Disk space
  docker exec mycontainer env                 # Environment variables

  # 5. Copy Files for Analysis
  docker cp mycontainer:/var/log/app.log ./   # Copy from container
  docker cp config.yml mycontainer:/etc/app/  # Copy to container

  # 6. Check Resource Usage
  docker stats mycontainer                    # Live stats
  docker top mycontainer                      # Running processes

  # 7. Network Debugging
  docker exec mycontainer ping -c 3 google.com
  docker exec mycontainer curl -v http://api:8080/health
  docker exec mycontainer nslookup database

  # 8. Health Check Status
  docker inspect --format='{{.State.Health.Status}}' mycontainer
  docker inspect --format='{{range .State.Health.Log}}{{.Output}}{{end}}' mycontainer

  # Common Issues & Solutions:

  # Issue: Container exits immediately
  docker logs mycontainer                     # Check why
  docker run -it --entrypoint /bin/bash myapp # Override entrypoint

  # Issue: Port already in use
  netstat -tlnp | grep 8080                   # Find what's using port
  docker run -p 8081:8080 myapp               # Use different port

  # Issue: Permission denied
  docker exec mycontainer ls -la /app
  docker run --user $(id -u):$(id -g) myapp   # Run as current user

  # Issue: Out of disk space
  docker system df                            # Check disk usage
  docker system prune -a                      # Cleanup

  # Issue: Network connectivity
  docker network inspect bridge
  docker exec mycontainer cat /etc/resolv.conf
  docker run --dns 8.8.8.8 myapp              # Custom DNS

  # Debug Container that Won't Start
  docker run --rm -it --entrypoint /bin/sh myapp  # Interactive debug
  docker run --rm myapp ls -la /app               # Check files exist

  # Container Performance Issues
  docker stats --no-stream mycontainer          # Check resources
  docker top mycontainer                        # CPU usage per process
  docker exec mycontainer ps aux --sort=-%mem   # Memory hogs

  # Advanced: strace in Container
  docker run --cap-add SYS_PTRACE myapp
  docker exec mycontainer strace -p 1         # Trace PID 1

  # Export Container for Analysis
  docker export mycontainer > container.tar
  tar -tf container.tar | grep -i log         # Find log files

  # Commit Container State for Debug
  docker commit mycontainer debug-image
  docker run -it debug-image /bin/bash        # Inspect saved state

  # Interview Questions - Docker:
  # Q1: Difference between CMD and ENTRYPOINT?
  # A: CMD provides defaults (can be overridden)
  #    ENTRYPOINT is main command (appended to, not replaced)

  # Q2: How to reduce Docker image size?
  # A: Multi-stage builds, alpine base, .dockerignore,
  #    remove cache, combine RUN commands

  # Q3: How to share data between containers?
  # A: Volumes (named or bind mounts), or network shares

  # Q4: Container vs VM?
  # A: Container shares host kernel (lightweight)
  #    VM has full OS (heavier, more isolated)

  # Q5: How to troubleshoot networking?
  # A: docker network inspect, exec into container,
  #    check DNS, ping other containers by name</code></pre>
</div>
</div>

<h2 id="kubernetes">31. KUBERNETES (kubectl) COMMANDS</h2>

<div>
  <h3>Cluster Information</h3>

  <div class="command-block">
    <h4>Cluster info - Enterprise Cluster Management</h4>
    <pre><code>
    # Kubernetes Cluster Information (Enterprise Standards)
    # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

    # Basic Cluster Information (Enterprise)
    kubectl cluster-info                                            # Display cluster endpoint info
    kubectl version                                                 # Client and server version details
    kubectl version --short                                         # Short version format
    kubectl version --client                                       # Client version only
    kubectl version --server                                       # Server version only

    # API and Resources (Enterprise Compliance)
    kubectl api-resources                                           # List all available API resources
    kubectl api-versions                                            # List API versions
    kubectl explain pods                                           # Explain resource schema
    kubectl explain deployment.spec                                # Explain specific field

    # Enterprise Cluster Health
    kubectl get componentstatuses                                   # Cluster component health (legacy)
    kubectl get cs                                                 # Short version of component status
    kubectl cluster-info dump                                      # Dump cluster configuration
    kubectl get nodes -o wide                                       # Node details with IPs
    kubectl top nodes                                              # Node resource usage

    # Enterprise Security and Compliance
    kubectl auth can-i create pods                                 # Check permissions
    kubectl auth can-i get pods --namespace=dev                   # Namespace-specific permissions
    kubectl get events --sort-by=.metadata.creationTimestamp        # Recent cluster events
    kubectl get events --field type=Warning                        # Warning events only</code></pre>
  </div>
  <div class="command-block">
    <h4>Context management (Multi-cluster) - Enterprise Multi-Environment</h4>
    <pre><code>
    # Kubernetes Context Management (Enterprise Standards)
    # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

    # Cluster and Context Management (Enterprise)
    kubectl config get-clusters                                     # List all clusters
    kubectl config get-contexts                                     # List all contexts
    kubectl config current-context                                  # Show current active context
    kubectl config use-context context-name                         # Switch to context

    # Cluster Configuration (Enterprise Setup)
    kubectl config set-cluster cluster-name --server=https://1.2.3.4:6443 --certificate-authority=/path/to/ca.crt  # Add cluster with CA
    kubectl config set-cluster cluster-name --insecure-skip-tls-verify=true  # Development cluster (insecure)
    kubectl config delete-cluster cluster-name                      # Remove cluster

    # Credentials Management (Enterprise Security)
    kubectl config set-credentials user-name --token=bearer_token   # Token-based authentication
    kubectl config set-credentials user-name --username=admin --password=password  # Basic auth
    kubectl config set-credentials user-name --client-certificate=/path/to/cert --client-key=/path/to/key  # Certificate auth
    kubectl config delete-credentials user-name                     # Remove credentials

    # Context Creation and Management (Enterprise)
    kubectl config set-context context-name --cluster=cluster-name --user=user-name --namespace=default  # Create context
    kubectl config set-context --current --namespace=mynamespace    # Update current context namespace
    kubectl config rename-context old-name new-name                 # Rename context
    kubectl config delete-context context-name                      # Delete context

    # Enterprise Multi-Environment Management
    kubectl config view --minify                                     # Show current context only
    kubectl config view --raw                                        # Show full kubeconfig
    kubectl config view --flatten                                    # Flatten kubeconfig (merge)
    kubectl config view -o jsonpath='{.contexts[*].name}'             # Extract context names

    # Security and Backup (Enterprise)
    kubectl config view --kubeconfig /path/to/kubeconfig              # Use specific config
    kubectl config view --merge > /backup/kubeconfig-$(date +%Y%m%d)  # Backup configuration</code></pre>
  </div>

  <div class="command-block">
    <h4>Configuration - Enterprise Config Management</h4>
    <pre><code>
    # Kubernetes Configuration Management (Enterprise Standards)
    # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

    # Configuration Viewing (Enterprise)
    kubectl config view                                             # Show complete kubeconfig
    kubectl config view --flatten                                    # Flatten merged kubeconfig
    kubectl config view --minify                                     # Show current context only
    kubectl config view --raw                                        # Show raw kubeconfig data

    # Context Operations (Enterprise)
    kubectl config get-contexts                                     # List all contexts
    kubectl config current-context                                  # Show active context
    kubectl config use-context context-name                         # Switch context
    kubectl config set-context --current --namespace=mynamespace    # Set namespace for current context

    # Namespace Management (Enterprise)
    kubectl config set-context context-name --namespace=production  # Set default namespace
    kubectl get namespaces                                          # List all namespaces
    kubectl create namespace enterprise-namespace                   # Create namespace

    # Kubernetes Delete Operations (Enterprise Standards)
    # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

    # Delete Resources
    kubectl delete namespace enterprise-namespace                   # Delete namespace

    # Enterprise Configuration Validation
    kubectl config view --kubeconfig /path/to/kubeconfig           # Validate specific config
    kubectl config view --output json                               # JSON format for parsing
    kubectl config view --output yaml                               # YAML format for backup

    # Security and Authentication (Enterprise)
    kubectl config view --output jsonpath='{.users[*].user}'          # Show all users
    kubectl config view --output jsonpath='{.clusters[*].cluster.server}'  # Show cluster servers
    kubectl config view --output jsonpath='{.contexts[*].context.namespace}'  # Show namespaces

    # Configuration Backup and Restore (Enterprise)
    kubectl config view > kubeconfig-backup-$(date +%Y%m%d).yaml      # Backup current config
    export KUBECONFIG=/path/to/kubeconfig                           # Set config file
    unset KUBECONFIG                                               # Reset to default config</code></pre>
    <h3>Pods</h3>

    <div class="command-block">
      <h4>List pods - Enterprise Pod Management</h4>
      <pre><code>
      # Kubernetes Pod Listing (Enterprise Standards)
      # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

      # Resource management (requests/limits)
      kubectl set resources deployment deployment-name --limits=cpu=200m,memory=512Mi --requests=cpu=100m,memory=256Mi
      kubectl describe pod pod-name | grep -A 5 "Limits"             # View pod resource limits
      kubectl describe pod pod-name | grep -A 5 "Requests"           # View pod resource requests</code></pre>
    </div>

    <div class="command-block">
      <h4>Describe pod</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl describe pod pod-name                                   # Detailed pod information</code></pre>
    </div>

    <div class="command-block">
      <h4>Create/delete pods</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl run nginx --image=nginx                                 # Run pod
      kubectl run nginx --image=nginx --dry-run=client -o yaml        # Generate YAML

      # Kubernetes Delete Operations (Enterprise Standards)
      # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

      # Delete Resources
      kubectl delete pod pod-name                                     # Delete pod

      # Kubernetes Delete Operations (Enterprise Standards)
      # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

      # Delete Resources
      kubectl delete pods --all                                       # Delete all pods</code></pre>
    </div>

    <div class="command-block">
      <h4>Pod logs</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl logs pod-name                                           # View pod logs
      kubectl logs pod-name -f                                        # Follow logs
      kubectl logs pod-name -c container-name                         # Specific container
      kubectl logs pod-name --previous                                # Previous instance logs
      kubectl logs pod-name --tail=100                                # Last 100 lines
      kubectl logs pod-name --since=10m                               # Last 10 minutes</code></pre>
    </div>

    <div class="command-block">
      <h4>Execute commands</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl exec pod-name -- command                                # Execute command
      kubectl exec -it pod-name -- bash                               # Interactive bash
      kubectl exec -it pod-name -c container-name -- bash             # Specific container</code></pre>
    </div>

    <div class="command-block">
      <h4>Port forwarding</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl port-forward pod-name 8080:80                           # Forward port</code></pre>
    </div>

    <div class="command-block">
      <h4>Copy files</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl cp pod-name:/path/file ./local                          # Copy from pod
      kubectl cp ./local pod-name:/path/                              # Copy to pod</code></pre>
    </div>

    <h3>Deployments</h3>

    <div class="command-block">
      <h4>List deployments</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get deployments                                         # List deployments
      kubectl get deploy                                              # Short form
      kubectl get deployments -A                                      # All namespaces</code></pre>
    </div>

    <div class="command-block">
      <h4>Create deployment</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl create deployment nginx --image=nginx                   # Create deployment
      kubectl create deployment nginx --image=nginx --replicas=3      # With replicas</code></pre>
    </div>

    <div class="command-block">
      <h4>Describe deployment</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl describe deployment deployment-name</code></pre>
    </div>

    <div class="command-block">
      <h4>Scale deployment</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl scale deployment deployment-name --replicas=5</code></pre>
    </div>

    <div class="command-block">
      <h4>Update deployment</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl set image deployment/deployment-name container=image:tag
      kubectl rollout restart deployment/deployment-name              # Restart deployment</code></pre>
    </div>

    <div class="command-block">
      <h4>Deployment rollout</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl rollout status deployment/deployment-name                 # Check status
      kubectl rollout history deployment/deployment-name                # View history
      kubectl rollout undo deployment/deployment-name                   # Rollback
      kubectl rollout undo deployment/deployment-name --to-revision=2   # Specific revision</code></pre>
    </div>

    <div class="command-block">
      <h4>Delete deployment</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl delete deployment deployment-name</code></pre>
    </div>
    <h3>Horizontal Pod Autoscaler (HPA)</h3>

    <div class="command-block">
      <h4>HPA management</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get hpa                                                                 # List HPAs
      kubectl get hpa -A                                                              # All namespaces
      kubectl autoscale deployment deployment-name --cpu-percent=50 --min=1 --max=10  # Create HPA
      kubectl describe hpa hpa-name                                                   # HPA details

      # Kubernetes Delete Operations (Enterprise Standards)
      # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

      # Delete Resources
      kubectl delete hpa hpa-name                                                     # Delete HPA</code></pre>
    </div>

    <h3>Services</h3>

    <div class="command-block">
      <h4>List services</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get services                                            # List services
      kubectl get svc                                                 # Short form
      kubectl get services -A                                         # All namespaces</code></pre>
    </div>

    <div class="command-block">
      <h4>Describe service</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl describe service service-name</code></pre>
    </div>

    <div class="command-block">
      <h4>Create service</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl expose deployment deployment-name --port=80 --type=NodePort   # Expose deployment
      kubectl create service clusterip myservice --tcp=80:8080              # ClusterIP service</code></pre>
    </div>

    <div class="command-block">
      <h4>Delete service</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl delete service service-name</code></pre>
    </div>

    <h3>Namespaces</h3>

    <div class="command-block">
      <h4>List namespaces</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get namespaces                                          # List all namespaces
      kubectl get ns                                                  # Short form</code></pre>
    </div>

    <div class="command-block">
      <h4>Create namespace</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl create namespace mynamespace</code></pre>
    </div>

    <div class="command-block">
      <h4>Delete namespace</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl delete namespace mynamespace</code></pre>
    </div>

    <div class="command-block">
      <h4>Set default namespace</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl config set-context --current --namespace=mynamespace</code></pre>
    </div>

    <h3>ConfigMaps and Secrets</h3>

    <div class="command-block">
      <h4>ConfigMaps</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get configmaps                                          # List ConfigMaps
      kubectl get cm                                                  # Short form
      kubectl describe configmap cm-name
      kubectl create configmap myconfig --from-literal=key=value
      kubectl create configmap myconfig --from-file=file.txt

      # Kubernetes Delete Operations (Enterprise Standards)
      # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

      # Delete Resources
      kubectl delete configmap cm-name</code></pre>
    </div>

    <div class="command-block">
      <h4>Secrets</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get secrets                                             # List secrets
      kubectl describe secret secret-name
      kubectl create secret generic mysecret --from-literal=password=mypass
      kubectl create secret generic mysecret --from-file=./secret.txt

      # Kubernetes Delete Operations (Enterprise Standards)
      # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

      # Delete Resources
      kubectl delete secret secret-name</code></pre>
    </div>

    <h3>Apply and Manage Resources</h3>

    <div class="command-block">
      <h4>Apply configuration</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl apply -f file.yaml                                      # Apply from file

      # Kubernetes Apply Operations (Enterprise Standards)
      # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

      # Apply Manifests
      kubectl apply -f directory/                                     # Apply all files in directory

      # Kubernetes Apply Operations (Enterprise Standards)
      # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

      # Apply Manifests
      kubectl apply -f https://url/file.yaml                          # Apply from URL</code></pre>
    </div>

    <div class="command-block">
      <h4>Create from file</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl create -f file.yaml                                     # Create from file</code></pre>
    </div>

    <div class="command-block">
      <h4>Delete from file</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl delete -f file.yaml                                     # Delete resources in file</code></pre>
    </div>

    <div class="command-block">
      <h4>Replace</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl replace -f file.yaml                                    # Replace existing resource</code></pre>
    </div>

    <div class="command-block">
      <h4>Edit resource</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl edit deployment deployment-name                         # Edit in editor</code></pre>
    </div>

    <div class="command-block">
      <h4>Patch resource</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl patch deployment deployment-name -p '{"spec":{"replicas":5}}'</code></pre>
    </div>

    <h3>Nodes</h3>

    <div class="command-block">
      <h4>List nodes</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get nodes                                               # List all nodes
      kubectl get nodes -o wide                                       # Wide output</code></pre>
    </div>

    <div class="command-block">
      <h4>Describe node</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl describe node node-name</code></pre>
    </div>

    <div class="command-block">
      <h4>Cordon/uncordon (maintenance)</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl cordon node-name                                            # Mark node unschedulable
      kubectl uncordon node-name                                          # Mark node schedulable
      kubectl drain node-name                                             # Drain node for maintenance
      kubectl drain node-name --ignore-daemonsets --delete-emptydir-data  # Force drain</code></pre>
    </div>

    <h3>Labels and Annotations</h3>

    <div class="command-block">
      <h4>Labels</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl label pods pod-name app=myapp                           # Add label
      kubectl label pods pod-name app-                                # Remove label
      kubectl label pods pod-name app=newapp --overwrite              # Update label</code></pre>
    </div>

    <div class="command-block">
      <h4>Annotations</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl annotate pods pod-name description="My pod"             # Add annotation
      kubectl annotate pods pod-name description-                     # Remove annotation</code></pre>
    </div>

    <h3>Resource Monitoring</h3>

    <div class="command-block">
      <h4>Resource usage</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl top nodes                                               # Node resource usage
      kubectl top pods                                                # Pod resource usage
      kubectl top pod pod-name                                        # Specific pod usage
      kubectl top pods -n namespace                                   # Namespace pods usage</code></pre>
    </div>

    <div class="command-block">
      <h4>Debugging</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get events                                              # List cluster events
      kubectl get events --sort-by=.metadata.creationTimestamp        # Sorted events
      kubectl get events -n namespace                                 # Namespace events</code></pre>
    </div>

    <div class="command-block">
      <h4>Troubleshooting</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl describe pod pod-name                                   # Detailed pod info (shows errors)
      kubectl logs pod-name --previous                                # Logs from crashed container
      kubectl get pod pod-name -o yaml                                # Full pod specification</code></pre>
    </div>

    <div class="command-block">
      <h4>Debug with ephemeral container</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl debug pod-name -it --image=busybox                      # Add debug container</code></pre>
    </div>

    <div class="command-block">
      <h4>Copy resources</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get deployment deployment-name -o yaml > deployment.yaml  # Export to file</code></pre>
    </div>

    <h3>RBAC (Role-Based Access Control)</h3>
    <div class="command-block">
      <h4>Understanding RBAC Components</h4>
      <pre><code># Role vs ClusterRole:
      #
      # Role: Namespace-scoped permissions
      kubectl create role pod-reader --verb=get,list --resource=pods -n dev
      # - Applies only to specific namespace
      # - Use for namespace-specific resources (pods, services, deployments)
      #
      # ClusterRole: Cluster-wide permissions
      kubectl create clusterrole node-reader --verb=get,list --resource=nodes
      # - Applies across entire cluster
      # - Use for cluster-scoped resources (nodes, PVs, namespaces)

      # RoleBinding vs ClusterRoleBinding:
      # - RoleBinding: Grants permissions within a namespace
      # - ClusterRoleBinding: Grants permissions cluster-wide

      # Security best practice:
      # Always use Role/RoleBinding for namespace-scoped access
      # Only use ClusterRole/ClusterRoleBinding when necessary</code></pre>
    </div>

    <div class="command-block">
      <h4>Service accounts</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get serviceaccounts                                     # List service accounts
      kubectl get sa                                                  # Short form
      kubectl create serviceaccount mysa                              # Create service account</code></pre>
    </div>

    <div class="command-block">
      <h4>Roles and RoleBindings</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get roles                                               # List roles
      kubectl get rolebindings                                        # List role bindings
      kubectl get clusterroles                                        # List cluster roles
      kubectl get clusterrolebindings                                 # List cluster role bindings</code></pre>
    </div>

    <div class="command-block">
      <h4>Create role</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl create role myrole --verb=get,list --resource=pods</code></pre>
    </div>

    <div class="command-block">
      <h4>Create role binding</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl create rolebinding mybinding --role=myrole --user=myuser</code></pre>
    </div>

    <h3>Jobs and CronJobs</h3>

    <div class="command-block">
      <h4>Jobs</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get jobs                                                # List jobs
      kubectl describe job job-name
      kubectl create job myjob --image=busybox -- echo "Hello"

      # Kubernetes Delete Operations (Enterprise Standards)
      # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

      # Delete Resources
      kubectl delete job job-name</code></pre>
    </div>

    <div class="command-block">
      <h4>CronJobs</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get cronjobs                                            # List cronjobs
      kubectl get cj                                                  # Short form
      kubectl describe cronjob cronjob-name
      kubectl create cronjob mycron --image=busybox --schedule="*/5 * * * *" -- echo "Hello"

      # Kubernetes Delete Operations (Enterprise Standards)
      # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

      # Delete Resources
      kubectl delete cronjob cronjob-name</code></pre>
    </div>

    <h3>Ingress</h3>

    <div class="command-block">
      <h4>Ingress</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get ingress                                             # List ingress resources
      kubectl get ing                                                 # Short form
      kubectl describe ingress ingress-name

      # Kubernetes Delete Operations (Enterprise Standards)
      # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

      # Delete Resources
      kubectl delete ingress ingress-name</code></pre>
    </div>

    <h3>Persistent Volumes</h3>

    <div class="command-block">
      <h4>Persistent Volumes</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get pv                                                  # List persistent volumes
      kubectl describe pv pv-name</code></pre>
    </div>

    <div class="command-block">
      <h4>Persistent Volume Claims</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get pvc                                                 # List PVCs
      kubectl describe pvc pvc-name

      # Kubernetes Delete Operations (Enterprise Standards)
      # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

      # Delete Resources
      kubectl delete pvc pvc-name</code></pre>
    </div>

    <h3>StatefulSets</h3>

    <div class="command-block">
      <h4>StatefulSets</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get statefulsets                                        # List StatefulSets
      kubectl get sts                                                 # Short form
      kubectl describe statefulset ss-name

      # Kubernetes Scale Operations (Enterprise Standards)
      # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

      # Scale Resources
      kubectl scale statefulset ss-name --replicas=5

      # Kubernetes Delete Operations (Enterprise Standards)
      # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

      # Delete Resources
      kubectl delete statefulset ss-name</code></pre>
    </div>

    <h3>DaemonSets</h3>

    <div class="command-block">
      <h4>DaemonSets</h4>
      <pre><code>
      # Enterprise Standards
      # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

      ubectl get daemonsets                                          # List DaemonSets
      kubectl get ds                                                  # Short form
      kubectl describe daemonset ds-name

      # Kubernetes Delete Operations (Enterprise Standards)
      # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

      # Delete Resources
      kubectl delete daemonset ds-name</code></pre>
    </div>
  </div>

  <h2 id="ansible">32. ANSIBLE COMMANDS</h2>

  <div>
    <h3>🤖 Ansible Automation - From Scratch to Hero (Complete Tutorial)</h3>

    <div class="command-block">
      <h4>📚 Chapter 1: What is Ansible? (Absolute Beginner)</h4>
      <pre><code>
      # What is Ansible?
      # Ansible = Open-source automation tool
      # Created by Michael DeHaan (2012)
      # Named after "Ansible" - communication system in Star Trek

      # Why Learn Ansible for DevOps?
      # 1. Agentless: No software to install on target servers
      # 2. Simple: Uses YAML (human-readable)
      # 3. Powerful: Automate complex tasks
      # 4. Idempotent: Same result every time
      # 5. Cross-platform: Linux, Windows, Network devices
      # 6. Free & Open Source

      # Ansible in DevOps Ecosystem:
      # - Configuration Management (Server setup)
      # - Application Deployment (Code deployment)
      # - Infrastructure Provisioning (Cloud setup)
      # - Security Compliance (Policy enforcement)
      # - Orchestration (Multi-server coordination)
      # - Continuous Delivery (CI/CD pipelines)

      # Prerequisites:
      # 1. Basic Linux commands
      # 2. YAML knowledge (simple)
      # 3. SSH understanding
      # 4. Python 3.6+ (for control node)
      # 5. Practice mindset! 🎯
    </code></pre>
  </div>

  <div class="command-block">
    <h4>🚀 Chapter 2: Ansible Installation and Setup</h4>
    <pre><code>
    # Step 1: Install Ansible on Control Node
    # On Red Hat/CentOS/Fedora:
    sudo dnf install ansible -y

    # On Debian/Ubuntu:
    sudo apt update
    sudo apt install ansible -y

    # On openSUSE:
    sudo zypper install ansible

    # On Arch Linux:
    sudo pacman -S ansible

    # Step 2: Verify Installation
    ansible --version
    ansible --version | head -1

    # Step 3: Create Inventory File
    mkdir -p ~/ansible
    cd ~/ansible
    nano inventory

    # Basic inventory format:
    [webservers]
    web1.example.com
    web2.example.com
    192.168.1.10

    [databases]
    db1.example.com
    db2.example.com

    [all:vars]
    ansible_user=ansible
    ansible_ssh_private_key_file=~/.ssh/ansible_key

    # Step 4: Setup SSH Key Authentication
    ssh-keygen -t rsa -b 4096 -f ~/.ssh/ansible_key
    ssh-copy-id -i ~/.ssh/ansible_key.pub ansible@web1.example.com

    # Step 5: Test Connection
    ansible all -m ping -i inventory
    ansible webservers -m ping -i inventory
  </code></pre>
</div>

<div class="command-block">
  <h4>📝 Chapter 3: Ansible Ad-Hoc Commands</h4>
  <pre><code>
  # Ad-hoc commands are one-time commands
  # Perfect for quick tasks and testing

  # Basic Syntax:
  ansible <pattern> -m <module> -a "<arguments>"

  # 1. Ping Test (Connectivity Check)
  ansible all -m ping -i inventory
  ansible webservers -m ping -i inventory
  ansible web1.example.com -m ping -i inventory

  # 2. Command Module (Basic Commands)
  ansible all -m command -a "uptime" -i inventory
  ansible all -m command -a "whoami" -i inventory
  ansible all -m command -a "df -h" -i inventory
  ansible all -m command -a "free -m" -i inventory

  # 3. Shell Module (Advanced Shell Features)
  ansible all -m shell -a "ps aux | grep nginx" -i inventory
  ansible all -m shell -a "ls -la /tmp > /tmp/output.txt" -i inventory
  ansible all -m shell -a "cat /proc/meminfo | grep MemTotal" -i inventory

  # 4. Copy Module (File Transfer)
  ansible all -m copy -a "src=/local/file dest=/remote/path" -i inventory
  ansible all -m copy -a "src=config.txt dest=/etc/config.txt mode=0644" -i inventory
  ansible all -m copy -a "src=script.sh dest=/usr/local/bin/script.sh mode=0755" -i inventory

  # 5. File Module (File Management)
  ansible all -m file -a "path=/tmp/testfile state=touch" -i inventory
  ansible all -m file -a "path=/tmp/testdir state=directory mode=0755" -i inventory
  ansible all -m file -a "path=/tmp/testfile state=absent" -i inventory
  ansible all -m file -a "src=/etc/hosts dest=/tmp/hosts_link state=link" -i inventory

  # 6. Package Management
  # For Red Hat/CentOS:
  ansible all -m dnf -a "name=nginx state=present" -i inventory
  ansible all -m dnf -a "name=nginx state=latest" -i inventory
  ansible all -m dnf -a "name=nginx state=absent" -i inventory

  # For Debian/Ubuntu:
  ansible all -m apt -a "name=nginx state=present update_cache=yes" -i inventory
  ansible all -m apt -a "name=nginx state=latest" -i inventory

  # 7. Service Management
  ansible all -m service -a "name=nginx state=started" -i inventory
  ansible all -m service -a "name=nginx state=stopped" -i inventory
  ansible all -m service -a "name=nginx state=restarted" -i inventory
  ansible all -m service -a "name=nginx enabled=yes" -i inventory

  # 8. User Management
  ansible all -m user -a "name=deploy state=present" -i inventory
  ansible all -m user -a "name=deploy state=absent remove=yes" -i inventory
  ansible all -m user -a "name=deploy password=$6$encrypted_password" -i inventory

  # 9. System Information (Facts)
  ansible all -m setup -i inventory
  ansible all -m setup -a "filter=ansible_distribution*" -i inventory
  ansible all -m setup -a "filter=ansible_memory_mb" -i inventory
  ansible all -m setup -a "filter=ansible_mounts" -i inventory

  # 10. Advanced Options
  # Become (sudo):
  ansible all -m command -a "whoami" --become -i inventory
  ansible all -m command -a "cat /etc/shadow" --become -i inventory

  # Limit to specific hosts:
  ansible all -m ping --limit "web1,web2" -i inventory
  ansible all -m ping --limit "webservers:!web1" -i inventory

  # Check mode (dry run):
  ansible all -m dnf -a "name=nginx state=present" --check -i inventory

  # Verbose output:
  ansible all -m ping -v -i inventory
  ansible all -m ping -vvv -i inventory
</code></pre>
</div>

<div class="command-block">
  <h4>📋 Chapter 4: Ansible Playbooks (YAML Automation)</h4>
  <pre><code>
  # Playbooks are YAML files for automation
  # They define tasks, roles, and configurations

  # Basic Playbook Structure:
  ---
  - name: Playbook Name
  hosts: target_hosts
  become: yes
  vars:
  variable_name: value
  tasks:
  - name: Task Name
  module_name:
  parameter1: value1
  parameter2: value2

  # Example 1: Simple Web Server Setup
  ---
  - name: Setup Nginx Web Server
  hosts: webservers
  become: yes
  tasks:
  - name: Install Nginx
  dnf:
  name: nginx
  state: present

  - name: Start and Enable Nginx
  service:
  name: nginx
  state: started
  enabled: yes

  - name: Copy Website Files
  copy:
  src: /local/website/
  dest: /var/www/html/
  mode: '0755'

  - name: Open Firewall Port
  firewalld:
  port: 80/tcp
  state: enabled
  permanent: yes
  immediate: yes

  # Example 2: Database Server Setup
  ---
  - name: Setup PostgreSQL Database
  hosts: databases
  become: yes
  vars:
  db_name: myapp
  db_user: appuser
  db_password: secretpassword
  tasks:
  - name: Install PostgreSQL
  dnf:
  name: postgresql-server
  state: present

  - name: Initialize Database
  command: postgresql-setup --initdb
  args:
  creates: /var/lib/pgsql/data/postgresql.conf

  - name: Start PostgreSQL
  service:
  name: postgresql
  state: started
  enabled: yes

  - name: Create Database
  postgresql_db:
  name: "{{ db_name }}"
  state: present

  - name: Create Database User
  postgresql_user:
  db: "{{ db_name }}"
  name: "{{ db_user }}"
  password: "{{ db_password }}"
  priv: "ALL"
  state: present

  # Example 3: Multi-Stage Application Deployment
  ---
  - name: Deploy Web Application
  hosts: webservers
  become: yes
  vars:
  app_name: mywebapp
  app_version: 1.2.3
  app_port: 8080
  tasks:
  - name: Create Application User
  user:
  name: "{{ app_name }}"
  shell: /bin/bash
  home: "/opt/{{ app_name }}"
  state: present

  - name: Download Application
  get_url:
  url: "https://releases.example.com/{{ app_name }}-{{ app_version }}.tar.gz"
  dest: "/tmp/{{ app_name }}-{{ app_version }}.tar.gz"

  - name: Extract Application
  unarchive:
  src: "/tmp/{{ app_name }}-{{ app_version }}.tar.gz"
  dest: "/opt/{{ app_name }}/"
  remote_src: yes
  owner: "{{ app_name }}"
  group: "{{ app_name }}"

  - name: Install Dependencies
  dnf:
  name:
  - python3
  - python3-pip
  state: present

  - name: Install Python Dependencies
  pip:
  requirements: "/opt/{{ app_name }}/requirements.txt"
  state: present

  - name: Create Systemd Service
  template:
  src: app.service.j2
  dest: "/etc/systemd/system/{{ app_name }}.service"
  notify: Restart Application

  - name: Enable and Start Service
  systemd:
  name: "{{ app_name }}"
  enabled: yes
  state: started

  handlers:
  - name: Restart Application
  systemd:
  name: "{{ app_name }}"
  state: restarted
</code></pre>
</div>

<div class="command-block">
  <h4>🔄 Chapter 5: Variables and Templates</h4>
  <pre><code>
  # Variables make playbooks flexible and reusable

  # 1. Playbook Variables
  ---
  - name: Configure Application
  hosts: appservers
  vars:
  app_name: myapp
  app_version: 2.1.0
  app_port: 3000
  debug_mode: true
  database_host: db.example.com
  tasks:
  - name: Create Config File
  template:
  src: app.conf.j2
  dest: "/etc/{{ app_name }}/app.conf"
  notify: Restart App

  # 2. Variable Files (vars_files)
  ---
  - name: Deploy with Variables
  hosts: appservers
  vars_files:
  - vars/common.yml
  - vars/{{ ansible_distribution }}.yml
  tasks:
  - name: Configure Application
  template:
  src: config.j2
  dest: "/etc/myapp/config.yml"

  # vars/common.yml:
  app_name: mywebapp
  app_port: 8080
  debug_level: INFO

  # vars/RedHat.yml:
  package_manager: dnf
  service_manager: systemd

  # vars/Ubuntu.yml:
  package_manager: apt
  service_manager: systemd

  # 3. Host Variables and Group Variables
  # Directory structure:
  # group_vars/
  #   webservers.yml
  #   databases.yml
  # host_vars/
  #   web1.example.com.yml
  #   db1.example.com.yml

  # group_vars/webservers.yml:
  nginx_worker_processes: 4
  nginx_keepalive_timeout: 65
  nginx_max_clients: 512

  # host_vars/web1.example.com.yml:
  nginx_worker_processes: 8
  nginx_max_clients: 1024

  # 4. Command Line Variables
  ansible-playbook playbook.yml -e "app_version=2.1.0 debug_mode=true"
  ansible-playbook playbook.yml --extra-vars "@vars.json"

  # 5. Registered Variables (Task Output)
  ---
  - name: Process System Information
  hosts: all
  tasks:
  - name: Get System Uptime
  command: uptime
  register: uptime_result

  - name: Display Uptime
  debug:
  msg: "System uptime: {{ uptime_result.stdout }}"

  - name: Get Disk Usage
  command: df -h /
  register: disk_usage

  - name: Check Disk Space
  fail:
  msg: "Disk usage too high: {{ disk_usage.stdout }}"
  when: "'90%' in disk_usage.stdout"

  # 6. Facts Variables
  ---
  - name: Use System Facts
  hosts: all
  tasks:
  - name: Display Distribution Info
  debug:
  msg: |
  Distribution: {{ ansible_distribution }}
  Version: {{ ansible_distribution_version }}
  Architecture: {{ ansible_architecture }}
  Memory: {{ ansible_memtotal_mb }}MB
  CPU Cores: {{ ansible_processor_cores }}

  - name: Install Package Based on Distribution
  dnf:
  name: nginx
  state: present
  when: ansible_distribution == "RedHat"

  - name: Install Package Based on Distribution
  apt:
  name: nginx
  state: present
  when: ansible_distribution == "Ubuntu"
</code></pre>
</div>

<div class="command-block">
  <h4>🎭 Chapter 6: Conditionals and Loops</h4>
  <pre><code>
  # Conditionals and loops control task execution

  # 1. When Conditions (Basic)
  ---
  - name: Conditional Tasks
  hosts: all
  tasks:
  - name: Install Apache on Red Hat
  dnf:
  name: httpd
  state: present
  when: ansible_distribution == "RedHat"

  - name: Install Apache on Ubuntu
  apt:
  name: apache2
  state: present
  when: ansible_distribution == "Ubuntu"

  - name: Start Service
  service:
  name: "{{ 'httpd' if ansible_distribution == 'RedHat' else 'apache2' }}"
  state: started
  when: ansible_distribution in ["RedHat", "Ubuntu"]

  # 2. Multiple Conditions
  ---
  - name: Complex Conditions
  hosts: all
  tasks:
  - name: Configure Production Settings
  template:
  src: production.conf.j2
  dest: /etc/app/production.conf
  when:
  - ansible_distribution == "RedHat"
  - ansible_memtotal_mb >= 2048
  - "'production' in group_names"

  - name: Configure Development Settings
  template:
  src: development.conf.j2
  dest: /etc/app/development.conf
  when:
  - ansible_distribution == "Ubuntu"
  - ansible_memtotal_mb < 2048

  # 3. Loops (with_items)
  ---
  - name: Loop Example
  hosts: all
  tasks:
  - name: Install Multiple Packages
  dnf:
  name: "{{ item }}"
  state: present
  loop:
  - nginx
  - git
  - vim
  - curl

  - name: Create Multiple Users
  user:
  name: "{{ item.name }}"
  groups: "{{ item.groups }}"
  state: present
  loop:
  - { name: 'alice', groups: 'developers' }
  - { name: 'bob', groups: 'developers,admins' }
  - { name: 'charlie', groups: 'testers' }

  # 4. Loops with Dictionaries
  ---
  - name: Dictionary Loop
  hosts: all
  vars:
  users:
  alice:
  uid: 1001
  shell: /bin/bash
  groups: developers
  bob:
  uid: 1002
  shell: /bin/zsh
  groups: admins
  tasks:
  - name: Create Users from Dictionary
  user:
  name: "{{ item.key }}"
  uid: "{{ item.value.uid }}"
  shell: "{{ item.value.shell }}"
  groups: "{{ item.value.groups }}"
  state: present
  loop: "{{ users | dict2items }}"
  when: item.value.uid >= 1000

  # 5. Loops with Files
  ---
  - name: File Loop Operations
  hosts: all
  tasks:
  - name: Find Configuration Files
  find:
  paths: /etc
  patterns: "*.conf"
  recurse: yes
  register: config_files

  - name: Backup Configuration Files
  copy:
  src: "{{ item.path }}"
  dest: "/backup/{{ item.path | basename }}"
  remote_src: yes
  loop: "{{ config_files.files }}"
  when: item.size > 0

  # 6. Conditional Loops
  ---
  - name: Conditional Loop Example
  hosts: all
  tasks:
  - name: Check Services
  service_facts:

  - name: Start Required Services
  service:
  name: "{{ item }}"
  state: started
  loop:
  - nginx
  - sshd
  - firewalld
  when: ansible_facts.services[item] is not defined or ansible_facts.services[item].state != 'running'

  # 7. Until Loops (Retry Logic)
  ---
  - name: Retry Logic
  hosts: all
  tasks:
  - name: Wait for Service to Start
  uri:
  url: "http://localhost:8080/health"
  method: GET
  register: health_check
  until: health_check.status == 200
  retries: 10
  delay: 5
  ignore_errors: yes
</code></pre>
</div>

<div class="command-block">
  <h4>🎭 Chapter 7: Roles and Best Practices</h4>
  <pre><code>
  # Roles organize playbooks into reusable components

  # 1. Role Structure
  # myrole/
  # ├── defaults/
  # │   └── main.yml
  # ├── files/
  # ├── handlers/
  # │   └── main.yml
  # ├── meta/
  # │   └── main.yml
  # ├── tasks/
  # │   └── main.yml
  # ├── templates/
  # ├── tests/
  # │   ├── inventory
  # │   └── test.yml
  # └── vars/
  #     └── main.yml

  # 2. Creating a Web Server Role
  # roles/webserver/tasks/main.yml
  ---
  - name: Install Nginx
  dnf:
  name: nginx
  state: present
  when: ansible_distribution == "RedHat"

  - name: Install Nginx
  apt:
  name: nginx
  state: present
  when: ansible_distribution == "Ubuntu"

  - name: Start and Enable Nginx
  service:
  name: nginx
  state: started
  enabled: yes

  - name: Create Website Directory
  file:
  path: /var/www/html
  state: directory
  mode: '0755'

  - name: Copy Index Page
  copy:
  src: index.html
  dest: /var/www/html/index.html
  mode: '0644'
  notify: Restart Nginx

  # roles/webserver/handlers/main.yml
  ---
  - name: Restart Nginx
  service:
  name: nginx
  state: restarted

  # roles/webserver/defaults/main.yml
  ---
  nginx_port: 80
  nginx_worker_processes: "{{ ansible_processor_cores }}"
  nginx_keepalive_timeout: 65

  # roles/webserver/templates/nginx.conf.j2
  user nginx;
  worker_processes {{ nginx_worker_processes }};
  error_log /var/log/nginx/error.log;
  pid /run/nginx.pid;

  events {
  worker_connections 1024;
  }

  http {
  server {
  listen {{ nginx_port }};
  server_name _;
  root /var/www/html;
  index index.html;
  }
  }

  # roles/webserver/files/index.html
  <!DOCTYPE html>
  <html>
    <head>
      <title>Welcome to {{ ansible_hostname }}</title>
      </head>
      <body>
        <h1>Hello from {{ ansible_hostname }}!</h1>
        <p>Managed by Ansible</p>
      </body>
    </html>

    # 3. Using Roles in Playbooks
    ---
    - name: Deploy Web Application
    hosts: webservers
    become: yes
    roles:
    - webserver
    - database
    - firewall

    # 4. Role Variables
    ---
    - name: Deploy with Role Variables
    hosts: webservers
    become: yes
    roles:
    - role: webserver
    vars:
    nginx_port: 8080
    nginx_worker_processes: 8
    - role: database
    when: "'database' in group_names"

    # 5. Role Dependencies
    # roles/webserver/meta/main.yml
    ---
    dependencies:
    - role: common
    - role: firewall
    vars:
    firewall_ports:
    - 80/tcp
    - 443/tcp

    # 6. Best Practices

    # a) Use Version Control
    git init
    git add .
    git commit -m "Initial role structure"
    git remote add origin https://github.com/user/ansible-roles.git
    git push -u origin main

    # b) Test Roles
    # roles/webserver/tests/test.yml
    ---
    - name: Test Webserver Role
    hosts: localhost
    connection: local
    gather_facts: yes
    roles:
    - webserver

    # c) Use ansible-lint for Code Quality
    pip install ansible-lint
    ansible-lint roles/webserver/

    # d) Use Molecule for Testing
    pip install molecule
    molecule init role webserver
    molecule test

    # e) Documentation
    # roles/webserver/README.md
    # Webserver Role
    # ==============
    #
    # This role installs and configures Nginx web server.
    #
    # ## Requirements
    # - Ansible 2.9+
    # - Red Hat or Ubuntu
    #
    # ## Role Variables
    # - `nginx_port`: Port for Nginx (default: 80)
    # - `nginx_worker_processes`: Number of worker processes
    #
    # ## Example Playbook
    # ```yaml
    # - hosts: webservers
    #   roles:
    #     - webserver
    # ```
  </code></pre>
</div>

<div>
  <h3>Ansible Ad-Hoc Commands</h3>

  <div class="command-block">
    <h4>Basic ad-hoc syntax</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible all -m module -a "arguments"</code></pre>
  </div>

  <div class="command-block">
    <h4>Ping hosts</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible all -m ping                                             # Ping all hosts
    ansible webservers -m ping                                      # Ping specific group
    ansible host1 -m ping                                           # Ping specific host</code></pre>
  </div>

  <div class="command-block">
    <h4>Command module (default)</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible all -m command -a "uptime"
    ansible all -a "uptime"                                         # Short form (command is default)
    ansible all -a "df -h"                                          # Disk usage
    ansible all -a "free -m"                                        # Memory info</code></pre>
  </div>

  <div class="command-block">
    <h4>Shell module (supports shell features)</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible all -m shell -a "ps aux | grep httpd"
    ansible all -m shell -a "ls -la /tmp > /tmp/output.txt"</code></pre>
  </div>

  <div class="command-block">
    <h4>Copy files</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible all -m copy -a "src=/local/file dest=/remote/path"
    ansible all -m copy -a "src=/local/file dest=/remote/path mode=0644 owner=user"</code></pre>
  </div>

  <div class="command-block">
    <h4>File operations</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible all -m file -a "path=/tmp/test state=touch"                 # Create file
    ansible all -m file -a "path=/tmp/test state=absent"                # Delete file
    ansible all -m file -a "path=/tmp/dir state=directory mode=0755"    # Create dir
    ansible all -m file -a "src=/path/file dest=/path/link state=link"  # Symlink</code></pre>
  </div>

  <div class="command-block">
    <h4>Package management</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible all -m dnf -a "name=httpd state=present"                # Install package
    ansible all -m dnf -a "name=httpd state=latest"                 # Update package
    ansible all -m dnf -a "name=httpd state=absent"                 # Remove package</code></pre>
  </div>

  <div class="command-block">
    <h4>Service management</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible all -m service -a "name=httpd state=started"            # Start service
    ansible all -m service -a "name=httpd state=stopped"            # Stop service
    ansible all -m service -a "name=httpd state=restarted"          # Restart service
    ansible all -m service -a "name=httpd enabled=yes"              # Enable at boot</code></pre>
  </div>

  <div class="command-block">
    <h4>User management</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible all -m user -a "name=john state=present"
    ansible all -m user -a "name=john state=absent remove=yes"</code></pre>
  </div>

  <div class="command-block">
    <h4>Gather facts</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible all -m setup                                            # Gather all facts
    ansible all -m setup -a "filter=ansible_distribution*"          # Filter facts</code></pre>
  </div>

  <div class="command-block">
    <h4>Execute with sudo</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible all -m command -a "whoami" --become
    ansible all -m command -a "whoami" -b                           # Short form</code></pre>
  </div>

  <div class="command-block">
    <h4>Specify user</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible all -m ping -u username
    ansible all -m ping --user username</code></pre>
  </div>

  <div class="command-block">
    <h4>Limit hosts</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible all -m ping --limit "host1,host2"
    ansible all -m ping --limit "webservers:!host1"                 # Exclude host1</code></pre>
  </div>

  <div class="command-block">
    <h4>Check mode (dry run)</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible all -m command -a "uptime" --check
    ansible all -m command -a "uptime" -C                           # Short form</code></pre>
  </div>

  <h3>Ansible Playbook Commands</h3>

  <div class="command-block">
    <h4>Run playbook</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-playbook playbook.yml                                   # Execute playbook</code></pre>
  </div>

  <div class="command-block">
    <h4>Check syntax</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-playbook playbook.yml --syntax-check</code></pre>
  </div>

  <div class="command-block">
    <h4>Dry run</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-playbook playbook.yml --check

    # Ansible Playbook Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Execute Playbooks
    ansible-playbook playbook.yml -C                                # Short form</code></pre>
  </div>

  <div class="command-block">
    <h4>Diff mode</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-playbook playbook.yml --diff                            # Show changes
    <pre><code>
    # Ansible Playbook Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Execute Playbooks
    ansible-playbook playbook.yml --check --diff                    # Dry run with diff</code></pre>
  </div>

  <div class="command-block">
    <h4>Limit execution</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-playbook playbook.yml --limit webservers
    <pre><code>
    # Ansible Playbook Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Execute Playbooks
    ansible-playbook playbook.yml -l host1,host2                    # Short form</code></pre>
  </div>

  <div class="command-block">
    <h4>Tags</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-playbook playbook.yml --tags "install,configure"
    <pre><code>
    # Ansible Playbook Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Execute Playbooks
    ansible-playbook playbook.yml --skip-tags "test"
    <pre><code>
    # Ansible Playbook Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Execute Playbooks
    ansible-playbook playbook.yml --list-tags                       # List available tags</code></pre>
  </div>

  <div class="command-block">
    <h4>Start at task</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-playbook playbook.yml --start-at-task "task name"</code></pre>
  </div>

  <div class="command-block">
    <h4>Step through</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-playbook playbook.yml --step                            # Confirm each task</code></pre>
  </div>

  <div class="command-block">
    <h4>Verbose output</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-playbook playbook.yml -v                                # Verbose
    <pre><code>
    # Ansible Playbook Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Execute Playbooks
    ansible-playbook playbook.yml -vv                               # More verbose
    <pre><code>
    # Ansible Playbook Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Execute Playbooks
    ansible-playbook playbook.yml -vvv                              # Very verbose
    <pre><code>
    # Ansible Playbook Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Execute Playbooks
    ansible-playbook playbook.yml -vvvv                             # Debug level</code></pre>
  </div>

  <div class="command-block">
    <h4>Extra variables</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-playbook playbook.yml --extra-vars "version=1.0 env=prod"
    <pre><code>
    # Ansible Playbook Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Execute Playbooks
    ansible-playbook playbook.yml -e "version=1.0"                  # Short form
    <pre><code>
    # Ansible Playbook Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Execute Playbooks
    ansible-playbook playbook.yml -e @vars.yml                      # From file</code></pre>
  </div>

  <div class="command-block">
    <h4>Inventory</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-playbook playbook.yml -i inventory.ini
    <pre><code>
    # Ansible Playbook Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Execute Playbooks
    ansible-playbook playbook.yml --inventory hosts                 # Alternative</code></pre>
  </div>

  <div class="command-block">
    <h4>Ask for passwords</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-playbook playbook.yml --ask-pass                        # SSH password
    <pre><code>
    # Ansible Playbook Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Execute Playbooks
    ansible-playbook playbook.yml --ask-become-pass                 # Sudo password
    <pre><code>
    # Ansible Playbook Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Execute Playbooks
    ansible-playbook playbook.yml -K                                # Ask become password (short)</code></pre>
  </div>

  <div class="command-block">
    <h4>List tasks</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-playbook playbook.yml --list-tasks</code></pre>
  </div>

  <div class="command-block">
    <h4>List hosts</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-playbook playbook.yml --list-hosts</code></pre>
  </div>

  <h3>Ansible Inventory Commands</h3>

  <div class="command-block">
    <h4>List inventory</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-inventory --list                                        # Show inventory in JSON
    ansible-inventory --graph                                       # Show inventory graph
    ansible-inventory --host hostname                               # Show host variables</code></pre>
  </div>

  <div class="command-block">
    <h4>Inventory file locations</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible all --list-hosts -i inventory.ini                       # Use specific inventory</code></pre>
  </div>

  <h3>Ansible Vault Commands</h3>

  <div class="command-block">
    <h4>Create encrypted file</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-vault create secret.yml</code></pre>
  </div>

  <div class="command-block">
    <h4>Edit encrypted file</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-vault edit secret.yml</code></pre>
  </div>

  <div class="command-block">
    <h4>Encrypt existing file</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-vault encrypt file.yml
    <pre><code>
    # Ansible Vault Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Encryption Management
    ansible-vault encrypt file1.yml file2.yml                       # Multiple files</code></pre>
  </div>

  <div class="command-block">
    <h4>Decrypt file</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-vault decrypt file.yml</code></pre>
  </div>

  <div class="command-block">
    <h4>View encrypted file</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-vault view file.yml</code></pre>
  </div>

  <div class="command-block">
    <h4>Change password</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-vault rekey file.yml</code></pre>
  </div>

  <div class="command-block">
    <h4>Use vault in playbook</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-playbook playbook.yml --ask-vault-pass
    <pre><code>
    # Ansible Playbook Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Execute Playbooks
    ansible-playbook playbook.yml --vault-password-file ~/.vault_pass
    <pre><code>
    # Ansible Playbook Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Execute Playbooks
    ansible-playbook playbook.yml --vault-id @prompt</code></pre>
  </div>

  <h3>Ansible Galaxy Commands</h3>

  <div class="command-block">
    <h4>Install role</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    nsible-galaxy install username.rolename
    ansible-galaxy install -r requirements.yml</code></pre>
  </div>

  <div class="command-block">
    <h4>List installed roles</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ansible-galaxy list</code></pre>
  </div>

  <div class="command-block">
    <h4>Remove role</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ansible-galaxy remove username.rolename</code></pre>
  </div>

  <div class="command-block">
    <h4>Initialize new role</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ansible-galaxy init rolename</code></pre>
  </div>

  <div class="command-block">
    <h4>Search for roles</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ansible-galaxy search elasticsearch</code></pre>
  </div>

  <div class="command-block">
    <h4>Role information</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ansible-galaxy info username.rolename</code></pre>
  </div>
  <h3>Advanced Ansible - Production-Ready Skills</h3>

  <div class="command-block">
    <h4>Ansible Architecture & Job Market Value</h4>
    <pre><code>
    # Why Ansible is THE Bridge from SysAdmin to DevOps:
    #
    # 1. Uses existing Linux knowledge (SSH-based)
    # 2. Agentless (no client installation needed)
    # 3. YAML-based (human-readable)
    # 4. Idempotent (safe to run multiple times)
    # 5. Massive adoption (60%+ companies use it)

    # Ansible vs Other Tools:
    #
    # Ansible vs Shell Scripts:
    #   Scripts: Imperative (how to do)
    #   Ansible: Declarative (what to achieve)
    #   Ansible handles errors, rollbacks, idempotency automatically
    #
    # Ansible vs Chef/Puppet:
    #   No agent required (push-based)
    #   Lower learning curve
    #   Faster deployment
    #   Better for cloud-native environments
    #
    # Ansible vs Terraform:
    #   Terraform: Infrastructure provisioning (IaC)
    #   Ansible: Configuration management (CM)
    #   Often used together: Terraform creates, Ansible configures

    # Market Demand (Bangladesh Context):
    # SysAdmin without Ansible: ৳30-45k
    # SysAdmin with Ansible: ৳50-70k
    # Junior DevOps with Ansible: ৳70-90k
    #
    # Interview Impact:
    # - 80% DevOps interviews ask Ansible
    # - Instantly differentiates from pure SysAdmins
    # - Shows automation mindset</code></pre>
  </div>

  <div class="command-block">
    <h4>Ansible Inventory - Advanced Patterns</h4>
    <pre><code>
    # Static Inventory (INI Format)
    # Path: /etc/ansible/hosts
    # Supported OS: Red Hat, Debian, Ubuntu, openSUSE, Arch


    # Web Servers Group

    [webservers]
    web1.example.com
    web2.example.com
    web3.example.com


    # Database Servers Group

    [databases]
    db1.example.com ansible_host=192.168.1.10 ansible_port=2222
    db2.example.com ansible_host=192.168.1.11


    # Load Balancers Group

    [loadbalancers]
    lb1.example.com ansible_user=admin ansible_ssh_private_key_file=~/.ssh/lb_key


    # Group of Groups (Parent–Child)

    [production:children]
    webservers
    databases

    [staging:children]
    webservers_staging
    databases_staging


    # Group Variables

    [webservers:vars]
    http_port=80
    nginx_version=1.21
    max_connections=1000

    [databases:vars]
    mysql_port=3306
    mysql_max_connections=500


    # Dynamic Inventory (YAML Format)
    # File: inventory.yml

    all:
    hosts:
    localhost:
    ansible_connection: local
    children:
    webservers:
    hosts:
    web[1:3].example.com          # Expands to web1, web2, web3
    vars:
    http_port: 80
    databases:
    hosts:
    db[a:c].example.com           # Expands to dba, dbb, dbc
    vars:
    mysql_port: 3306


    # AWS Dynamic Inventory (aws_ec2 Plugin)
    # File: aws_ec2.yml

    plugin: aws_ec2
    regions:
    - us-east-1
    - us-west-2
    filters:
    tag:Environment: production
    keyed_groups:
    - key: tags.Role
    prefix: role
    - key: placement.region
    prefix: region


    # Inventory Usage

    ansible-inventory -i aws_ec2.yml --graph            # View inventory structure
    ansible-inventory -i aws_ec2.yml --list             # List all discovered hosts


    # Inventory Best Practices
    # Use dynamic inventory for cloud environments
    # Group by function (webservers, databases)
    # Use host variables for host-specific configuration
    # Use group variables for shared configuration
    # Store sensitive data securely with Ansible Vault


    # Ansible Vault Operations (Enterprise)

    ansible-vault                                       # Encryption and secrets management
  </code></pre>

</div>

<div class="command-block">
  <h4>Ansible Playbooks - Production Patterns</h4>
  <pre><code>
  # Basic Playbook Structure
  ---
  - name: Configure web servers
  hosts: webservers
  become: yes                    # Run with sudo
  vars:
  http_port: 80
  max_clients: 200

  tasks:
  - name: Install nginx
  dnf:
  name: nginx
  state: present

  - name: Copy nginx config
  template:
  src: nginx.conf.j2
  dest: /etc/nginx/nginx.conf
  notify: restart nginx

  - name: Start nginx service
  service:
  name: nginx
  state: started
  enabled: yes

  handlers:
  - name: restart nginx
  service:
  name: nginx
  state: restarted

  # Multi-Play Playbook (Multiple hosts/tasks)
  ---
  - name: Deploy database
  hosts: databases
  become: yes
  tasks:
  - name: Install MySQL
  dnf:
  name: mysql-server
  state: present

  - name: Deploy application
  hosts: webservers
  become: yes
  tasks:
  - name: Deploy app code
  git:
  repo: https://github.com/company/app.git
  dest: /var/www/app

  # Advanced Task Options
  tasks:
  - name: Install packages
  dnf:
  name: "{{ item }}"
  state: present
  loop:                        # Loop through list
  - nginx
  - php-fpm
  - mysql-client
  when: ansible_os_family == "RedHat"  # Conditional
  tags: packages               # Tag for selective execution

  - name: Check if service exists
  stat:
  path: /etc/systemd/system/myapp.service
  register: service_status     # Save result to variable

  - name: Debug output
  debug:
  msg: "Service exists: {{ service_status.stat.exists }}"

  # Error Handling
  tasks:
  - name: Attempt risky operation
  command: /usr/bin/risky_command
  register: result
  ignore_errors: yes           # Continue even if fails

  - name: Handle failure
  debug:
  msg: "Operation failed, rolling back"
  when: result.failed

  - name: Force failure
  fail:
  msg: "Critical check failed"
  when: some_condition

  # Delegation & Local Actions
  tasks:
  - name: Run on control node
  command: aws s3 sync /backup s3://bucket/
  delegate_to: localhost       # Run on Ansible controller

  - name: Run once for all hosts
  command: /usr/local/bin/update_loadbalancer.sh
  run_once: yes                # Execute only once, not per host

  # Async Tasks (Long-running operations)
  tasks:
  - name: Long running backup
  shell: /usr/local/bin/backup.sh
  async: 3600                  # Max runtime: 1 hour
  poll: 0                      # Don't wait (fire and forget)
  register: backup_job

  - name: Check backup status
  async_status:
  jid: "{{ backup_job.ansible_job_id }}"
  register: job_result
  until: job_result.finished
  retries: 30
  delay: 60                    # Check every 60 seconds</code></pre>
</div>

<div class="command-block">
  <h4>Ansible Roles - Reusable Automation</h4>
  <pre><code>
  # Role Directory Structure

  # Ansible Roles Structure (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Role Directory
  roles/
  nginx/
  tasks/           # Main tasks
  main.yml
  handlers/        # Event handlers
  main.yml
  templates/       # Jinja2 templates
  nginx.conf.j2
  files/           # Static files
  index.html
  vars/            # Role variables
  main.yml
  defaults/        # Default variables (lowest priority)
  main.yml
  meta/            # Role metadata (dependencies)
  main.yml

  # Create Role Structure
  ansible-galaxy init nginx        # Creates directory structure

  # Example Role -
  # Ansible Roles Structure (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Role Directory
  roles/nginx/tasks/main.yml
  ---
  - name: Install nginx
  dnf:
  name: nginx
  state: present

  - name: Copy nginx config
  template:
  src: nginx.conf.j2
  dest: /etc/nginx/nginx.conf
  notify: restart nginx

  - name: Ensure nginx is running
  service:
  name: nginx
  state: started
  enabled: yes

  # Role Handlers -
  # Ansible Roles Structure (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Role Directory
  roles/nginx/handlers/main.yml
  ---
  - name: restart nginx
  service:
  name: nginx
  state: restarted

  # Role Variables -
  # Ansible Roles Structure (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Role Directory
  roles/nginx/defaults/main.yml
  ---
  nginx_port: 80
  nginx_user: nginx
  nginx_worker_processes: auto
  nginx_worker_connections: 1024

  # Using Roles in Playbook
  ---
  - name: Configure web servers
  hosts: webservers
  roles:
  - common           # Apply common role first
  - nginx            # Then nginx role
  - { role: mysql, mysql_port: 3306 }  # Role with parameters

  # Role Dependencies -
  # Ansible Roles Structure (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Role Directory
  roles/wordpress/meta/main.yml
  ---
  dependencies:
  - role: common
  - role: nginx
  nginx_port: 80
  - role: php
  php_version: 8.1
  - role: mysql
  mysql_version: 8.0

  # Ansible Galaxy - Share/Download Roles
  ansible-galaxy install geerlingguy.nginx           # Install from Galaxy
  ansible-galaxy install -r requirements.yml         # Install from file

  # requirements.yml
  ---
  roles:
  - name: geerlingguy.nginx
  version: 3.1.4
  - src: https://github.com/company/ansible-role-app.git
  name: company-app
  version: main

  # Role Best Practices:
  # 1. Keep roles focused (single responsibility)
  # 2. Use defaults for all variables
  # 3. Document role in README.md
  # 4. Test roles independently
  # 5. Version control roles separately
  # 6. Use Galaxy for common roles (don't reinvent)</code></pre>
</div>

<div class="command-block">
  <h4>Ansible Variables & Templates - Dynamic Configuration</h4>
  <pre><code>
  # Variable Precedence (lowest to highest):
  # 1. role defaults
  # 2. inventory file/script group vars
  # 3. inventory group_vars/all
  # 4. playbook group_vars/all
  # 5. inventory group_vars/*
  # 6. playbook group_vars/*
  # 7. inventory file/script host vars
  # 8. inventory host_vars/*
  # 9. playbook host_vars/*
  # 10. host facts
  # 11. play vars
  # 12. play vars_prompt
  # 13. play vars_files
  # 14. role vars (defined in role/vars/main.yml)
  # 15. block vars
  # 16. task vars
  # 17. include_vars
  # 18. set_facts / registered vars
  # 19. role (and include_role) params
  # 20. include params
  # 21. extra vars (command line -e)

  # Variable Files - group_vars/webservers.yml
  ---
  http_port: 80
  https_port: 443
  ssl_certificate: /etc/ssl/certs/server.crt
  ssl_key: /etc/ssl/private/server.key
  max_connections: 1000

  # Host-Specific Variables - host_vars/
  # Web Server Configuration (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Server Entry
  web1.example.com.yml
  ---
  server_id: 1
  backup_enabled: true
  monitoring_agent: datadog

  # Jinja2 Templates - templates/nginx.conf.j2
  user {{ nginx_user }};
  worker_processes {{ nginx_worker_processes }};

  events {
  worker_connections {{ nginx_worker_connections }};
  }

  http {
  server {
  listen {{ http_port }};
  server_name {{ ansible_hostname }};

  {% if ssl_enabled %}
  listen {{ https_port }} ssl;
  ssl_certificate {{ ssl_certificate }};
  ssl_certificate_key {{ ssl_key }};
  {% endif %}

  location / {
  proxy_pass http://{{ backend_server }}:{{ backend_port }};
  }
  }
  }

  # Using Templates in Tasks
  - name: Configure nginx
  template:
  src: nginx.conf.j2
  dest: /etc/nginx/nginx.conf
  owner: root
  group: root
  mode: '0644'
  validate: 'nginx -t -c %s'  # Validate before deploying
  notify: restart nginx

  # Ansible Facts (Auto-discovered variables)
  - name: Display facts
  debug:
  msg: |
  OS: {{ ansible_distribution }} {{ ansible_distribution_version }}
  IP: {{ ansible_default_ipv4.address }}
  CPU: {{ ansible_processor_cores }}
  RAM: {{ ansible_memtotal_mb }} MB
  Hostname: {{ ansible_hostname }}

  # Custom Facts - /etc/ansible/facts.d/custom.fact
  [application]
  version=2.1.5
  environment=production

  # Accessing Custom Facts
  {{ ansible_local.custom.application.version }}

  # Registered Variables (Capture task output)
  - name: Check disk space
  shell: df -h /
  register: disk_output

  - name: Show disk space
  debug:
  var: disk_output.stdout_lines

  # Set Facts (Define variables dynamically)
  - name: Set environment-based variables
  set_fact:
  db_host: "{{ 'prod-db.example.com' if environment == 'production' else 'dev-db.example.com' }}"
  cache_ttl: "{{ 3600 if environment == 'production' else 60 }}"

  # Prompting for Variables
  - name: Deploy application
  hosts: webservers
  vars_prompt:
  - name: app_version
  prompt: "Enter application version"
  private: no
  - name: db_password
  prompt: "Enter database password"
  private: yes  # Hide input</code></pre>
</div>

<div class="command-block">
  <h4>Ansible Vault - Secrets Management</h4>
  <pre><code>
  # Create Encrypted File

  # Ansible Vault Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Encryption Management
  ansible-vault create secrets.yml                                 # Create encrypted file
  # Enter password, then edit file:
  ---
  db_password: MySecretPassword123
  api_key: abc123xyz789
  aws_access_key: AKIAIOSFODNN7EXAMPLE

  # Encrypt Existing File

  # Ansible Vault Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Encryption Management
  ansible-vault encrypt group_vars/production.yml

  # Edit Encrypted File

  # Ansible Vault Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Encryption Management
  ansible-vault edit secrets.yml

  # Decrypt File

  # Ansible Vault Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Encryption Management
  ansible-vault decrypt secrets.yml

  # View Encrypted File (without decrypting)

  # Ansible Vault Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Encryption Management
  ansible-vault view secrets.yml

  # Change Vault Password

  # Ansible Vault Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Encryption Management
  ansible-vault rekey secrets.yml

  # Using Vault in Playbooks

  # Ansible Playbook Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Execute Playbooks
  ansible-playbook site.yml --ask-vault-pass
  # OR

  # Ansible Playbook Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Execute Playbooks
  ansible-playbook site.yml --vault-password-file ~/.vault_pass

  # Vault Password File (~/.vault_pass)
  MyVaultPassword123
  # Secure it:
  chmod 600 ~/.vault_pass

  # Encrypting Specific Variables (inline)
  # vars.yml
  ---
  public_key: ssh-rsa AAAAB3NzaC1...
  private_key: !vault |
  $ANSIBLE_VAULT;1.1;AES256
  66386439653765386265663837353...

  # Encrypt String

  # Ansible Vault Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Encryption Management
  ansible-vault encrypt_string 'MySecretPassword' --name 'db_password'
  # Output:
  db_password: !vault |
  $ANSIBLE_VAULT;1.1;AES256
  66386439653765386265663837353...

  # Multiple Vault IDs (Different passwords for different environments)

  # Ansible Playbook Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Execute Playbooks
  ansible-playbook site.yml --vault-id dev@prompt --vault-id prod@~/.vault_pass_prod

  # Best Practices:
  # 1. Never commit unencrypted secrets
  # 2. Use different vault passwords per environment
  # 3. Store vault password in CI/CD secrets
  # 4. Encrypt only sensitive data, not entire files
  # 5. Regular password rotation
  # 6. Use external secret managers (HashiCorp Vault) for production</code></pre>
</div>

<div class="command-block">
  <h4>Real-World Ansible Scenarios - Job Interview Ready</h4>
  <pre><code>
  # Scenario 1: Zero-Downtime Deployment
  ---
  - name: Rolling deployment
  hosts: webservers
  serial: 1                      # Deploy one server at a time
  max_fail_percentage: 0         # Fail if ANY server fails

  pre_tasks:
  - name: Remove from load balancer
  command: /usr/local/bin/remove_from_lb.sh {{ inventory_hostname }}
  delegate_to: localhost

  - name: Wait for connections to drain
  wait_for:
  timeout: 30

  tasks:
  - name: Deploy new application version
  git:
  repo: https://github.com/company/app.git
  dest: /var/www/app
  version: "{{ app_version }}"

  - name: Restart application
  systemd:
  name: myapp
  state: restarted

  - name: Wait for application to be ready
  wait_for:
  port: 8080
  delay: 5
  timeout: 60

  post_tasks:
  - name: Add back to load balancer
  command: /usr/local/bin/add_to_lb.sh {{ inventory_hostname }}
  delegate_to: localhost

  - name: Run smoke tests
  uri:
  url: http://{{ inventory_hostname }}:8080/health
  status_code: 200

  # Scenario 2: Conditional Execution Based on OS
  ---
  - name: Install packages across different OS
  hosts: all
  tasks:
  - name: Install nginx on RedHat
  dnf:
  name: nginx
  state: present
  when: ansible_os_family == "RedHat"

  - name: Install nginx on Debian
  apt:
  name: nginx
  state: present
  when: ansible_os_family == "Debian"

  - name: Configure nginx (all systems)
  template:
  src: nginx.conf.j2
  dest: "{{ '/etc/nginx/nginx.conf' if ansible_os_family == 'RedHat' else '/etc/nginx/sites-available/default' }}"

  # Scenario 3: Database Backup Automation
  ---
  - name: Automated MySQL backup
  hosts: databases
  become: yes
  vars:
  backup_dir: /backup/mysql
  retention_days: 7

  tasks:
  - name: Create backup directory
  file:
  path: "{{ backup_dir }}"
  state: directory
  mode: '0700'

  - name: Perform MySQL dump
  mysql_db:
  name: all
  state: dump
  target: "{{ backup_dir }}/backup_{{ ansible_date_time.iso8601_basic_short }}.sql.gz"
  register: backup_result

  - name: Remove old backups
  find:
  paths: "{{ backup_dir }}"
  age: "{{ retention_days }}d"
  patterns: "backup_*.sql.gz"
  register: old_backups

  - name: Delete old backup files
  file:
  path: "{{ item.path }}"
  state: absent
  loop: "{{ old_backups.files }}"

  - name: Send backup to S3
  aws_s3:
  bucket: company-backups
  object: mysql/{{ backup_result.target | basename }}
  src: "{{ backup_result.target }}"
  mode: put
  delegate_to: localhost

  # Scenario 4: User Management Across Fleet
  ---
  - name: Manage users across all servers
  hosts: all
  become: yes
  vars:
  users_to_create:
  - { name: 'alice', groups: 'wheel', ssh_key: '~/.ssh/alice.pub' }
  - { name: 'bob', groups: 'developers', ssh_key: '~/.ssh/bob.pub' }
  users_to_remove:
  - 'charlie'
  - 'david'

  tasks:
  - name: Create users
  user:
  name: "{{ item.name }}"
  groups: "{{ item.groups }}"
  shell: /bin/bash
  create_home: yes
  loop: "{{ users_to_create }}"

  - name: Add SSH keys
  authorized_key:
  user: "{{ item.name }}"
  key: "{{ lookup('file', item.ssh_key) }}"
  loop: "{{ users_to_create }}"

  - name: Remove old users
  user:
  name: "{{ item }}"
  state: absent
  remove: yes
  loop: "{{ users_to_remove }}"

  # Scenario 5: Security Hardening
  ---
  - name: Harden SSH configuration
  hosts: all
  become: yes
  tasks:
  - name: Configure SSH daemon
  lineinfile:
  path: /etc/ssh/sshd_config
  regexp: "{{ item.regexp }}"
  line: "{{ item.line }}"
  state: present
  loop:
  - { regexp: '^PermitRootLogin', line: 'PermitRootLogin no' }
  - { regexp: '^PasswordAuthentication', line: 'PasswordAuthentication no' }
  - { regexp: '^Port', line: 'Port 2222' }
  - { regexp: '^MaxAuthTries', line: 'MaxAuthTries 3' }
  notify: restart sshd

  - name: Install fail2ban
  dnf:
  name: fail2ban
  state: present

  - name: Configure fail2ban
  copy:
  src: jail.local
  dest: /etc/fail2ban/jail.local
  notify: restart fail2ban

  handlers:
  - name: restart sshd
  systemd:
  name: sshd
  state: restarted

  - name: restart fail2ban
  systemd:
  name: fail2ban
  state: restarted
  enabled: yes</code></pre>
</div>

<div class="command-block">
  <h4>Ansible Best Practices - Production Standards</h4>
  <pre><code>
  # Directory Structure (Standard Layout)
  ansible-project/
  ├── ansible.cfg              # Ansible configuration
  ├── inventory/
  │   ├── production/
  │   │   ├── hosts
  │   │   └── group_vars/
  │   │       ├── all.yml
  │   │       ├── webservers.yml
  │   │       └── databases.yml
  │   └── staging/
  │       ├── hosts
  │       └── group_vars/
  ├──
  # Ansible Roles Structure (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Role Directory
  roles/
  │   ├── common/
  │   ├── nginx/
  │   └── mysql/
  ├── playbooks/
  │   ├── site.yml             # Master playbook
  │   ├── webservers.yml
  │   └── databases.yml
  ├── group_vars/
  │   └── all/
  │       ├── vars.yml
  │       └── vault.yml        # Encrypted
  ├── host_vars/
  ├── files/
  ├── templates/
  └── README.md

  # ansible.cfg (Project Configuration)
  [defaults]
  inventory = ./inventory/production/hosts
  remote_user = ansible
  private_key_file = ~/.ssh/ansible_key
  host_key_checking = False
  retry_files_enabled = False
  gathering = smart
  fact_caching = jsonfile
  fact_caching_connection = /tmp/ansible_facts
  fact_caching_timeout = 86400
  callback_whitelist = profile_tasks, timer

  [privilege_escalation]
  become = True
  become_method = sudo
  become_user = root
  become_ask_pass = False

  [ssh_connection]
  pipelining = True
  control_path = /tmp/ansible-ssh-%%h-%%p-%%r

  # Playbook Best Practices:
  # 1. Use roles for reusability
  # 2. One task = one action (atomic operations)
  # 3. Always use 'name' for tasks (documentation)
  # 4. Use 'changed_when' to control change status
  # 5. Tag tasks for selective execution
  # 6. Use 'check_mode' for validation
  # 7. Implement handlers for service restarts
  # 8. Use 'become' only when needed (security)
  # 9. Validate templates before deployment
  # 10. Keep playbooks idempotent

  # Testing Strategies:
  # 1. Syntax check

  # Ansible Playbook Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Execute Playbooks
  ansible-playbook site.yml --syntax-check

  # 2. Dry run (check mode)

  # Ansible Playbook Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Execute Playbooks
  ansible-playbook site.yml --check

  # 3. Diff mode (show changes)

  # Ansible Playbook Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Execute Playbooks
  ansible-playbook site.yml --check --diff

  # 4. Limit to specific hosts

  # Ansible Playbook Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Execute Playbooks
  ansible-playbook site.yml --limit webservers

  # 5. Step through tasks

  # Ansible Playbook Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Execute Playbooks
  ansible-playbook site.yml --step

  # 6. Start at specific task

  # Ansible Playbook Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Execute Playbooks
  ansible-playbook site.yml --start-at-task="Install nginx"

  # Performance Optimization:
  # 1. Enable pipelining (reduce SSH overhead)
  # 2. Use fact caching (avoid gathering every run)
  # 3. Disable fact gathering when not needed
  # 4. Use async for long-running tasks
  # 5. Increase forks (parallel execution)
  # 6. Use mitogen plugin (5-10x faster)

  # Security Best Practices:
  # 1. Use Ansible Vault for secrets
  # 2. Separate inventory per environment
  # 3. Use jump host for production access
  # 4. Implement RBAC (who can run what)
  # 5. Log all playbook executions
  # 6. Use --check before production runs
  # 7. Store playbooks in version control
  # 8. Review playbooks before execution

  # CI/CD Integration:
  # GitLab CI Example:
  deploy:
  stage: deploy
  script:
  -
  # Ansible Playbook Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Execute Playbooks
  ansible-playbook -i inventory/production playbooks/site.yml
  only:
  - main
  when: manual

  # Interview Questions - Ansible:
  # Q1: Difference between playbook and role?
  # A: Playbook defines tasks, roles package related tasks/templates/handlers

  # Q2: How to achieve idempotency?
  # A: Use modules that check state (package, service, file)
  #    Avoid shell/command modules when possible
  #    Use 'creates' or 'removes' with command/shell

  # Q3: How to handle sensitive data?
  # A: Ansible Vault for encryption, separate vault per environment

  # Q4: Serial vs parallel execution?
  # A: serial: 1 = rolling deployment (one at a time)
  #    default = parallel execution (all hosts simultaneously)

  # Q5: Difference between include and import?
  # A: import: Static (processed at parse time)
  #    include: Dynamic (processed at runtime)</code></pre>
</div>

<h3>Ansible Configuration</h3>

<div class="command-block">
  <h4>View configuration</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  nsible-config view                                             # View current config
  ansible-config list                                             # List all settings
  ansible-config dump                                             # Dump current settings</code></pre>
</div>

<div class="command-block">
  <h4>Configuration file locations</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  etc/ansible/ansible.cfg                                        # System-wide
  ~/.ansible.cfg                                                  # User-specific
  ./ansible.cfg                                                   # Project-specific (highest priority)</code></pre>
</div>
<h3>Ansible Performance Optimization</h3>

<div class="command-block">
  <h4>Speed up playbook execution</h4>
  <pre><code>
  # ansible.cfg optimizations
  [defaults]
  forks = 50                                                      # Parallel execution (default: 5)
  gathering = smart                                               # Smart fact gathering
  fact_caching = jsonfile                                         # Cache facts
  fact_caching_connection = /tmp/ansible_facts
  fact_caching_timeout = 86400                                    # 24 hours
  host_key_checking = False                                       # Skip SSH key verification
  pipelining = True                                               # SSH pipelining (faster)

  # Disable fact gathering when not needed

  # Ansible Playbook Operations (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Execute Playbooks
  ansible-playbook playbook.yml --skip-tags facts
  # OR in playbook:
  gather_facts: no</code></pre>
</div>

<div class="command-block">
  <h4>Mitogen plugin (5-10x faster)</h4>
  <pre><code>
  # Enterprise Standards
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  ip install mitogen
  # ansible.cfg
  [defaults]
  strategy_plugins = /path/to/mitogen/ansible_mitogen/plugins/strategy
  strategy = mitogen_linear</code></pre>
</div>
</div>

<h2 id="terraform">33. TERRAFORM COMMANDS</h2>

<div>
  <h3>Terraform Initialization</h3>

  <div class="command-block">
    <h4>Initialize working directory</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    terraform init                                                  # Initialize Terraform

    # Terraform Init Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Initialize
    terraform init -upgrade                                         # Upgrade providers

    # Terraform Init Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Initialize
    terraform init -reconfigure                                     # Reconfigure backend

    # Terraform Init Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Initialize
    terraform init -migrate-state                                   # Migrate state</code></pre>
  </div>

  <h3>Terraform Planning</h3>

  <div class="command-block">
    <h4>Create execution plan</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    terraform plan                                                  # Show execution plan

    # Terraform Plan Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Plan Changes
    terraform plan -out=plan.out                                    # Save plan to file

    # Terraform Plan Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Plan Changes
    terraform plan -var="key=value"                                 # Pass variables

    # Terraform Plan Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Plan Changes
    terraform plan -var-file="vars.tfvars"                          # Use variable file

    # Terraform Plan Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Plan Changes
    terraform plan -target=resource.name                            # Target specific resource

    # Terraform Plan Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Plan Changes
    terraform plan -refresh=false                                   # Don't refresh state

    # Terraform Plan Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Plan Changes
    terraform plan -destroy                                         # Plan destroy</code></pre>
  </div>

  <h3>Terraform Apply</h3>

  <div class="command-block">
    <h4>Apply changes</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    terraform apply                                                 # Apply with confirmation

    # Terraform Apply Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Apply Changes
    terraform apply -auto-approve                                   # Apply without confirmation

    # Terraform Apply Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Apply Changes
    terraform apply plan.out                                        # Apply saved plan

    # Terraform Apply Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Apply Changes
    terraform apply -var="key=value"                                # With variables

    # Terraform Apply Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Apply Changes
    terraform apply -target=resource.name                           # Target specific resource

    # Terraform Apply Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Apply Changes
    terraform apply -parallelism=10                                 # Set parallelism</code></pre>
  </div>

  <h3>Terraform Destroy</h3>

  <div class="command-block">
    <h4>Destroy infrastructure</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    terraform destroy                                               # Destroy with confirmation
    terraform destroy -auto-approve                                 # Destroy without confirmation
    terraform destroy -target=resource.name                         # Destroy specific resource</code></pre>
  </div>

  <h3>Terraform State</h3>
  <div class="command-block">
    <h4>State Management - Critical Safety Guidelines</h4>
    <pre><code>
    # NEVER manually edit state files directly
    # State file corruption can destroy infrastructure

    # Always backup state before modifications
    terraform state pull > backup-$(date +%Y%m%d-%H%M%S).tfstate

    # State operations are DANGEROUS in production:
    # - terraform state rm: Removes from tracking (doesn't destroy)
    # - terraform state mv: Can break resource references
    # - Manual state edits: HIGH RISK of data loss

    # Best Practices:
    # 1. Use remote state (S3, Terraform Cloud) with locking
    # 2. Enable state versioning
    # 3. Test state operations in dev environment first
    # 4. Always review changes with '
    # Terraform Plan Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Plan Changes
    terraform plan' before 'apply'
    # 5. Use workspaces for environment isolation

    # State locking prevents concurrent modifications
    # Without locking, multiple users can corrupt state</code></pre>
  </div>
  <div class="command-block">
    <h4>Remote state backend (S3 example)</h4>
    <pre><code>
    # backend.tf
    terraform {
    backend "s3" {
    bucket         = "my-terraform-state"
    key            = "prod/terraform.tfstate"
    region         = "us-east-1"
    encrypt        = true
    dynamodb_table = "terraform-locks"                          # State locking
    }
    }

    # Initialize with backend

    # Terraform Init Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Initialize
    terraform init

    # Migrate state to remote backend

    # Terraform Init Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Initialize
    terraform init -migrate-state

    # Benefits:
    # - Shared state for team collaboration
    # - State locking (prevents concurrent modifications)
    # - Encryption at rest
    # - Version history (S3 versioning)</code></pre>
  </div>

  <div class="command-block">
    <h4>State management</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    terraform state list                                            # List resources in state
    terraform state show resource.name                              # Show resource details
    terraform state pull                                            # Pull remote state
    terraform state push                                            # Push local state
    terraform state mv source destination                           # Move resource
    terraform state rm resource.name                                # Remove resource from state
    terraform state replace-provider old new                        # Replace provider</code></pre>
  </div>

  <div class="command-block">
    <h4>State backup</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    terraform state list > state_backup.txt</code></pre>
  </div>

  <h3>Terraform Workspace</h3>

  <div class="command-block">
    <h4>Workspace management</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    terraform workspace list                                        # List workspaces
    terraform workspace show                                        # Show current workspace
    terraform workspace new dev                                     # Create new workspace
    terraform workspace select dev                                  # Switch workspace
    terraform workspace delete dev                                  # Delete workspace</code></pre>
  </div>
  <div class="command-block">
    <h4>Workspace use cases</h4>
    <pre><code># Workspaces for environment separation (development, staging, production)
    terraform workspace new dev
    terraform workspace new staging
    terraform workspace new prod

    # Use workspace name in configuration
    resource "aws_instance" "example" {
    tags = {
    Environment = terraform.workspace
    Name        = "server-${terraform.workspace}"
    }
    }

    # Switch between environments
    terraform workspace select dev

    # Terraform Apply Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Apply Changes
    terraform apply                                                 # Applies only to dev workspace

    # Best Practices:
    # - Use workspaces for similar infrastructure across environments
    # - Don't use workspaces for completely different infrastructure
    # - Consider separate state files for production (safer)
    # - Workspaces share same backend configuration</code></pre>
  </div>

  <h3>Terraform Output</h3>

  <div class="command-block">
    <h4>View outputs</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    terraform output                                                # Show all outputs
    terraform output output_name                                    # Show specific output
    terraform output -json                                          # JSON format
    terraform output -raw output_name                               # Raw value (no quotes)</code></pre>
  </div>

  <h3>Terraform Validation</h3>

  <div class="command-block">
    <h4>Validate configuration</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    terraform validate                                              # Validate configuration
    terraform fmt                                                   # Format configuration files
    terraform fmt -check                                            # Check if formatting needed
    terraform fmt -recursive                                        # Format recursively
    terraform fmt -diff                                             # Show formatting changes</code></pre>
  </div>

  <h3>Terraform Console</h3>

  <div class="command-block">
    <h4>Interactive console</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    terraform console                                               # Launch interactive console
    # In console, can test expressions:
    # > var.instance_type
    # > aws_instance.example.id</code></pre>
  </div>

  <h3>Terraform Import</h3>

  <div class="command-block">
    <h4>Import existing resources</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    terraform import resource.name id                               # Import resource
    terraform import aws_instance.example i-1234567890abcdef0</code></pre>
  </div>

  <h3>Terraform Graph</h3>

  <div class="command-block">
    <h4>Generate dependency graph</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    terraform graph                                                 # Output DOT format
    terraform graph | dot -Tpng > graph.png                         # Generate image</code></pre>
  </div>

  <h3>Terraform Providers</h3>

  <div class="command-block">
    <h4>Provider management</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    terraform providers                                             # Show providers
    terraform providers lock                                        # Lock provider versions
    terraform providers mirror dir                                  # Mirror providers to directory
    terraform providers schema -json                                # Show provider schema</code></pre>
  </div>

  <h3>Terraform Debug</h3>

  <div class="command-block">
    <h4>Enable debug logging</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    xport TF_LOG=TRACE                                             # Set log level (TRACE, DEBUG, INFO, WARN, ERROR)
    export TF_LOG_PATH=terraform.log                                # Log to file

    # Terraform Apply Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Apply Changes
    terraform apply                                                 # Run with logging</code></pre>
  </div>

  <h3>Terraform Taint</h3>

  <div class="command-block">
    <h4>Mark resources for recreation</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    terraform taint resource.name                                   # Taint resource (deprecated)
    terraform untaint resource.name                                 # Untaint resource (deprecated)

    # Modern replacement

    # Terraform Apply Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Apply Changes
    terraform apply -replace=resource.name                          # Force replacement</code></pre>
  </div>
</div>

<h2 id="aws">34. AWS CLI COMMANDS</h2>

<div>
  <h3>AWS CLI Configuration</h3>

  <div class="command-block">
    <h4>Understanding AWS Configuration Files</h4>
    <pre><code>
    # ~/.aws/credentials - Stores access keys (NEVER commit to git)
    [default]
    aws_access_key_id = AKIAIOSFODNN7EXAMPLE
    aws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY

    [production]
    aws_access_key_id = AKIAI44QH8DHBEXAMPLE
    aws_secret_access_key = je7MtGbClwBF/2Zp9Utk/h3yCo8nvbEXAMPLEKEY

    # ~/.aws/config - Stores region and output preferences
    [default]
    region = us-east-1
    output = json

    [profile production]
    region = us-west-2
    output = table

    # Configuration precedence (highest to lowest):
    # 1. Command line options (--region us-west-2)
    # 2. Environment variables (AWS_REGION, AWS_ACCESS_KEY_ID)
    # 3. AWS credentials file (~/.aws/credentials)
    # 4. AWS config file (~/.aws/config)
    # 5. Instance metadata (for EC2 instances with IAM roles)</code></pre>
  </div>

  <div class="command-block">
    <h4>Configure AWS CLI</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws configure                                                   # Interactive configuration
    aws configure list                                              # List configuration
    aws configure get region                                        # Get specific value
    aws configure set region us-west-2                              # Set specific value</code></pre>
  </div>

  <div class="command-block">
    <h4>Named profiles</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws configure --profile myprofile
    aws s3 ls --profile myprofile
    export AWS_PROFILE=myprofile                                    # Set default profile for session</code></pre>
  </div>

  <div class="command-block">
    <h4>Configuration files</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    /.aws/credentials                                              # Credentials file
    ~/.aws/config                                                   # Configuration file</code></pre>
  </div>

  <h3>AWS EC2 Commands</h3>

  <div class="command-block">
    <h4>List instances</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ec2 describe-instances                                      # All instances
    aws ec2 describe-instances --instance-ids i-1234567890abcdef0
    aws ec2 describe-instances --filters "Name=instance-state-name,Values=running"
    aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,State.Name]' --output table</code></pre>
  </div>

  <div class="command-block">
    <h4>Start/stop instances</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ec2 start-instances --instance-ids i-1234567890abcdef0
    aws ec2 stop-instances --instance-ids i-1234567890abcdef0
    aws ec2 reboot-instances --instance-ids i-1234567890abcdef0
    aws ec2 terminate-instances --instance-ids i-1234567890abcdef0</code></pre>
  </div>

  <div class="command-block">
    <h4>Create instance</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ec2 run-instances --image-id ami-12345678 --instance-type t2.micro --key-name MyKey</code></pre>
  </div>

  <div class="command-block">
    <h4>Describe images (AMIs)</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ec2 describe-images --owners self
    aws ec2 describe-images --image-ids ami-12345678</code></pre>
  </div>

  <div class="command-block">
    <h4>Security groups</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ec2 describe-security-groups
    aws ec2 create-security-group --group-name MyGroup --description "My security group"
    aws ec2 authorize-security-group-ingress --group-id sg-12345678 --protocol tcp --port 22 --cidr 0.0.0.0/0</code></pre>
  </div>

  <div class="command-block">
    <h4>Key pairs</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ec2 describe-key-pairs
    aws ec2 create-key-pair --key-name MyKey --query 'KeyMaterial' --output text > MyKey.pem</code></pre>
  </div>

  <div class="command-block">
    <h4>EBS volumes</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ec2 describe-volumes
    aws ec2 create-volume --size 10 --availability-zone us-west-2a
    aws ec2 attach-volume --volume-id vol-12345678 --instance-id i-1234567890abcdef0 --device /dev/sdf
    aws ec2 detach-volume --volume-id vol-12345678</code></pre>
  </div>

  <div class="command-block">
    <h4>Snapshots</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ec2 describe-snapshots --owner-ids self
    aws ec2 create-snapshot --volume-id vol-12345678 --description "My snapshot"
    aws ec2 delete-snapshot --snapshot-id snap-12345678</code></pre>
  </div>

  <h3>AWS S3 Commands</h3>

  <div class="command-block">
    <h4>List buckets</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws s3 ls                                                       # List all buckets
    aws s3 ls s3://bucket-name                                      # List bucket contents
    aws s3 ls s3://bucket-name/prefix/                              # List with prefix</code></pre>
  </div>

  <div class="command-block">
    <h4>Copy files</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws s3 cp file.txt s3://bucket-name/                            # Upload file
    aws s3 cp s3://bucket-name/file.txt ./                          # Download file
    aws s3 cp s3://bucket1/file.txt s3://bucket2/                   # Copy between buckets
    aws s3 cp directory s3://bucket-name/ --recursive               # Upload directory</code></pre>
  </div>

  <div class="command-block">
    <h4>Sync</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws s3 sync directory s3://bucket-name/                         # Sync directory to S3
    aws s3 sync s3://bucket-name/ directory                         # Sync S3 to directory
    aws s3 sync s3://bucket1/ s3://bucket2/                         # Sync between buckets</code></pre>
  </div>

  <div class="command-block">
    <h4>Move files</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws s3 mv file.txt s3://bucket-name/                            # Move file
    aws s3 mv s3://bucket-name/file.txt ./                          # Move from S3</code></pre>
  </div>

  <div class="command-block">
    <h4>Remove files</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws s3 rm s3://bucket-name/file.txt                             # Delete file
    aws s3 rm s3://bucket-name/ --recursive                         # Delete all objects</code></pre>
  </div>

  <div class="command-block">
    <h4>Bucket operations</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws s3 mb s3://bucket-name                                      # Create bucket
    aws s3 rb s3://bucket-name                                      # Remove empty bucket
    aws s3 rb s3://bucket-name --force                              # Remove bucket with contents</code></pre>
  </div>

  <div class="command-block">
    <h4>S3 API commands</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws s3api list-buckets                                          # List buckets (API)
    aws s3api get-object --bucket bucket-name --key file.txt output.txt
    aws s3api put-object --bucket bucket-name --key file.txt --body file.txt
    aws s3api delete-object --bucket bucket-name --key file.txt</code></pre>
  </div>

  <div class="command-block">
    <h4>Bucket policies</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws s3api get-bucket-policy --bucket bucket-name
    aws s3api put-bucket-policy --bucket bucket-name --policy file://policy.json</code></pre>
  </div>

  <div class="command-block">
    <h4>Bucket versioning</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws s3api get-bucket-versioning --bucket bucket-name
    aws s3api put-bucket-versioning --bucket bucket-name --versioning-configuration Status=Enabled</code></pre>
  </div>

  <h3>AWS IAM Commands</h3>

  <div class="command-block">
    <h4>Users</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws iam list-users                                              # List all users
    aws iam get-user --user-name username                           # Get user details
    aws iam create-user --user-name username
    aws iam delete-user --user-name username</code></pre>
  </div>

  <div class="command-block">
    <h4>Access keys</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws iam list-access-keys --user-name username
    aws iam create-access-key --user-name username
    aws iam delete-access-key --user-name username --access-key-id AKIAIOSFODNN7EXAMPLE</code></pre>
  </div>

  <div class="command-block">
    <h4>Groups</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws iam list-groups                                             # List all groups
    aws iam create-group --group-name groupname
    aws iam add-user-to-group --user-name username --group-name groupname
    aws iam remove-user-from-group --user-name username --group-name groupname</code></pre>
  </div>

  <div class="command-block">
    <h4>Policies</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws iam list-policies                                           # List all policies
    aws iam get-policy --policy-arn arn:aws:iam::aws:policy/AdministratorAccess
    aws iam list-attached-user-policies --user-name username
    aws iam attach-user-policy --user-name username --policy-arn arn</code></pre>
  </div>

  <div class="command-block">
    <h4>Roles</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws iam list-roles                                              # List all roles
    aws iam get-role --role-name rolename
    aws iam create-role --role-name rolename --assume-role-policy-document file://trust-policy.json</code></pre>
  </div>

  <h3>AWS RDS Commands</h3>

  <div class="command-block">
    <h4>DB instances</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws rds describe-db-instances                                   # List all DB instances
    aws rds describe-db-instances --db-instance-identifier mydb
    aws rds create-db-instance --db-instance-identifier mydb --db-instance-class db.t2.micro --engine mysql --master-username admin --master-user-password password
    aws rds delete-db-instance --db-instance-identifier mydb --skip-final-snapshot
    aws rds reboot-db-instance --db-instance-identifier mydb</code></pre>
  </div>

  <div class="command-block">
    <h4>Snapshots</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws rds describe-db-snapshots
    aws rds create-db-snapshot --db-instance-identifier mydb --db-snapshot-identifier mydb-snapshot
    aws rds delete-db-snapshot --db-snapshot-identifier mydb-snapshot</code></pre>
  </div>

  <h3>AWS Lambda Commands</h3>

  <div class="command-block">
    <h4>List functions</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws lambda list-functions                                       # List all functions</code></pre>
  </div>

  <div class="command-block">
    <h4>Get function</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws lambda get-function --function-name myfunction</code></pre>
  </div>

  <div class="command-block">
    <h4>Create function</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws lambda create-function --function-name myfunction --runtime python3.9 --role arn --handler lambda_function.lambda_handler --zip-file fileb://function.zip</code></pre>
  </div>

  <div class="command-block">
    <h4>Update function code</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws lambda update-function-code --function-name myfunction --zip-file fileb://function.zip</code></pre>
  </div>

  <div class="command-block">
    <h4>Invoke function</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws lambda invoke --function-name myfunction output.txt
    aws lambda invoke --function-name myfunction --payload '{"key":"value"}' output.txt</code></pre>
  </div>

  <div class="command-block">
    <h4>Delete function</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws lambda delete-function --function-name myfunction</code></pre>
  </div>

  <h3>AWS CloudWatch Commands</h3>

  <div class="command-block">
    <h4>Logs</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws logs describe-log-groups                                    # List log groups
    aws logs describe-log-streams --log-group-name /aws/lambda/myfunction
    aws logs get-log-events --log-group-name /aws/lambda/myfunction --log-stream-name stream-name
    aws logs filter-log-events --log-group-name /aws/lambda/myfunction --filter-pattern "ERROR"</code></pre>
  </div>

  <div class="command-block">
    <h4>Metrics</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws cloudwatch list-metrics                                     # List all metrics
    aws cloudwatch get-metric-statistics --namespace AWS/EC2 --metric-name CPUUtilization --dimensions Name=InstanceId,Value=i-1234567890abcdef0 --start-time 2024-01-01T00:00:00Z --end-time 2024-01-02T00:00:00Z --period 3600 --statistics Average</code></pre>
  </div>

  <div class="command-block">
    <h4>Alarms</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws cloudwatch describe-alarms
    aws cloudwatch put-metric-alarm --alarm-name cpu-alarm --alarm-description "CPU alarm" --metric-name CPUUtilization --namespace AWS/EC2 --statistic Average --period 300 --threshold 80 --comparison-operator GreaterThanThreshold
    aws cloudwatch delete-alarms --alarm-names cpu-alarm</code></pre>
  </div>

  <h3>AWS VPC Commands</h3>

  <div class="command-block">
    <h4>VPCs</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ec2 describe-vpcs                                           # List VPCs
    aws ec2 create-vpc --cidr-block 10.0.0.0/16
    aws ec2 delete-vpc --vpc-id vpc-12345678</code></pre>
  </div>

  <div class="command-block">
    <h4>Subnets</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ec2 describe-subnets                                        # List subnets
    aws ec2 create-subnet --vpc-id vpc-12345678 --cidr-block 10.0.1.0/24
    aws ec2 delete-subnet --subnet-id subnet-12345678</code></pre>
  </div>

  <div class="command-block">
    <h4>Internet Gateways</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ec2 describe-internet-gateways
    aws ec2 create-internet-gateway
    aws ec2 attach-internet-gateway --internet-gateway-id igw-12345678 --vpc-id vpc-12345678
    aws ec2 detach-internet-gateway --internet-gateway-id igw-12345678 --vpc-id vpc-12345678</code></pre>
  </div>

  <div class="command-block">
    <h4>Route tables</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ec2 describe-route-tables
    aws ec2 create-route --route-table-id rtb-12345678 --destination-cidr-block 0.0.0.0/0 --gateway-id igw-12345678</code></pre>
  </div>
  <h3>AWS CloudFormation Commands</h3>

  <div class="command-block">
    <h4>Stack management</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws cloudformation create-stack --stack-name mystack --template-body file://template.yml
    aws cloudformation update-stack --stack-name mystack --template-body file://template.yml
    aws cloudformation delete-stack --stack-name mystack
    aws cloudformation describe-stacks --stack-name mystack         # Stack details
    aws cloudformation list-stacks                                  # List all stacks
    aws cloudformation list-stack-resources --stack-name mystack    # List stack resources</code></pre>
  </div>

  <div class="command-block">
    <h4>Stack validation</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws cloudformation validate-template --template-body file://template.yml  # Validate template</code></pre>
  </div>

  <div class="command-block">
    <h4>Change sets (preview changes)</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws cloudformation create-change-set --stack-name mystack --change-set-name mychangeset --template-body file://template.yml
    aws cloudformation describe-change-set --change-set-name mychangeset --stack-name mystack
    aws cloudformation execute-change-set --change-set-name mychangeset --stack-name mystack
    aws cloudformation delete-change-set --change-set-name mychangeset --stack-name mystack</code></pre>
  </div>
  <h3>AWS ECS Commands</h3>

  <div class="command-block">
    <h4>Cluster management</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ecs list-clusters                                           # List ECS clusters
    aws ecs create-cluster --cluster-name mycluster                 # Create cluster
    aws ecs describe-clusters --clusters mycluster                  # Cluster details
    aws ecs delete-cluster --cluster mycluster                      # Delete cluster</code></pre>
  </div>

  <div class="command-block">
    <h4>Task definitions</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ecs register-task-definition --cli-input-json file://task-definition.json  # Register task definition
    aws ecs list-task-definitions                                   # List task definitions
    aws ecs describe-task-definition --task-definition mytask:1     # Task definition details
    aws ecs deregister-task-definition --task-definition mytask:1   # Deregister task definition</code></pre>
  </div>

  <div class="command-block">
    <h4>Services</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ecs create-service --cluster mycluster --service-name myservice --task-definition mytask --desired-count 2
    aws ecs list-services --cluster mycluster                                         # List services
    aws ecs describe-services --cluster mycluster --services myservice                # Service details
    aws ecs update-service --cluster mycluster --service myservice --desired-count 5  # Scale service
    aws ecs delete-service --cluster mycluster --service myservice                    # Delete service</code></pre>
  </div>

  <div class="command-block">
    <h4>Tasks</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ecs run-task --cluster mycluster --task-definition mytask   # Run task
    aws ecs list-tasks --cluster mycluster                          # List running tasks
    aws ecs describe-tasks --cluster mycluster --tasks task-id      # Task details
    aws ecs stop-task --cluster mycluster --task task-id            # Stop task</code></pre>
  </div>

  <h3>AWS CLI Output Formatting</h3>

  <div class="command-block">
    <h4>Output formats</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ec2 describe-instances --output json                        # JSON (default)
    aws ec2 describe-instances --output text                        # Text
    aws ec2 describe-instances --output table                       # Table
    aws ec2 describe-instances --output yaml                        # YAML</code></pre>
  </div>

  <div class="command-block">
    <h4>Query with JMESPath</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,State.Name,PrivateIpAddress]' --output table
    aws ec2 describe-instances --query 'Reservations[*].Instances[?State.Name==`running`].[InstanceId]' --output text</code></pre>
  </div>
  <h3>Advanced AWS Cloud - Remote Job Ready Skills</h3>

  <div class="command-block">
    <h4>Why AWS Mastery = Remote Job Entry</h4>
    <pre><code># AWS Market Demand (Bangladesh):
    # Linux SysAdmin: (1-2 years)
    # Linux + AWS Basics: (1-2 years)
    # Linux + AWS + Automation: (1-2 years)
    # Remote AWS DevOps: (1-2 years)

    # Companies Using AWS in Bangladesh:
    # - 70% of tech companies
    # - All major banks (BRAC, City, Dutch-Bangla)
    # - Telecom (Grameenphone, Robi)
    # - E-commerce (Daraz, Pathao, Foodpanda)
    # - Startups (90%+ use AWS)

    # AWS Skills That Get You Hired:
    # 1. EC2 (Compute) - ✅ MUST KNOW
    # 2. VPC (Networking) - ✅ MUST KNOW
    # 3. IAM (Security) - ✅ MUST KNOW
    # 4. S3 + EBS (Storage) - ✅ MUST KNOW
    # 5. RDS (Database) - Important
    # 6. CloudWatch (Monitoring) - Important
    # 7. Lambda (Serverless) - Good to have
    # 8. ECS/EKS (Containers) - Advanced

    # Interview Focus Areas:
    # - EC2 instance types and pricing
    # - VPC subnet design (public/private)
    # - IAM roles vs users
    # - S3 bucket policies
    # - Security group vs NACL
    # - EBS vs instance store
    # - High availability patterns</code></pre>
  </div>

  <div class="command-block">
    <h4>AWS EC2 - Production Server Management (CRITICAL SKILL)</h4>
    <pre><code>
    # Launch EC2 Instance (Production Pattern)
    aws ec2 run-instances \
    --image-id ami-0c55b159cbfafe1f0 \
    --instance-type t3.medium \
    --key-name my-keypair \
    --security-group-ids sg-0123456789abcdef0 \
    --subnet-id subnet-0bb1c79de3EXAMPLE \
    --user-data file://userdata.sh \
    --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=web-prod},{Key=Environment,Value=production}]' \
    --iam-instance-profile Name=EC2-S3-ReadOnly \
    --block-device-mappings '[{"DeviceName":"/dev/sda1","Ebs":{"VolumeSize":50,"VolumeType":"gp3","DeleteOnTermination":false}}]'

    # userdata.sh (Auto-configuration on launch)
    #!/bin/bash
    yum update -y
    yum install -y nginx
    systemctl start nginx
    systemctl enable nginx
    echo "Hello from $(hostname)" > /var/www/html/index.html

    # List Instances (Production Query)
    aws ec2 describe-instances \
    --filters "Name=tag:Environment,Values=production" "Name=instance-state-name,Values=running" \
    --query 'Reservations[*].Instances[*].[InstanceId,InstanceType,PublicIpAddress,PrivateIpAddress,State.Name,Tags[?Key==`Name`].Value|[0]]' \
    --output table

    # Instance Types Explained (Interview Question):
    # t3.micro   - 2 vCPU, 1GB RAM    - Dev/Test
    # t3.medium  - 2 vCPU, 4GB RAM    - Small prod apps
    # t3.large   - 2 vCPU, 8GB RAM    - Medium workloads
    # m5.large   - 2 vCPU, 8GB RAM    - Balanced prod
    # c5.large   - 2 vCPU, 4GB RAM    - Compute-intensive
    # r5.large   - 2 vCPU, 16GB RAM   - Memory-intensive

    # Start/Stop Instances (Cost Optimization)
    aws ec2 stop-instances --instance-ids i-1234567890abcdef0
    aws ec2 start-instances --instance-ids i-1234567890abcdef0

    # Terminate Instance
    aws ec2 terminate-instances --instance-ids i-1234567890abcdef0

    # Create AMI (Golden Image for Auto Scaling)
    aws ec2 create-image \
    --instance-id i-1234567890abcdef0 \
    --name "web-server-v1.2-$(date +%Y%m%d)" \
    --description "Production web server with Nginx 1.21" \
    --no-reboot

    # Change Instance Type (Vertical Scaling)
    aws ec2 stop-instances --instance-ids i-1234567890abcdef0
    aws ec2 modify-instance-attribute \
    --instance-id i-1234567890abcdef0 \
    --instance-type "{\"Value\": \"t3.large\"}"
    aws ec2 start-instances --instance-ids i-1234567890abcdef0

    # Security Groups (Firewall Rules)
    # Create Security Group
    aws ec2 create-security-group \
    --group-name web-server-sg \
    --description "Production web server security group" \
    --vpc-id vpc-0123456789abcdef0

    # Add HTTP Rule
    aws ec2 authorize-security-group-ingress \
    --group-id sg-0123456789abcdef0 \
    --protocol tcp \
    --port 80 \
    --cidr 0.0.0.0/0

    # Add HTTPS Rule
    aws ec2 authorize-security-group-ingress \
    --group-id sg-0123456789abcdef0 \
    --protocol tcp \
    --port 443 \
    --cidr 0.0.0.0/0

    # Add SSH Rule (Restricted)
    aws ec2 authorize-security-group-ingress \
    --group-id sg-0123456789abcdef0 \
    --protocol tcp \
    --port 22 \
    --cidr 203.0.113.0/24

    # Remove Rule
    aws ec2 revoke-security-group-ingress \
    --group-id sg-0123456789abcdef0 \
    --protocol tcp \
    --port 22 \
    --cidr 0.0.0.0/0

    # Elastic IP (Static Public IP)
    # Allocate
    aws ec2 allocate-address --domain vpc

    # Associate with Instance
    aws ec2 associate-address \
    --instance-id i-1234567890abcdef0 \
    --allocation-id eipalloc-0123456789abcdef0

    # Disassociate
    aws ec2 disassociate-address --association-id eipassoc-0123456789

    # Release (Delete)
    aws ec2 release-address --allocation-id eipalloc-0123456789abcdef0

    # Key Pairs (SSH Access)
    # Create Key Pair
    aws ec2 create-key-pair \
    --key-name prod-key \
    --query 'KeyMaterial' \
    --output text > prod-key.pem

    # Secure the key
    chmod 400 prod-key.pem

    # SSH to Instance
    ssh -i prod-key.pem ec2-user@54.123.45.67

    # Interview Question: How to connect to EC2 without key pair?
    # Answer: Use Session Manager (AWS Systems Manager)
    aws ssm start-session --target i-1234567890abcdef0</code></pre>
  </div>

  <div class="command-block">
    <h4>AWS VPC - Network Architecture (HIGH DEMAND SKILL)</h4>
    <pre><code>
    # VPC Architecture for Production:
    #
    # VPC: 10.0.0.0/16
    # ├── Availability Zone A
    # │   ├── Public Subnet: 10.0.1.0/24 (Load Balancer, NAT)
    # │   └── Private Subnet: 10.0.10.0/24 (App Servers)
    # └── Availability Zone B
    #     ├── Public Subnet: 10.0.2.0/24 (Load Balancer, NAT)
    #     └── Private Subnet: 10.0.11.0/24 (App Servers, Databases)

    # Create VPC
    aws ec2 create-vpc \
    --cidr-block 10.0.0.0/16 \
    --tag-specifications 'ResourceType=vpc,Tags=[{Key=Name,Value=prod-vpc}]'

    # Enable DNS Hostname
    aws ec2 modify-vpc-attribute \
    --vpc-id vpc-0123456789abcdef0 \
    --enable-dns-hostnames

    # Create Public Subnet (for Load Balancers)
    aws ec2 create-subnet \
    --vpc-id vpc-0123456789abcdef0 \
    --cidr-block 10.0.1.0/24 \
    --availability-zone us-east-1a \
    --tag-specifications 'ResourceType=subnet,Tags=[{Key=Name,Value=public-1a}]'

    # Auto-assign Public IP
    aws ec2 modify-subnet-attribute \
    --subnet-id subnet-0123456789abcdef0 \
    --map-public-ip-on-launch

    # Create Private Subnet (for App Servers)
    aws ec2 create-subnet \
    --vpc-id vpc-0123456789abcdef0 \
    --cidr-block 10.0.10.0/24 \
    --availability-zone us-east-1a \
    --tag-specifications 'ResourceType=subnet,Tags=[{Key=Name,Value=private-1a}]'

    # Internet Gateway (Public Internet Access)
    # Create IGW
    aws ec2 create-internet-gateway \
    --tag-specifications 'ResourceType=internet-gateway,Tags=[{Key=Name,Value=prod-igw}]'

    # Attach to VPC
    aws ec2 attach-internet-gateway \
    --internet-gateway-id igw-0123456789abcdef0 \
    --vpc-id vpc-0123456789abcdef0

    # Route Tables
    # Create Public Route Table
    aws ec2 create-route-table \
    --vpc-id vpc-0123456789abcdef0 \
    --tag-specifications 'ResourceType=route-table,Tags=[{Key=Name,Value=public-rt}]'

    # Add Route to Internet Gateway
    aws ec2 create-route \
    --route-table-id rtb-0123456789abcdef0 \
    --destination-cidr-block 0.0.0.0/0 \
    --gateway-id igw-0123456789abcdef0

    # Associate with Public Subnet
    aws ec2 associate-route-table \
    --route-table-id rtb-0123456789abcdef0 \
    --subnet-id subnet-0123456789abcdef0

    # NAT Gateway (Private Instances Internet Access)
    # Allocate Elastic IP
    aws ec2 allocate-address --domain vpc

    # Create NAT Gateway in Public Subnet
    aws ec2 create-nat-gateway \
    --subnet-id subnet-public-1a \
    --allocation-id eipalloc-0123456789abcdef0 \
    --tag-specifications 'ResourceType=natgateway,Tags=[{Key=Name,Value=prod-nat-1a}]'

    # Create Private Route Table
    aws ec2 create-route-table \
    --vpc-id vpc-0123456789abcdef0 \
    --tag-specifications 'ResourceType=route-table,Tags=[{Key=Name,Value=private-rt}]'

    # Add Route to NAT Gateway
    aws ec2 create-route \
    --route-table-id rtb-private \
    --destination-cidr-block 0.0.0.0/0 \
    --nat-gateway-id nat-0123456789abcdef0

    # Associate with Private Subnet
    aws ec2 associate-route-table \
    --route-table-id rtb-private \
    --subnet-id subnet-private-1a

    # Network ACLs (Subnet-level Firewall)
    # Create NACL
    aws ec2 create-network-acl \
    --vpc-id vpc-0123456789abcdef0 \
    --tag-specifications 'ResourceType=network-acl,Tags=[{Key=Name,Value=public-nacl}]'

    # Allow Inbound HTTP
    aws ec2 create-network-acl-entry \
    --network-acl-id acl-0123456789abcdef0 \
    --rule-number 100 \
    --protocol tcp \
    --port-range From=80,To=80 \
    --cidr-block 0.0.0.0/0 \
    --rule-action allow \
    --ingress

    # Allow Outbound (Return Traffic)
    aws ec2 create-network-acl-entry \
    --network-acl-id acl-0123456789abcdef0 \
    --rule-number 100 \
    --protocol tcp \
    --port-range From=1024,To=65535 \
    --cidr-block 0.0.0.0/0 \
    --rule-action allow \
    --egress

    # VPC Flow Logs (Network Traffic Monitoring)
    aws ec2 create-flow-logs \
    --resource-type VPC \
    --resource-ids vpc-0123456789abcdef0 \
    --traffic-type ALL \
    --log-destination-type cloud-watch-logs \
    --log-group-name /aws/vpc/flowlogs \
    --deliver-logs-permission-arn arn:aws:iam::123456789012:role/flowlogsRole

    # VPC Peering (Connect VPCs)
    aws ec2 create-vpc-peering-connection \
    --vpc-id vpc-prod \
    --peer-vpc-id vpc-staging \
    --peer-region us-east-1

    # Accept Peering
    aws ec2 accept-vpc-peering-connection \
    --vpc-peering-connection-id pcx-0123456789abcdef0

    # Interview Questions - VPC:
    # Q1: Difference between Security Group and NACL?
    # A: SG is stateful (return traffic auto-allowed), instance-level
    #    NACL is stateless (must allow both directions), subnet-level

    # Q2: Why use NAT Gateway?
    # A: Allow private subnet instances to access internet (updates, APIs)
    #    without exposing them to inbound internet traffic

    # Q3: What's the difference between Public and Private subnet?
    # A: Public has route to Internet Gateway (0.0.0.0/0 → IGW)
    #    Private has route to NAT Gateway (0.0.0.0/0 → NAT)</code></pre>
  </div>

  <div class="command-block">
    <h4>AWS IAM - Security & Access Control (SECURITY = JOB SECURITY)</h4>
    <pre><code>
    # IAM Best Practices (Follow This = Get Hired):
    # 1. Never use root account for daily tasks
    # 2. Enable MFA on root and admin accounts
    # 3. Use roles for EC2, Lambda (not access keys)
    # 4. Follow least privilege principle
    # 5. Rotate credentials regularly
    # 6. Use groups for permission management

    # Create IAM User
    aws iam create-user --user-name developer

    # Create Password
    aws iam create-login-profile \
    --user-name developer \
    --password 'MySecurePassword123!' \
    --password-reset-required

    # Create Access Keys (for CLI/API)
    aws iam create-access-key --user-name developer

    # Create IAM Group
    aws iam create-group --group-name developers

    # Add User to Group
    aws iam add-user-to-group \
    --user-name developer \
    --group-name developers

    # Attach Managed Policy to Group
    aws iam attach-group-policy \
    --group-name developers \
    --policy-arn arn:aws:iam::aws:policy/AmazonEC2ReadOnlyAccess

    # Common AWS Managed Policies:
    # - AdministratorAccess (Full access - dangerous)
    # - PowerUserAccess (No IAM/billing access)
    # - ReadOnlyAccess (Read-only everything)
    # - AmazonEC2FullAccess (EC2 full control)
    # - AmazonS3FullAccess (S3 full control)
    # - AmazonRDSFullAccess (RDS full control)

    # Create Custom Policy
    aws iam create-policy \
    --policy-name EC2-Start-Stop-Only \
    --policy-document file://ec2-start-stop-policy.json

    # ec2-start-stop-policy.json
    {
    "Version": "2012-10-17",
    "Statement": [
    {
    "Effect": "Allow",
    "Action": [
    "ec2:StartInstances",
    "ec2:StopInstances",
    "ec2:DescribeInstances"
    ],
    "Resource": "*",
    "Condition": {
    "StringEquals": {
    "ec2:ResourceTag/Environment": "development"
    }
    }
    }
    ]
    }

    # Attach Custom Policy
    aws iam attach-user-policy \
    --user-name developer \
    --policy-arn arn:aws:iam::123456789012:policy/EC2-Start-Stop-Only

    # IAM Roles (For EC2, Lambda, ECS)
    # Create Role for EC2
    aws iam create-role \
    --role-name EC2-S3-ReadOnly \
    --assume-role-policy-document file://trust-policy-ec2.json

    # trust-policy-ec2.json
    {
    "Version": "2012-10-17",
    "Statement": [
    {
    "Effect": "Allow",
    "Principal": {
    "Service": "ec2.amazonaws.com"
    },
    "Action": "sts:AssumeRole"
    }
    ]
    }

    # Attach S3 Read Policy to Role
    aws iam attach-role-policy \
    --role-name EC2-S3-ReadOnly \
    --policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess

    # Create Instance Profile
    aws iam create-instance-profile \
    --instance-profile-name EC2-S3-ReadOnly-Profile

    # Add Role to Instance Profile
    aws iam add-role-to-instance-profile \
    --instance-profile-name EC2-S3-ReadOnly-Profile \
    --role-name EC2-S3-ReadOnly

    # Attach to EC2 Instance
    aws ec2 associate-iam-instance-profile \
    --instance-id i-1234567890abcdef0 \
    --iam-instance-profile Name=EC2-S3-ReadOnly-Profile

    # Now EC2 can access S3 without access keys!
    # Inside EC2:
    aws s3 ls  # Works automatically using instance role

    # MFA (Multi-Factor Authentication)
    # Enable Virtual MFA Device
    aws iam enable-mfa-device \
    --user-name admin \
    --serial-number arn:aws:iam::123456789012:mfa/admin \
    --authentication-code1 123456 \
    --authentication-code2 789012

    # Password Policy (Organization Security)
    aws iam update-account-password-policy \
    --minimum-password-length 14 \
    --require-symbols \
    --require-numbers \
    --require-uppercase-characters \
    --require-lowercase-characters \
    --allow-users-to-change-password \
    --max-password-age 90 \
    --password-reuse-prevention 5

    # List Users and Their Policies
    aws iam list-users
    aws iam list-attached-user-policies --user-name developer
    aws iam list-groups-for-user --user-name developer

    # Generate Temporary Credentials (STS)
    aws sts assume-role \
    --role-arn arn:aws:iam::123456789012:role/EC2-S3-ReadOnly \
    --role-session-name mysession

    # Access Advisor (See what permissions are used)
    aws iam generate-service-last-accessed-details \
    --arn arn:aws:iam::123456789012:user/developer

    # Delete User (Cleanup)
    aws iam delete-access-key --user-name developer --access-key-id AKIAIOSFODNN7EXAMPLE
    aws iam remove-user-from-group --user-name developer --group-name developers
    aws iam delete-user --user-name developer

    # Interview Questions - IAM:
    # Q1: User vs Role - when to use what?
    # A: User - for people (permanent credentials)
    #    Role - for services/temporary access (no permanent credentials)

    # Q2: How to give EC2 access to S3?
    # A: Create IAM role with S3 policy, attach to EC2 instance profile
    #    NEVER put access keys in EC2!

    # Q3: What's the principle of least privilege?
    # A: Give minimum permissions needed to do the job, nothing more</code></pre>
  </div>

  <div class="command-block">
    <h4>AWS Storage - S3 & EBS (DATA = BUSINESS)</h4>
    <pre><code>
    # S3 (Simple Storage Service) - Object Storage

    # Create Bucket
    aws s3 mb s3://mycompany-backups-2025
    aws s3 mb s3://mycompany-static-website --region us-west-2

    # Upload Files
    aws s3 cp app.log s3://mycompany-backups-2025/
    aws s3 cp /var/www/html s3://mycompany-static-website/ --recursive

    # Download Files
    aws s3 cp s3://mycompany-backups-2025/app.log ./
    aws s3 sync s3://mycompany-static-website/ /var/www/html/

    # List Objects
    aws s3 ls s3://mycompany-backups-2025/
    aws s3 ls s3://mycompany-backups-2025/logs/ --recursive --human-readable

    # Delete Objects
    aws s3 rm s3://mycompany-backups-2025/old-file.log
    aws s3 rm s3://mycompany-backups-2025/logs/ --recursive

    # Sync (Like rsync for S3)
    aws s3 sync /local/backup s3://mycompany-backups-2025/daily/ --delete

    # S3 Storage Classes (Cost Optimization):
    # Standard - Frequent access, $0.023/GB
    # Intelligent-Tiering - Auto-optimize, $0.023/GB
    # Standard-IA - Infrequent access, $0.0125/GB
    # One Zone-IA - Single AZ, $0.01/GB
    # Glacier Instant - Archive with instant retrieval, $0.004/GB
    # Glacier Flexible - Archive, minutes-hours retrieval, $0.0036/GB
    # Glacier Deep Archive - Long-term archive, 12 hours retrieval, $0.00099/GB

    # Set Storage Class
    aws s3 cp file.txt s3://bucket/ --storage-class STANDARD_IA

    # Bucket Versioning (Protect Against Deletion)
    aws s3api put-bucket-versioning \
    --bucket mycompany-backups-2025 \
    --versioning-configuration Status=Enabled

    # List Object Versions
    aws s3api list-object-versions --bucket mycompany-backups-2025

    # Lifecycle Policy (Auto-transition/delete)
    aws s3api put-bucket-lifecycle-configuration \
    --bucket mycompany-backups-2025 \
    --lifecycle-configuration file://lifecycle.json

    # lifecycle.json
    {
    "Version": "2012-10-17",
    "Statement": [
    {
    "Id": "Move old backups to Glacier",
    "Status": "Enabled",
    "Prefix": "backups/",
    "Transitions": [
    {
    "Days": 30,
    "StorageClass": "GLACIER"
    }
    ],
    "Expiration": {
    "Days": 365
    }
    }
    ]
    }

    # Bucket Policy (Public Website)
    aws s3api put-bucket-policy \
    --bucket mycompany-static-website \
    --policy file://public-website-policy.json

    # public-website-policy.json
    {
    "Version": "2012-10-17",
    "Statement": [
    {
    "Sid": "PublicReadGetObject",
    "Effect": "Allow",
    "Principal": "*",
    "Action": "s3:GetObject",
    "Resource": "arn:aws:s3:::mycompany-static-website/*"
    }
    ]
    }

    # Static Website Hosting
    aws s3 website s3://mycompany-static-website/ \
    --index-document index.html \
    --error-document error.html

    # Access website at: http://mycompany-static-website.s3-website-us-east-1.amazonaws.com

    # S3 Encryption
    # Server-Side Encryption (SSE-S3)
    aws s3 cp file.txt s3://bucket/ --sse AES256

    # SSE-KMS (with AWS Key Management Service)
    aws s3 cp file.txt s3://bucket/ --sse aws:kms --sse-kms-key-id alias/my-key

    # EBS (Elastic Block Store) - Block Storage for EC2

    # Create EBS Volume
    aws ec2 create-volume \
    --size 100 \
    --volume-type gp3 \
    --availability-zone us-east-1a \
    --tag-specifications 'ResourceType=volume,Tags=[{Key=Name,Value=data-volume}]'

    # EBS Volume Types:
    # gp3 - General Purpose SSD (default, best price/performance)
    # gp2 - General Purpose SSD (older generation)
    # io2 - Provisioned IOPS SSD (high performance databases)
    # st1 - Throughput Optimized HDD (big data, logs)
    # sc1 - Cold HDD (infrequent access, lowest cost)

    # Attach Volume to EC2
    aws ec2 attach-volume \
    --volume-id vol-0123456789abcdef0 \
    --instance-id i-1234567890abcdef0 \
    --device /dev/sdf

    # Inside EC2, format and mount:
    # lsblk
    # mkfs -t ext4 /dev/xvdf
    # mkdir /data
    # mount /dev/xvdf /data
    # echo "/dev/xvdf /data ext4 defaults,nofail 0 2" >> /etc/fstab

    # Detach Volume
    aws ec2 detach-volume --volume-id vol-0123456789abcdef0

    # Create Snapshot (Backup)
    aws ec2 create-snapshot \
    --volume-id vol-0123456789abcdef0 \
    --description "Daily backup $(date +%Y%m%d)"

    # List Snapshots
    aws ec2 describe-snapshots --owner-ids self

    # Create Volume from Snapshot
    aws ec2 create-volume \
    --snapshot-id snap-0123456789abcdef0 \
    --availability-zone us-east-1a

    # Copy Snapshot to Another Region (Disaster Recovery)
    aws ec2 copy-snapshot \
    --source-region us-east-1 \
    --source-snapshot-id snap-0123456789abcdef0 \
    --destination-region us-west-2 \
    --description "DR backup"

    # Delete Snapshot
    aws ec2 delete-snapshot --snapshot-id snap-0123456789abcdef0

    # Modify Volume (Increase Size/Change Type)
    aws ec2 modify-volume \
    --volume-id vol-0123456789abcdef0 \
    --size 200 \
    --volume-type gp3

    # Inside EC2, extend filesystem:
    # lsblk
    # growpart /dev/xvdf 1
    # resize2fs /dev/xvdf1

    # Interview Questions - Storage:
    # Q1: S3 vs EBS - when to use what?
    # A: S3 - Object storage, accessed via HTTP, unlimited size, cheaper
    #    EBS - Block storage, attached to EC2, limited size, faster

    # Q2: How to make S3 bucket public?
    # A: Bucket policy with Principal: "*" and Action: "s3:GetObject"
    #    Also disable "Block Public Access" settings

    # Q3: How to backup EC2 data?
    # A: Create EBS snapshots (automated via lifecycle policies)
    #    Or sync to S3 using aws s3 sync</code></pre>
  </div>

  <div class="command-block">
    <h4>AWS Real-World Production Patterns (THIS GETS YOU HIRED)</h4>
    <pre><code>
    # Scenario 1: High-Availability Web Application
    #
    # Architecture:
    # - 2 Availability Zones
    # - Auto Scaling Group (2-10 instances)
    # - Application Load Balancer
    # - RDS MySQL (Multi-AZ)
    # - S3 for static assets
    # - CloudFront CDN

    # 1. Create Launch Template
    aws ec2 create-launch-template \
    --launch-template-name web-server-template \
    --version-description "v1.0" \
    --launch-template-data '{
    "ImageId": "ami-0c55b159cbfafe1f0",
    "InstanceType": "t3.medium",
    "KeyName": "prod-key",
    "SecurityGroupIds": ["sg-web-servers"],
    "UserData": "'"$(base64 userdata.sh)"'",
    "IamInstanceProfile": {"Name": "EC2-S3-ReadOnly"},
    "TagSpecifications": [{
    "ResourceType": "instance",
    "Tags": [
    {"Key": "Name", "Value": "web-server"},
    {"Key": "Environment", "Value": "production"}
    ]
    }]
    }'

    # 2. Create Auto Scaling Group
    aws autoscaling create-auto-scaling-group \
    --auto-scaling-group-name web-asg \
    --launch-template LaunchTemplateName=web-server-template,Version='$Latest' \
    --min-size 2 \
    --max-size 10 \
    --desired-capacity 2 \
    --vpc-zone-identifier "subnet-public-1a,subnet-public-1b" \
    --target-group-arns arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/web-tg/abc123 \
    --health-check-type ELB \
    --health-check-grace-period 300

    # 3. Create Scaling Policies
    # Scale Up when CPU > 70%
    aws autoscaling put-scaling-policy \
    --auto-scaling-group-name web-asg \
    --policy-name scale-up \
    --scaling-adjustment 2 \
    --adjustment-type ChangeInCapacity

    # Scale Down when CPU < 30%
    aws autoscaling put-scaling-policy \
    --auto-scaling-group-name web-asg \
    --policy-name scale-down \
    --scaling-adjustment -1 \
    --adjustment-type ChangeInCapacity

    # 4. Create Application Load Balancer
    aws elbv2 create-load-balancer \
    --name web-alb \
    --subnets subnet-public-1a subnet-public-1b \
    --security-groups sg-0123456789abcdef0 \
    --scheme internet-facing \
    --type application \
    --tags Key=Name,Value=production-alb

    # 5. Create Target Group
    aws elbv2 create-target-group \
    --name web-tg \
    --protocol HTTP \
    --port 80 \
    --vpc-id vpc-0123456789abcdef0 \
    --health-check-protocol HTTP \
    --health-check-path /health \
    --health-check-interval-seconds 30 \
    --health-check-timeout-seconds 5 \
    --healthy-threshold-count 2 \
    --unhealthy-threshold-count 2 \
    --target-type instance

    # 6. Create Listener
    aws elbv2 create-listener \
    --load-balancer-arn arn:aws:elasticloadbalancing:us-east-1:123456789012:loadbalancer/app/web-alb/abc123 \
    --protocol HTTP \
    --port 80 \
    --default-actions Type=forward,TargetGroupArn=arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/web-tg/abc123

    # 7. Register Auto Scaling Group with Target Group
    aws autoscaling attach-load-balancer-target-groups \
    --auto-scaling-group-name web-asg \
    --target-group-arns arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/web-tg/abc123

    # 8. Create RDS Database (Multi-AZ)
    aws rds create-db-instance \
    --db-instance-identifier prod-mysql \
    --db-instance-class db.t3.medium \
    --engine mysql \
    --engine-version 8.0 \
    --master-username admin \
    --master-user-password 'MyS3cureP@ssw0rd' \
    --allocated-storage 100 \
    --storage-type gp3 \
    --multi-az \
    --backup-retention-period 7 \
    --preferred-backup-window 02:00-03:00 \
    --preferred-maintenance-window sun:03:00-sun:04:00 \
    --db-subnet-group-name prod-db-subnet-group \
    --vpc-security-group-ids sg-database \
    --tags Key=Environment,Value=production

    # 9. Create CloudFront Distribution (CDN)
    aws cloudfront create-distribution \
    --origin-domain-name mycompany-static-website.s3.amazonaws.com \
    --default-root-object index.html \
    --enabled \
    --comment "Production website CDN"

    # Scenario 2: Serverless API with Lambda
    # Architecture: API Gateway + Lambda + DynamoDB

    # 1. Create DynamoDB Table
    aws dynamodb create-table \
    --table-name users \
    --attribute-definitions AttributeName=user_id,AttributeType=S \
    --key-schema AttributeName=user_id,KeyType=HASH \
    --billing-mode PAY_PER_REQUEST \
    --tags Key=Environment,Value=production

    # 2. Create IAM Role for Lambda
    aws iam create-role \
    --role-name lambda-dynamodb-role \
    --assume-role-policy-document '{
    "Version": "2012-10-17",
    "Statement": [{
    "Effect": "Allow",
    "Principal": {"Service": "lambda.amazonaws.com"},
    "Action": "sts:AssumeRole"
    }]
    }'

    aws iam attach-role-policy \
    --role-name lambda-dynamodb-role \
    --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole

    aws iam attach-role-policy \
    --role-name lambda-dynamodb-role \
    --policy-arn arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess

    # 3. Create Lambda Function
    aws lambda create-function \
    --function-name user-api \
    --runtime python3.9 \
    --role arn:aws:iam::123456789012:role/lambda-dynamodb-role \
    --handler lambda_function.lambda_handler \
    --zip-file fileb://function.zip \
    --timeout 30 \
    --memory-size 256 \
    --environment Variables={TABLE_NAME=users}

    # 4. Create API Gateway
    aws apigateway create-rest-api \
    --name user-api-gateway \
    --description "User management API" \
    --endpoint-configuration types=REGIONAL

    # 5. Create Resource and Method
    aws apigateway create-resource \
    --rest-api-id abc123def \
    --parent-id root \
    --path-part users

    aws apigateway put-method \
    --rest-api-id abc123def \
    --resource-id def456 \
    --http-method GET \
    --authorization-type NONE

    aws apigateway put-integration \
    --rest-api-id abc123def \
    --resource-id def456 \
    --http-method GET \
    --type AWS_PROXY \
    --integration-http-method POST \
    --uri arn:aws:apigateway:us-east-1:lambda:path/2015-03-31/functions/arn:aws:lambda:us-east-1:123456789012:function:user-api/invocations

    # 6. Deploy API
    aws apigateway create-deployment \
    --rest-api-id abc123def \
    --stage-name prod

    # Scenario 3: Containerized Application with ECS
    # Architecture: ECS Fargate + Application Load Balancer

    # 1. Create ECS Cluster
    aws ecs create-cluster \
    --cluster-name production-cluster \
    --tags key=Environment,value=production

    # 2. Create Task Definition
    aws ecs register-task-definition \
    --family web-app \
    --network-mode awsvpc \
    --cpu 512 \
    --memory 1024 \
    --requires-compatibilities FARGATE \
    --execution-role-arn arn:aws:iam::123456789012:role/ecsTaskExecutionRole \
    --container-definitions '[{
    "name": "web-app",
    "image": "123456789012.dkr.ecr.us-east-1.amazonaws.com/web-app:latest",
    "portMappings": [{"containerPort": 80, "hostPort": 80}],
    "logConfiguration": {
    "logDriver": "awslogs",
    "options": {
    "awslogs-group": "/ecs/web-app",
    "awslogs-region": "us-east-1",
    "awslogs-stream-prefix": "ecs"
    }
    }
    }]'

    # 3. Create Service
    aws ecs create-service \
    --cluster production-cluster \
    --service-name web-service \
    --task-definition web-app:1 \
    --desired-count 2 \
    --launch-type FARGATE \
    --network-configuration "awsvpcConfiguration={subnets=[subnet-private-1a,subnet-private-1b],securityGroups=[sg-ecs],assignPublicIp=DISABLED}" \
    --load-balancers "targetGroupArn=arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/ecs-tg/abc123,containerName=web-app,containerPort=80"

    # Scenario 4: Disaster Recovery Setup
    # Architecture: Cross-region replication for critical services

    # 1. S3 Cross-Region Replication
    aws s3api put-bucket-replication \
    --bucket mycompany-backups-2025 \
    --replication-configuration '{
    "Role": "arn:aws:iam::123456789012:role/s3-replication-role",
    "Rules": [{
    "Status": "Enabled",
    "Priority": 1,
    "DeleteMarkerReplication": { "Status": "Disabled" },
    "Filter": { "Prefix": "" },
    "Destination": {
    "Bucket": "arn:aws:s3:::mycompany-backups-dr",
    "StorageClass": "STANDARD"
    }
    }]
    }'

    # 2. RDS Read Replica in another region
    aws rds create-db-instance-read-replica \
    --db-instance-identifier prod-mysql-dr \
    --source-db-instance-identifier prod-mysql \
    --region us-west-2 \
    --db-instance-class db.t3.medium

    # 3. Route 53 DNS Failover
    aws route53 change-resource-record-sets \
    --hosted-zone-id Z123456789ABCD \
    --change-batch '{
    "Comment": "Failover configuration",
    "Changes": [{
    "Action": "CREATE",
    "ResourceRecordSet": {
    "Name": "api.mycompany.com",
    "Type": "A",
    "SetIdentifier": "primary",
    "Failover": "PRIMARY",
    "AliasTarget": {
    "HostedZoneId": "Z2FDTNDATAQYW2",
    "DNSName": "web-alb-1234567890.us-east-1.elb.amazonaws.com",
    "EvaluateTargetHealth": true
    }
    }
    }, {
    "Action": "CREATE",
    "ResourceRecordSet": {
    "Name": "api.mycompany.com",
    "Type": "A",
    "SetIdentifier": "secondary",
    "Failover": "SECONDARY",
    "AliasTarget": {
    "HostedZoneId": "Z2FDTNDATAQYW2",
    "DNSName": "web-alb-dr-1234567890.us-west-2.elb.amazonaws.com",
    "EvaluateTargetHealth": true
    }
    }
    }]
    }'

    # Scenario 5: Monitoring and Alerting
    # Architecture: CloudWatch + SNS + Lambda

    # 1. Create CloudWatch Alarm for High CPU
    aws cloudwatch put-metric-alarm \
    --alarm-name high-cpu-alarm \
    --alarm-description "CPU utilization is high" \
    --metric-name CPUUtilization \
    --namespace AWS/EC2 \
    --statistic Average \
    --period 300 \
    --threshold 80 \
    --comparison-operator GreaterThanThreshold \
    --dimensions Name=AutoScalingGroupName,Value=web-asg \
    --evaluation-periods 2 \
    --alarm-actions arn:aws:sns:us-east-1:123456789012:production-alerts

    # 2. Create SNS Topic for Alerts
    aws sns create-topic \
    --name production-alerts \
    --tags Key=Environment,Value=production

    aws sns subscribe \
    --topic-arn arn:aws:sns:us-east-1:123456789012:production-alerts \
    --protocol email \
    --notification-endpoint admin@mycompany.com

    # 3. Create CloudWatch Logs Metric Filter
    aws logs put-metric-filter \
    --log-group-name /var/log/nginx/access.log \
    --filter-name "5xx-errors" \
    --filter-pattern '[ip, identity, user, timestamp, request, status_code=5*, size, referrer, user_agent]' \
    --metric-transformations '[
    {
    "metricName": "5xxErrorCount",
    "metricNamespace": "WebServer",
    "metricValue": "1",
    "defaultValue": 0
    }
    ]'

    # 4. Create Dashboard
    aws cloudwatch put-dashboard \
    --dashboard-name production-dashboard \
    --dashboard-body '{
    "widgets": [
    {
    "type": "metric",
    "x": 0,
    "y": 0,
    "width": 12,
    "height": 6,
    "properties": {
    "metrics": [
    ["AWS/EC2", "CPUUtilization", "AutoScalingGroupName", "web-asg"],
    ["AWS/ApplicationELB", "TargetResponseTime", "LoadBalancer", "app/web-alb/abc123"]
    ],
    "period": 300,
    "stat": "Average",
    "region": "us-east-1",
    "title": "Performance Metrics"
    }
    }
    ]
    }'

    # Interview Questions - Production Patterns:
    # Q1: What's the difference between Multi-AZ and Read Replica?
    # A: Multi-AZ - Synchronous replication for high availability (failover)
    #    Read Replica - Asynchronous replication for read scaling

    # Q2: How to handle database migrations in production?
    # A: Use Blue/Green deployment or Database migration service
    #    Always backup before migration, test in staging first

    # Q3: What monitoring metrics are critical for production?
    # A: CPU/Memory utilization, Disk I/O, Network throughput
    #    Application metrics: Error rates, Latency, Request count
    #    Business metrics: User count, Transaction volume

    # Q4: How to secure production environments?
    # A: Use private subnets for app servers, Security groups with least privilege
    #    Enable CloudTrail for audit, Use KMS for encryption
    #    Regular security patches, WAF for web applications

    # Q5: How to optimize AWS costs in production?
    # A: Use reserved instances for steady workloads
    #    Right-size instances (don't over-provision)
    #    Use Auto Scaling to match demand
    #    Implement S3 lifecycle policies
    #    Monitor with Cost Explorer and set budgets</code></pre>
  </div>

  <div class="command-block">
    <h4>AWS Database - RDS & DynamoDB (DATA MANAGEMENT SKILLS)</h4>
    <pre><code>
    # RDS (Relational Database Service)

    # Create MySQL Database
    aws rds create-db-instance \
    --db-instance-identifier prod-db \
    --db-instance-class db.t3.medium \
    --engine mysql \
    --engine-version 8.0 \
    --master-username admin \
    --master-user-password 'Str0ngP@ssw0rd!' \
    --allocated-storage 100 \
    --storage-type gp3 \
    --backup-retention-period 7 \
    --preferred-backup-window 02:00-03:00 \
    --multi-az \
    --publicly-accessible false \
    --vpc-security-group-ids sg-database \
    --db-subnet-group-name prod-db-subnet-group \
    --tags Key=Environment,Value=production

    # Create DB Subnet Group
    aws rds create-db-subnet-group \
    --db-subnet-group-name prod-db-subnet-group \
    --db-subnet-group-description "Production database subnet group" \
    --subnet-ids subnet-private-1a subnet-private-1b

    # Create Read Replica
    aws rds create-db-instance-read-replica \
    --db-instance-identifier prod-db-replica \
    --source-db-instance-identifier prod-db

    # Take Manual Snapshot
    aws rds create-db-snapshot \
    --db-snapshot-identifier prod-db-snapshot-$(date +%Y%m%d) \
    --db-instance-identifier prod-db

    # Restore from Snapshot
    aws rds restore-db-instance-from-db-snapshot \
    --db-instance-identifier restored-db \
    --db-snapshot-identifier prod-db-snapshot-20241225

    # Modify Database (Scale up)
    aws rds modify-db-instance \
    --db-instance-identifier prod-db \
    --db-instance-class db.t3.large \
    --apply-immediately

    # Create Parameter Group
    aws rds create-db-parameter-group \
    --db-parameter-group-name custom-mysql-params \
    --db-parameter-group-family mysql8.0 \
    --description "Custom MySQL parameters"

    # Update Parameter
    aws rds modify-db-parameter-group \
    --db-parameter-group-name custom-mysql-params \
    --parameters "ParameterName=max_connections,ParameterValue=100,ApplyMethod=immediate"

    # DynamoDB (NoSQL Database)

    # Create Table
    aws dynamodb create-table \
    --table-name users \
    --attribute-definitions \
    AttributeName=user_id,AttributeType=S \
    AttributeName=email,AttributeType=S \
    --key-schema \
    AttributeName=user_id,KeyType=HASH \
    --global-secondary-indexes '[{
    "IndexName": "email-index",
    "KeySchema": [
    {"AttributeName": "email", "KeyType": "HASH"}
    ],
    "Projection": {
    "ProjectionType": "ALL"
    },
    "ProvisionedThroughput": {
    "ReadCapacityUnits": 5,
    "WriteCapacityUnits": 5
    }
    }]' \
    --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5

    # Put Item
    aws dynamodb put-item \
    --table-name users \
    --item '{
    "user_id": {"S": "123"},
    "email": {"S": "john@example.com"},
    "name": {"S": "John Doe"},
    "age": {"N": "30"},
    "active": {"BOOL": true}
    }'

    # Query Item
    aws dynamodb query \
    --table-name users \
    --key-condition-expression "user_id = :v1" \
    --expression-attribute-values '{":v1": {"S": "123"}}'

    # Scan Table
    aws dynamodb scan \
    --table-name users \
    --filter-expression "active = :active" \
    --expression-attribute-values '{":active": {"BOOL": true}}'

    # Update Item
    aws dynamodb update-item \
    --table-name users \
    --key '{"user_id": {"S": "123"}}' \
    --update-expression "SET #n = :name" \
    --expression-attribute-names '{"#n": "name"}' \
    --expression-attribute-values '{":name": {"S": "John Updated"}}'

    # Delete Item
    aws dynamodb delete-item \
    --table-name users \
    --key '{"user_id": {"S": "123"}}'

    # Update Table (Add GSI)
    aws dynamodb update-table \
    --table-name users \
    --attribute-definitions AttributeName=created_at,AttributeType=N \
    --global-secondary-index-updates '[{
    "Create": {
    "IndexName": "created_at-index",
    "KeySchema": [
    {"AttributeName": "created_at", "KeyType": "HASH"}
    ],
    "Projection": {
    "ProjectionType": "ALL"
    },
    "ProvisionedThroughput": {
    "ReadCapacityUnits": 5,
    "WriteCapacityUnits": 5
    }
    }
    }]'

    # Enable Auto Scaling
    aws application-autoscaling register-scalable-target \
    --service-namespace dynamodb \
    --resource-id "table/users" \
    --scalable-dimension "dynamodb:table:ReadCapacityUnits" \
    --min-capacity 5 \
    --max-capacity 100

    aws application-autoscaling put-scaling-policy \
    --service-namespace dynamodb \
    --resource-id "table/users" \
    --scalable-dimension "dynamodb:table:ReadCapacityUnits" \
    --policy-name ReadScalingPolicy \
    --policy-type TargetTrackingScaling \
    --target-tracking-scaling-policy-configuration '{
    "TargetValue": 70.0,
    "PredefinedMetricSpecification": {
    "PredefinedMetricType": "DynamoDBReadCapacityUtilization"
    },
    "ScaleOutCooldown": 60,
    "ScaleInCooldown": 60
    }'

    # Backup Table
    aws dynamodb create-backup \
    --table-name users \
    --backup-name users-backup-$(date +%Y%m%d)

    # Restore from Backup
    aws dynamodb restore-table-from-backup \
    --target-table-name users-restored \
    --backup-arn arn:aws:dynamodb:us-east-1:123456789012:table/users/backup/016235...

    # Interview Questions - Databases:
    # Q1: When to use RDS vs DynamoDB?
    # A: RDS - When you need ACID compliance, complex queries, joins
    #    DynamoDB - When you need low latency, massive scale, serverless

    # Q2: How to handle database backups?
    # A: RDS - Automated backups + manual snapshots
    #    DynamoDB - Point-in-time recovery + on-demand backups

    # Q3: What is Multi-AZ in RDS?
    # A: Synchronous replication to standby in another AZ
    #    Automatic failover during primary failure

    # Q4: How to optimize DynamoDB costs?
    # A: Use on-demand capacity for unpredictable workloads
    #    Use auto-scaling for predictable patterns
    #    Use TTL to automatically delete old items</code></pre>
  </div>

  <div class="command-block">
    <h4>AWS Automation & DevOps (REMOTE JOB READY)</h4>
    <pre><code>
    # CloudFormation (Infrastructure as Code)

    # Create Stack from Template
    aws cloudformation create-stack \
    --stack-name production-web-stack \
    --template-body file://web-infrastructure.yaml \
    --parameters \
    ParameterKey=Environment,ParameterValue=production \
    ParameterKey=InstanceType,ParameterValue=t3.medium \
    ParameterKey=DBInstanceClass,ParameterValue=db.t3.medium \
    --capabilities CAPABILITY_IAM \
    --tags Key=Environment,Value=production

    # Update Stack
    aws cloudformation update-stack \
    --stack-name production-web-stack \
    --template-body file://web-infrastructure.yaml \
    --parameters \
    ParameterKey=Environment,ParameterValue=production \
    ParameterKey=InstanceType,ParameterValue=t3.large \
    ParameterKey=DBInstanceClass,ParameterValue=db.t3.large

    # Describe Stack
    aws cloudformation describe-stacks \
    --stack-name production-web-stack

    # List Stack Resources
    aws cloudformation describe-stack-resources \
    --stack-name production-web-stack

    # Delete Stack
    aws cloudformation delete-stack \
    --stack-name production-web-stack

    # CloudFormation Template Example (web-infrastructure.yaml)
    AWSTemplateFormatVersion: '2010-09-09'
    Description: Production Web Application Infrastructure

    Parameters:
    Environment:
    Type: String
    Default: production
    AllowedValues: [production, staging, development]

    InstanceType:
    Type: String
    Default: t3.medium
    AllowedValues: [t3.micro, t3.small, t3.medium, t3.large]

    DBInstanceClass:
    Type: String
    Default: db.t3.medium
    AllowedValues: [db.t3.micro, db.t3.small, db.t3.medium, db.t3.large]

    Resources:
    VPC:
    Type: AWS::EC2::VPC
    Properties:
    CidrBlock: 10.0.0.0/16
    Tags:
    - Key: Name
    Value: !Sub ${Environment}-vpc
    - Key: Environment
    Value: !Ref Environment

    PublicSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
    VpcId: !Ref VPC
    CidrBlock: 10.0.1.0/24
    AvailabilityZone: !Select [0, !GetAZs '']
    MapPublicIpOnLaunch: true
    Tags:
    - Key: Name
    Value: !Sub ${Environment}-public-1a

    InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
    Tags:
    - Key: Name
    Value: !Sub ${Environment}-igw

    VPCGatewayAttachment:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
    VpcId: !Ref VPC
    InternetGatewayId: !Ref InternetGateway

    WebServerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
    GroupDescription: Security group for web servers
    VpcId: !Ref VPC
    SecurityGroupIngress:
    - IpProtocol: tcp
    FromPort: 80
    ToPort: 80
    CidrIp: 0.0.0.0/0
    - IpProtocol: tcp
    FromPort: 443
    ToPort: 443
    CidrIp: 0.0.0.0/0
    - IpProtocol: tcp
    FromPort: 22
    ToPort: 22
    CidrIp: 203.0.113.0/24

    WebServerInstance:
    Type: AWS::EC2::Instance
    Properties:
    InstanceType: !Ref InstanceType
    ImageId: ami-0c55b159cbfafe1f0
    SubnetId: !Ref PublicSubnet1
    SecurityGroupIds:
    - !Ref WebServerSecurityGroup
    Tags:
    - Key: Name
    Value: !Sub ${Environment}-web-server
    - Key: Environment
    Value: !Ref Environment
    UserData:
    Fn::Base64: !Sub |
    #!/bin/bash
    yum update -y
    yum install -y nginx
    systemctl start nginx
    systemctl enable nginx

    Outputs:
    WebServerPublicIP:
    Description: Public IP of web server
    Value: !GetAtt WebServerInstance.PublicIp

    WebServerURL:
    Description: URL of web server
    Value: !Sub http://${WebServerInstance.PublicIp}

    # AWS CLI Automation Scripts

    # Automated Backup Script
    #!/bin/bash
    # backup-script.sh

    DATE=$(date +%Y%m%d)
    REGION="us-east-1"

    # Backup RDS
    aws rds create-db-snapshot \
    --db-snapshot-identifier prod-db-backup-${DATE} \
    --db-instance-identifier prod-db \
    --region ${REGION}

    # Backup EBS Volumes
    VOLUMES=$(aws ec2 describe-volumes \
    --filters "Name=tag:Backup,Values=true" \
    --query "Volumes[*].VolumeId" \
    --output text \
    --region ${REGION})

    for VOLUME in $VOLUMES; do
    aws ec2 create-snapshot \
    --volume-id ${VOLUME} \
    --description "Daily backup ${DATE}" \
    --tag-specifications 'ResourceType=snapshot,Tags=[{Key=Name,Value=backup},{Key=Date,Value='${DATE}'}]' \
    --region ${REGION}
    done

    # Sync to S3 Backup Bucket
    aws s3 sync /data/backups s3://mycompany-backups/daily/${DATE}/ \
    --delete \
    --region ${REGION}

    # Cleanup old backups (older than 30 days)
    aws ec2 describe-snapshots \
    --owner-ids self \
    --filters "Name=tag:Date,Values=*" \
    --query "Snapshots[?StartTime<='$(date --date='30 days ago' +%Y-%m-%d)'].SnapshotId" \
    --output text \
    --region ${REGION} | xargs -r aws ec2 delete-snapshot --snapshot-ids

    # Instance Health Check Script
    #!/bin/bash
    # health-check.sh

    INSTANCES=$(aws ec2 describe-instances \
    --filters "Name=tag:Environment,Values=production" \
    --query "Reservations[*].Instances[*].InstanceId" \
    --output text)

    for INSTANCE in $INSTANCES; do
    STATUS=$(aws ec2 describe-instance-status \
    --instance-ids ${INSTANCE} \
    --query "InstanceStatuses[0].InstanceStatus.Status" \
    --output text)

    if [ "$STATUS" != "ok" ]; then
    # Send alert
    aws sns publish \
    --topic-arn arn:aws:sns:us-east-1:123456789012:alerts \
    --subject "Instance ${INSTANCE} unhealthy" \
    --message "Instance ${INSTANCE} status: ${STATUS}"

    # Attempt restart
    aws ec2 reboot-instances --instance-ids ${INSTANCE}
    fi
    done

    # Cost Optimization Script
    #!/bin/bash
    # cost-optimizer.sh

    # Stop development instances at night
    DEV_INSTANCES=$(aws ec2 describe-instances \
    --filters "Name=tag:Environment,Values=development" "Name=instance-state-name,Values=running" \
    --query "Reservations[*].Instances[*].InstanceId" \
    --output text)

    # Stop at 8 PM
    if [ $(date +%H) -eq 20 ]; then
    if [ -n "$DEV_INSTANCES" ]; then
    aws ec2 stop-instances --instance-ids $DEV_INSTANCES
    echo "Stopped development instances: $DEV_INSTANCES"
    fi
    fi

    # Start at 8 AM
    if [ $(date +%H) -eq 8 ]; then
    if [ -n "$DEV_INSTANCES" ]; then
    aws ec2 start-instances --instance-ids $DEV_INSTANCES
    echo "Started development instances: $DEV_INSTANCES"
    fi
    fi

    # Cleanup unused EBS volumes
    UNATTACHED_VOLUMES=$(aws ec2 describe-volumes \
    --filters "Name=status,Values=available" \
    --query "Volumes[?CreateTime<='$(date --date='7 days ago' +%Y-%m-%d)'].VolumeId" \
    --output text)

    for VOLUME in $UNATTACHED_VOLUMES; do
    # Check if volume has important tags
    TAGS=$(aws ec2 describe-volumes \
    --volume-ids ${VOLUME} \
    --query "Volumes[0].Tags[?Key=='Protected'].Value" \
    --output text)

    if [ -z "$TAGS" ]; then
    aws ec2 delete-volume --volume-id ${VOLUME}
    echo "Deleted unused volume: ${VOLUME}"
    fi
    done

    # AWS Systems Manager Automation

    # Run Command on multiple instances
    aws ssm send-command \
    --document-name "AWS-RunShellScript" \
    --document-version "1" \
    --targets "Key=tag:Environment,Values=production" \
    --parameters 'commands=["yum update -y", "systemctl restart nginx"]' \
    --comment "Security patches and service restart" \
    --output-s3-bucket-name "mycompany-ssm-logs" \
    --output-s3-key-prefix "commands/"

    # Create Maintenance Window
    aws ssm create-maintenance-window \
    --name "Production-Maintenance" \
    --schedule "cron(0 2 ? * SUN *)" \
    --duration 4 \
    --cutoff 1 \
    --allow-unassociated-targets \
    --tags Key=Environment,Value=production

    # Register Targets
    aws ssm register-target-with-maintenance-window \
    --window-id mw-0123456789abcdef0 \
    --resource-type "INSTANCE" \
    --targets "Key=tag:Environment,Values=production"

    # Register Task
    aws ssm register-task-with-maintenance-window \
    --window-id mw-0123456789abcdef0 \
    --targets "Key=WindowTargetIds,Values=target-id-123" \
    --task-arn "AWS-RunPatchBaseline" \
    --service-role-arn "arn:aws:iam::123456789012:role/SSM-Maintenance-Role" \
    --task-type "RUN_COMMAND" \
    --max-concurrency "10" \
    --max-errors "2"

    # AWS CodeDeploy for Automated Deployments

    # Create Application
    aws deploy create-application \
    --application-name web-application \
    --compute-platform Server

    # Create Deployment Group
    aws deploy create-deployment-group \
    --application-name web-application \
    --deployment-group-name production-dg \
    --service-role-arn arn:aws:iam::123456789012:role/CodeDeployRole \
    --deployment-config-name CodeDeployDefault.OneAtATime \
    --auto-scaling-groups name=web-asg \
    --deployment-style deploymentType=IN_PLACE,deploymentOption=WITHOUT_TRAFFIC_CONTROL \
    --load-balancer-info '{"targetGroupInfoList": [{"name": "web-tg"}]}'

    # Create Deployment
    aws deploy create-deployment \
    --application-name web-application \
    --deployment-group-name production-dg \
    --deployment-config-name CodeDeployDefault.OneAtATime \
    --description "Deploy version 1.2.0" \
    --github-location repository=mycompany/web-app,commitId=abc123

    # Monitor Deployment
    aws deploy get-deployment \
    --deployment-id d-1234567890

    # Interview Questions - Automation:
    # Q1: Benefits of Infrastructure as Code?
    # A: Version control, reproducibility, consistency
    #    Automated deployments, reduced human error

    # Q2: How to handle secrets in automation?
    # A: Use AWS Secrets Manager or Parameter Store
    #    Never store in code, use IAM roles when possible

    # Q3: What is Blue/Green deployment?
    # A: Deploy new version alongside old version
    #    Switch traffic after testing, rollback if issues

    # Q4: How to monitor automated processes?
    # A: CloudWatch alarms for failures
    #    SNS notifications, Lambda for remediation
    #    Log everything to CloudWatch Logs</code></pre>
  </div>
</div>

<h2 id="container-security">35. CONTAINER SECURITY (Image scanning, Pod security policies, K8s RBAC)</h2>

<div>
  <h3>Image Scanning</h3>
  <div class="command-block">
    <h4>Scan Docker Images</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ocker scan image_name                                # Scan Docker image for vulnerabilities
    trivy image_name                                      # Scan container image using Trivy</code></pre>
  </div>

  <h3>Pod Security Policies & RBAC</h3>
  <div class="command-block">
    <h4>Create Role and RoleBinding in Kubernetes</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ubectl create role pod-reader --verb=get,list,watch --resource=pods
    kubectl create rolebinding pod-reader-binding --role=pod-reader --user=dev_user
    kubectl get roles
    kubectl get rolebindings</code></pre>
  </div>
</div>

<h2 id="cicd-tools">36. CI/CD TOOLS (Jenkins, GitHub Actions, GitLab CI pipelines)</h2>

<div>
  <h3>Jenkins CLI</h3>
  <div class="command-block">
    <h4>Basic Jenkins Commands</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    java -jar jenkins-cli.jar -s http://jenkins_url build job_name       # Trigger build
    java -jar jenkins-cli.jar -s http://jenkins_url list-jobs            # List jobs
    java -jar jenkins-cli.jar -s http://jenkins_url get-job job_name     # Get job configuration</code></pre>
  </div>
  <h3>Jenkins Pipeline Examples</h3>

  <div class="command-block">
    <h4>Jenkinsfile (Declarative pipeline)</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    pipeline {
    agent any

    stages {
    stage('Build') {
    steps {
    sh 'npm install'
    sh 'npm run build'
    }
    }
    stage('Test') {
    steps {
    sh 'npm test'
    }
    }
    stage('Deploy') {
    steps {
    sh '
    # Docker Build Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Build Images
    docker build -t myapp:latest .'
    sh '
    # Docker Push Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Push Images
    docker push myapp:latest'
    }
    }
    }

    post {
    success {
    echo 'Pipeline succeeded!'
    }
    failure {
    echo 'Pipeline failed!'
    }
    }
    }</code></pre>
  </div>

  <h3>GitHub Actions Workflow</h3>

  <div class="command-block">
    <h4>.github/workflows/ci.yml</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    ame: CI/CD Pipeline

    on:
    push:
    branches: [ main ]
    pull_request:
    branches: [ main ]

    jobs:
    build:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Node.js
    uses: actions/setup-node@v3
    with:
    node-version: '18'

    - name: Install dependencies
    run: npm ci

    - name: Run tests
    run: npm test

    - name: Build Docker image
    run:
    # Docker Build Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Build Images
    docker build -t myapp:latest .

    - name: Push to registry
    env:
    DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
    DOCKER_PASSWORD: ${{ secrets.DOCKER_PASSWORD }}
    run: |
    echo $DOCKER_PASSWORD | docker login -u $DOCKER_USERNAME --password-stdin

    # Docker Push Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Push Images
    docker push myapp:latest</code></pre>
  </div>

  <h3>GitLab CI Pipeline</h3>

  <div class="command-block">
    <h4>.gitlab-ci.yml</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    tages:
    - build
    - test
    - deploy

    variables:
    DOCKER_IMAGE: registry.gitlab.com/myproject/myapp

    build:
    stage: build
    script:
    - npm install
    - npm run build
    artifacts:
    paths:
    - dist/

    test:
    stage: test
    script:
    - npm test

    deploy:
    stage: deploy
    script:
    -
    # Docker Build Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Build Images
    docker build -t $DOCKER_IMAGE:$CI_COMMIT_SHA .
    -
    # Docker Push Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Push Images
    docker push $DOCKER_IMAGE:$CI_COMMIT_SHA
    only:
    - main</code></pre>
  </div>

  <h3>GitHub Actions</h3>
  <div class="command-block">
    <h4>Workflow Commands</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    h workflow list                                          # List workflows
    gh workflow run workflow.yml                              # Trigger workflow run
    gh run list                                               # Show workflow runs</code></pre>
  </div>

  <h3>GitLab CI</h3>
  <div class="command-block">
    <h4>Pipeline Commands</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    itlab-runner list                                                                 # List registered runners
    gitlab-runner exec docker job_name                                                 # Run job locally
    curl --header "PRIVATE-TOKEN: <token>" https://gitlab.com/api/v4/projects/:id/pipelines   # Trigger pipeline via API</code></pre>
  </div>
</div>

<h2 id="advanced-cloud">37. ADVANCED CLOUD (Terraform modules, AWS services: S3, EC2, RDS, IAM, Multi-cloud)</h2>

<div>
  <h3>Terraform Commands</h3>
  <div class="command-block">
    <h4>Manage Infrastructure</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    terraform init                                          # Initialize working directory

    # Terraform Plan Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Plan Changes
    terraform plan                                              # Preview changes
    <pre><code>
    # Terraform Apply Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Apply Changes
    terraform apply                                             # Apply infrastructure changes
    terraform destroy                                           # Destroy infrastructure
    terraform fmt                                               # Format configuration files
    terraform validate                                          # Validate configuration
    terraform state list                                        # List resources in state</code></pre>
  </div>

  <h3>AWS Services via CLI</h3>
  <div class="command-block">
    <h4>EC2, S3, RDS, IAM</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    aws ec2 describe-instances                                  # List EC2 instances
    aws s3 ls                                                   # List S3 buckets
    aws s3 cp file s3://bucket_name/                            # Upload file to S3
    aws rds describe-db-instances                               # List RDS instances
    aws iam list-users                                          # List IAM users
    aws iam create-role --role-name MyRole --assume-role-policy-document file://trust-policy.json</code></pre>
  </div>

  <h3>Multi-Cloud / Advanced Terraform</h3>
  <div class="command-block">
    <h4>Modules and Workspaces</h4>
    <pre><code>
    # Enterprise Standards
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    odule "vpc" {
    source = "./modules/vpc"
    cidr_block = "10.0.0.0/16"
    }

    terraform workspace new azure
    terraform workspace select aws</code></pre>
  </div>
</div>

<h2 id="advanced-devops">38. ADVANCED DEVOPS WORKFLOWS</h2>

<div>
  <div class="command-block">
    <h4>Multi-Stage Docker Builds</h4>
    <pre><code>
    # Multi-stage Dockerfile example
    FROM node:16-alpine AS builder
    WORKDIR /app
    COPY package*.json ./
    RUN npm ci --only=production

    FROM node:16-alpine AS runtime
    WORKDIR /app
    COPY --from=builder /app/node_modules ./node_modules
    COPY . .
    EXPOSE 3000
    CMD ["node", "server.js"]

    # Build and push automation

    # Docker Build Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Build Images
    docker build -t myapp:${BUILD_NUMBER} .
    docker tag myapp:${BUILD_NUMBER} registry.com/myapp:${BUILD_NUMBER}

    # Docker Push Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Push Images
    docker push registry.com/myapp:${BUILD_NUMBER}

    # Clean up old images
    docker image prune -f
    docker volume prune -f</code></pre>
  </div>

  <div class="command-block">
    <h4>Kubernetes Advanced Deployments</h4>
    <pre><code>
    # Rolling update with zero downtime
    kubectl set image deployment/myapp myapp=myapp:v2.0
    kubectl rollout status deployment/myapp
    kubectl rollout history deployment/myapp
    kubectl rollout undo deployment/myapp --to-revision=1

    # Canary deployment

    # Kubernetes Apply Operations (Enterprise Standards)
    # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

    # Apply Manifests
    kubectl apply -f - <<EOF
    apiVersion: argoproj.io/v1alpha1
    kind: Rollout
    metadata:
    name: myapp
    spec:
    replicas: 5
    strategy:
    canary:
    steps:
    - setWeight: 20
    - pause: {duration: 10m}
    - setWeight: 40
    - pause: {duration: 10m}
    - setWeight: 60
    - pause: {duration: 10m}
    - setWeight: 80
    - pause: {duration: 10m}
    EOF

    # Blue-green deployment

    # Kubernetes Apply Operations (Enterprise Standards)
    # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

    # Apply Manifests
    kubectl apply -f - <<EOF
    apiVersion: argoproj.io/v1alpha1
    kind: Rollout
    metadata:
    name: myapp
    spec:
    replicas: 5
    strategy:
    blueGreen:
    activeService: myapp-active
    previewService: myapp-preview
    autoPromotionEnabled: false
    scaleDownDelaySeconds: 30
    EOF</code></pre>
  </div>

  <div class="command-block">
    <h4>Infrastructure as Code Patterns</h4>
    <pre><code># Terraform modules structure
    terraform/
    ├── modules/
    │   ├── vpc/
    │   │   ├── main.tf
    │   │   ├── variables.tf
    │   │   └── outputs.tf
    │   ├── eks/
    │   └── rds/
    ├── environments/
    │   ├── dev/
    │   ├── staging/
    │   └── prod/
    └── scripts/

    # Advanced Terraform commands
    terraform fmt -recursive
    terraform validate

    # Terraform Plan Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Plan Changes
    terraform plan -var-file=prod.tfvars -out=plan.tfplan

    # Terraform Apply Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Apply Changes
    terraform apply plan.tfplan
    terraform state list
    terraform state show aws_instance.web
    terraform import aws_instance.web i-1234567890abcdef0

    # Terraform workspaces
    terraform workspace new dev
    terraform workspace select dev
    terraform workspace list
    terraform workspace delete staging</code></pre>
  </div>

  <div class="command-block">
    <h4>Ansible Advanced Patterns</h4>
    <pre><code># Ansible roles structure

    # Ansible Roles Structure (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Role Directory
    roles/
    ├── common/
    │   ├── tasks/
    │   ├── handlers/
    │   ├── templates/
    │   ├── files/
    │   ├── vars/
    │   └── defaults/
    └── nginx/
    ├── tasks/
    ├── handlers/
    ├── templates/
    └── vars/

    # Advanced playbook with conditions
    ---
    - name: Deploy web application
    hosts: webservers
    become: yes
    vars:
    app_version: "{{ app_version | default('latest') }}"
    environment: "{{ environment | default('dev') }}"

    tasks:
    - name: Install packages
    package:
    name: "{{ item }}"
    state: present
    loop:
    - nginx
    - python3-pip
    when: ansible_os_family == "RedHat"

    - name: Configure nginx
    template:
    src: nginx.conf.j2
    dest: /etc/nginx/nginx.conf
    notify: restart nginx
    vars:
    worker_processes: "{{ ansible_processor_cores }}"

    - name: Deploy application
    git:
    repo: https://github.com/user/app.git
    dest: /var/www/app
    version: "{{ app_version }}"
    when: environment == "prod"

    handlers:
    - name: restart nginx
    service:
    name: nginx
    state: restarted

    # Ansible vault for secrets

    # Ansible Vault Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Encryption Management
    ansible-vault create secrets.yml                                 # Create encrypted file

    # Ansible Vault Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Encryption Management
    ansible-vault edit secrets.yml

    # Ansible Vault Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Encryption Management
    ansible-vault encrypt secrets.yml

    # Ansible Playbook Operations (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Execute Playbooks
    ansible-playbook --ask-vault-pass deploy.yml</code></pre>
  </div>
</div>

<h2 id="monitoring-observability">39. MONITORING & OBSERVABILITY STACK</h2>

<div>
  <div class="command-block">
    <h4>Prometheus Advanced Configuration</h4>
    <pre><code># Prometheus configuration with multiple targets
    global:
    scrape_interval: 15s
    evaluation_interval: 15s

    rule_files:
    - "alert_rules.yml"

    alerting:
    alertmanagers:
    - static_configs:
    - targets:
    - alertmanager:9093

    scrape_configs:
    - job_name: 'kubernetes-apiservers'
    kubernetes_sd_configs:
    - role: endpoints
    scheme: https
    tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabel_configs:
    - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
    action: keep
    regex: default;kubernetes;https

    - job_name: 'kubernetes-nodes'
    kubernetes_sd_configs:
    - role: node
    relabel_configs:
    - action: labelmap
    regex: __meta_kubernetes_node_label_(.+)

    - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
    action: keep
    regex: true</code></pre>
  </div>

  <div class="command-block">
    <h4>Grafana Dashboard Automation</h4>
    <pre><code># Grafana API automation
    export GRAFANA_URL="http://grafana:3000"
    export GRAFANA_TOKEN="eyJrIjoi..."

    # Create dashboard
    curl -X POST \
    -H "Authorization: Bearer $GRAFANA_TOKEN" \
    -H "Content-Type: application/json" \
    -d @dashboard.json \
    "$GRAFANA_URL/api/dashboards/db"

    # Create data source
    curl -X POST \
    -H "Authorization: Bearer $GRAFANA_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
    "name": "Prometheus",
    "type": "prometheus",
    "url": "http://prometheus:9090",
    "access": "proxy",
    "isDefault": true
    }' \
    "$GRAFANA_URL/api/datasources"

    # Export dashboard
    curl -H "Authorization: Bearer $GRAFANA_TOKEN" \
    "$GRAFANA_URL/api/dashboards/uid/dashboard-uid" | jq . > dashboard-backup.json</code></pre>
  </div>

  <div class="command-block">
    <h4>ELK Stack Advanced Setup</h4>
    <pre><code># Elasticsearch cluster configuration
    cluster.name: "production-cluster"
    node.name: "es-node-1"
    path.data: /usr/share/elasticsearch/data
    path.logs: /usr/share/elasticsearch/logs
    network.host: 0.0.0.0
    discovery.type: single-node
    xpack.security.enabled: true
    xpack.monitoring.collection.enabled: true

    # Logstash advanced pipeline
    input {
    beats {
    port => 5044
    }
    kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["application-logs"]
    }
    }

    filter {
    if [fields][service] == "nginx" {
    grok {
    match => { "message" => "%{NGINX_ACCESS}" }
    }
    date {
    match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
    geoip {
    source => "clientip"
    }
    }

    if [fields][service] == "application" {
    json {
    source => "message"
    }
    }
    }

    output {
    elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{[fields][service]}-%{+YYYY.MM.dd}"
    template_name => "logs"
    template_pattern => "logs-*"
    }
    }

    # Kibana index patterns and visualizations
    curl -X POST "kibana:5601/api/saved_objects/_import" \
    -H "kbn-xsrf: true" \
    --form file=@dashboards.ndjson</code></pre>
  </div>

  <div class="command-block">
    <h4>Distributed Tracing with Jaeger</h4>
    <pre><code># Jaeger deployment with Kubernetes
    apiVersion: apps/v1
    kind: Deployment
    metadata:
    name: jaeger
    spec:
    replicas: 1
    selector:
    matchLabels:
    app: jaeger
    template:
    metadata:
    labels:
    app: jaeger
    spec:
    containers:
    - name: jaeger
    image: jaegertracing/all-in-one:latest
    ports:
    - containerPort: 16686
    - containerPort: 14268
    env:
    - name: COLLECTOR_ZIPKIN_HTTP_PORT
    value: "9411"

    # OpenTelemetry collector configuration
    receivers:
    otlp:
    protocols:
    grpc:
    endpoint: 0.0.0.0:4317
    http:
    endpoint: 0.0.0.0:4318

    processors:
    batch:
    memory_limiter:
    limit_mib: 512

    exporters:
    jaeger:
    endpoint: jaeger:14250
    tls:
    insecure: true

    service:
    pipelines:
    traces:
    receivers: [otlp]
    processors: [memory_limiter, batch]
    exporters: [jaeger]</code></pre>
  </div>
</div>

<h2 id="security-compliance">40. SECURITY & COMPLIANCE AUTOMATION</h2>

<div>
  <div class="command-block">
    <h4>Container Security Scanning</h4>
    <pre><code># Trivy integration in CI/CD
    #!/bin/bash
    IMAGE_NAME="myapp:${BUILD_NUMBER}"

    # Scan for vulnerabilities
    trivy image --severity HIGH,CRITICAL --format json --output trivy-report.json "$IMAGE_NAME"

    # Check for critical vulnerabilities
    CRITICAL_COUNT=$(jq '.Results[]?.Vulnerabilities[]? | select(.Severity == "CRITICAL") | .VulnerabilityID' trivy-report.json | wc -l)

    if [ "$CRITICAL_COUNT" -gt 0 ]; then
    echo "Found $CRITICAL_COUNT critical vulnerabilities"
    exit 1
    fi

    # Generate SARIF report for GitHub
    trivy image --format sarif --output trivy.sarif "$IMAGE_NAME"

    # Grype alternative scanning
    grype "$IMAGE_NAME" -o json > grype-report.json</code></pre>
  </div>

  <div class="command-block">
    <h4>Kubernetes Security Policies</h4>
    <pre><code>
    # Pod Security Standards
    apiVersion: v1
    kind: Namespace
    metadata:
    name: secure-namespace
    labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted

    # Network Policy example
    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
    name: deny-all
    spec:
    podSelector: {}
    policyTypes:
    - Ingress
    - Egress

    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
    name: allow-frontend
    spec:
    podSelector:
    matchLabels:
    app: frontend
    policyTypes:
    - Ingress
    - Egress
    ingress:
    - from:
    - podSelector:
    matchLabels:
    app: ingress
    ports:
    - protocol: TCP
    port: 80

    # OPA Gatekeeper policies
    apiVersion: templates.gatekeeper.sh/v1beta1
    kind: ConstraintTemplate
    metadata:
    name: k8srequiredlabels
    spec:
    crd:
    spec:
    names:
    kind: K8sRequiredLabels
    targets:
    - target: admission.k8s.gatekeeper.sh
    rego: |
    package k8srequiredlabels
    violation[{"msg": msg}] {
    required := input.parameters.labels
    provided := input.review.object.metadata.labels
    missing := {x | x := required[_] not in provided}
    count(missing) > 0
    msg := sprintf("missing required labels: %v", [missing])
    }</code></pre>
  </div>

  <div class="command-block">
    <h4>Compliance Automation</h4>
    <pre><code>
    # CIS Benchmarks automation
    #!/bin/bash
    # CIS Kubernetes Benchmark automation

    # Kubernetes Apply Operations (Enterprise Standards)
    # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

    # Apply Manifests
    kubectl apply -f https://github.com/aquasecurity/kube-bench/releases/latest/download/kube-bench.yaml
    kubectl exec -it $(kubectl get pods -l app=kube-bench -o jsonpath='{.items[0].metadata.name}') -- \
    kube-bench --config-dir=/etc/kube-bench/cfg --benchmark cis-1.6

    # OpenSCAP scanning
    oscap xccdf eval \
    --profile xccdf_org.ssgproject.content_profile_cis \
    --results-arf results.arf \
    --report report.html \
    /usr/share/xml/scap/ssg/content/ssg-rhel8-xccdf.xml

    # AWS Config rules
    aws configservice put-config-rule \
    --config-rule file://s3-bucket-public-read-prohibited.json

    aws configservice start-config-rules-evaluation \
    --config-rule-name s3-bucket-public-read-prohibited

    # Azure Policy
    az policy assignment create \
    --name 'storage-accounts-secure-transfer' \
    --policy 'storage-accounts-secure-transfer' \
    --scope '/subscriptions/{subscription-id}'</code></pre>
  </div>

  <div class="command-block">
    <h4>Secrets Management</h4>
    <pre><code>
    # HashiCorp Vault integration
    vault auth enable kubernetes
    vault write auth/kubernetes/config \
    kubernetes_host="https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT" \
    kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt \
    token_reviewer_jwt=@/var/run/secrets/kubernetes.io/serviceaccount/token

    vault policy write myapp - <<EOF
    path "secret/data/myapp/*" {
    capabilities = ["read"]
    }
    EOF

    vault write auth/kubernetes/role/myapp \
    bound_service_account_names=myapp \
    bound_service_account_namespaces=default \
    policies=myapp \
    ttl=24h

    # Kubernetes Secrets Operator
    apiVersion: v1
    kind: Secret
    metadata:
    name: vault-secret
    annotations:
    vault.security.banzaicloud.io/vault-role: "myapp"
    vault.security.banzaicloud.io/vault-addr: "https://vault:8200"
    vault.security.banzaicloud.io/vault-tls-secret: "vault-tls"
    type: Opaque
    stringData:
    vault-path: "secret/data/myapp/database"
    vault-keys: "username,password"</code></pre>
  </div>
</div>

<h2 id="automation">41. AUTOMATION ONE-LINERS COMMANDS</h2>
<h3>Automation One-Liners</h3>

<div>
  <div class="command-block">
    <h4>Python & Pip - Enterprise Python Management</h4>
    <pre><code>
    # Enterprise Python and Pip Management (Enterprise Standards)
    # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

    # Python Version and Environment (Enterprise)
    python3 --version                                               # Check Python version (compatibility verification)
    python3 -V                                                    # Alternative version check
    python3 --version | grep -Po '(?<=Python )\d+\.\d+'    # Extract version number
    python3 -c "import sys; print(sys.version)"       # Programmatic version check

    # Enterprise Package Management (Enterprise)
    pip install package_name                                        # Install Python package (basic)
    pip install package_name==1.2.3                               # Install specific version (stability)
    pip install 'package_name>=1.0,<2.0'                        # Install with version constraints
    pip install --user package_name                                 # User-level installation (no sudo)
    pip install --upgrade package_name                              # Upgrade package with dependencies
    pip install -r requirements.txt                               # Install from requirements file

    # Enterprise Package Operations (Enterprise)
    pip uninstall package_name                                      # Remove package cleanly
    pip uninstall -y package_name                                 # Uninstall without confirmation (automation)
    pip list                                                        # List all installed packages
    pip list --format=columns                                      # Formatted package listing
    pip list --outdated                                           # Show outdated packages (security)
    pip show package_name                                           # Show detailed package information
    pip check                                                       # Check for dependency conflicts

    # Enterprise Virtual Environment Management (Enterprise)
    python3 -m venv env_name                                        # Create virtual environment
    python3 -m venv --prompt="myproject" env_name              # Custom environment prompt
    python3 -m venv --system-site-packages env_name       # Include system packages
    source env_name/bin/activate                                    # Activate virtual environment
    deactivate                                                      # Deactivate virtual environment
    virtualenv env_name                                             # Alternative virtual environment creation
    virtualenv --python=python3.9 env_name                       # Specific Python version

    # Enterprise Best Practices (Enterprise)
    # - Always use virtual environments for project isolation
    # - Pin package versions in requirements.txt for reproducibility
    # - Use --user flag for user-level installations when possible
    # - Regularly audit packages with pip list --outdated
    # - Use pip freeze > requirements.txt for dependency capture
    # - Implement security scanning with pip audit
    # - Document Python version requirements for compatibility

    # Enterprise Security Considerations (Enterprise)
    # - Use pip install --user to avoid system-wide installations
    # - Verify package integrity: pip install --require-hashes -r requirements.txt
    # - Use private package indexes for enterprise packages
    # - Implement package signing verification for security
    # - Regular security audits with pip-audit or safety tools
    # - Use virtual environments to isolate dependencies
    # - Document package sources and installation procedures
  </code></pre>
</div>

<div class="command-block">
  <h4>Git Version Control - Enterprise Repository Management</h4>
  <pre><code>
  # Enterprise Git Version Control (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Repository Operations (Enterprise)
  git clone repo_url                                              # Clone repository (full history)
  git clone --depth 1 repo_url                                   # Shallow clone (latest only)
  git clone --branch feature-branch repo_url                        # Clone specific branch
  git clone --recursive repo_url                                  # Clone with submodules
  git init                                                        # Initialize new repository

  # Status and Staging (Enterprise)
  git status                                                      # Show working directory status
  git status --porcelain                                         # Machine-readable output (scripting)
  git status --short                                              # Compact status display
  git add file                                                    # Stage specific file
  git add .                                                       # Stage all changes in current directory
  git add -A                                                      # Stage all changes (including deletions)
  git add -p                                                      # Interactive staging (patch by patch)

  # Enterprise Commit Operations (Enterprise)
  git commit -m "message"                                         # Commit with message
  git commit -m "message" --author="name <email>"            # Commit with custom author
  git commit --amend                                            # Amend last commit (careful in shared repos)
  git commit --no-verify                                        # Bypass pre-commit hooks (emergency)
  git commit -a -m "message"                                    # Stage and commit in one step

  # Branch Management (Enterprise)
  git branch                                                      # List all branches
  git branch -r                                                   # List remote branches only
  git branch -a                                                   # List all branches (local + remote)
  git branch feature-branch                                       # Create new branch
  git checkout branch_name                                        # Switch to branch
  git checkout -b feature-branch                                 # Create and switch branch
  git merge branch_name                                           # Merge branch into current
  git rebase branch_name                                          # Rebase current branch
  git branch -d branch_name                                     # Delete local branch
  git push origin --delete branch_name                             # Delete remote branch

  # Synchronization Operations (Enterprise)
  git push                                                        # Push to remote (current branch)
  git push origin main                                           # Push specific branch
  git push --all origin                                          # Push all branches
  git push --tags origin                                         # Push all tags
  git pull                                                        # Pull and merge (fast-forward)
  git pull --rebase                                              # Pull and rebase (cleaner history)
  git fetch                                                       # Fetch without merging
  git fetch --all                                                # Fetch all remotes

  # Enterprise History and Diff (Enterprise)
  git log                                                         # Show commit history
  git log --oneline                                              # Compact log display
  git log --graph --oneline --all                               # Visual branch history
  git log --author="name"                                        # Filter by author
  git log --since="1 week ago"                                  # Filter by time
  git diff                                                        # Show unstaged changes
  git diff --staged                                               # Show staged changes
  git diff commit1 commit2                                       # Compare commits
  git diff branch1..branch2                                     # Compare branches

  # Stashing and Recovery (Enterprise)
  git stash                                                       # Stash current changes
  git stash save "message"                                       # Stash with message
  git stash list                                                  # List all stashes
  git stash pop                                                   # Apply and remove latest stash
  git stash apply stash@{0}                                     # Apply stash without removing
  git stash clear                                                 # Remove all stashes
  git stash branch branch_name stash@{0}                         # Create branch from stash

  # Enterprise Reset and Recovery (Enterprise)
  git reset --soft HEAD~1                                      # Reset staging area (keep changes)
  git reset --mixed HEAD~1                                      # Reset staging area (keep changes unstaged)
  git reset --hard HEAD~1                                      # Reset everything (destructive)
  git revert commit_id                                           # Create new commit to undo changes
  git cherry-pick commit_id                                       # Apply specific commit
  git clean -fd                                                   # Remove untracked files and directories

  # Enterprise Best Practices (Enterprise)
  # - Always use descriptive commit messages following enterprise standards
  # - Use feature branches for development work
  # - Regular pulls to stay updated with main branch
  # - Use .gitignore to exclude sensitive files
  # - Sign commits for security: git commit -S -m "message"
  # - Use git hooks for automated quality checks
  # - Document branching strategies in team guidelines
  # - Use tags for release management

  # Enterprise Security Considerations (Enterprise)
  # - Use SSH keys for authentication instead of passwords
  # - Enable signed commits for code integrity verification
  # - Regular audit of repository access and permissions
  # - Use git-secrets or similar tools to prevent credential leaks
  # - Implement branch protection rules for main branches
  # - Use two-factor authentication for repository access
  # - Document and rotate access credentials regularly
</code></pre>
</div>

<div class="command-block">
  <h4>Docker - Enterprise Container Management</h4>
  <pre><code>
  # Enterprise Docker Container Management (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Container Operations (Enterprise)
  docker run -it --name container_name image                      # Run container interactively
  docker run -d --name container_name image                      # Run container in background
  docker run -p 8080:80 --name web_container image          # Run with port mapping
  docker run -v /host/path:/container/path --name data_container image  # Run with volume mount
  docker run --restart unless-stopped --name service_container image  # Auto-restart container
  docker run --memory="512m" --cpus="1.0" --name limited_container image  # Resource limits

  # Container Management (Enterprise)
  docker ps                                                       # List running containers
  docker ps -a                                                    # List all containers (including stopped)
  docker ps -q                                                    # List container IDs only (scripting)
  docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"  # Custom format output
  docker exec -it container_name bash                             # Enter running container
  docker exec -u root container_name command                    # Execute as root user
  docker stop container_name                                      # Stop container gracefully
  docker kill container_name                                       # Force stop container
  docker restart container_name                                    # Restart container
  docker rm container_name                                        # Remove container
  docker rm -f container_name                                     # Force remove running container
  docker container prune                                         # Remove all stopped containers

  # Enterprise Image Operations (Enterprise)
  docker images                                                   # List all images
  docker images -q                                                # List image IDs only
  docker images --format "table {{.Repository}}\t{{.Tag}}\t{{.Size}}"  # Custom format
  docker pull image_name                                          # Pull image from registry
  docker pull image_name:tag                                      # Pull specific version
  docker push image_name                                          # Push image to registry
  docker push registry.com/namespace/image:tag                   # Push to specific registry
  docker rmi image_name                                           # Remove image
  docker rmi -f image_name                                        # Force remove image
  docker image prune                                             # Remove dangling images
  docker build -t image_name .                                    # Build Docker image
  docker build -t image_name:1.0 .                              # Build with tag
  docker build -f Dockerfile.prod -t image_name .               # Build with specific Dockerfile

  # Enterprise Volume and Network Management (Enterprise)
  docker volume create volume_name                               # Create named volume
  docker volume ls                                                # List volumes
  docker volume inspect volume_name                               # Inspect volume details
  docker volume rm volume_name                                    # Remove volume
  docker network create network_name                              # Create network
  docker network ls                                              # List networks
  docker network connect network_name container_name              # Connect container to network
  docker network disconnect network_name container_name         # Disconnect from network
</code></pre>
<div class="command-block">
  <h4>Kubernetes (kubectl) - Enterprise Cluster Management</h4>
  <pre><code>
  # Enterprise Kubernetes Management (Enterprise Standards)
  # Compatible with: Red Hat OpenShift, Debian, Ubuntu, openSUSE, Arch

  # Cluster Information and Status (Enterprise)
  kubectl cluster-info                                            # Display cluster endpoint info
  kubectl version                                                 # Client and server version details
  kubectl version --short                                         # Short version format
  kubectl get nodes                                               # List cluster nodes
  kubectl get nodes -o wide                                       # Show additional node details
  kubectl top nodes                                               # Show node resource usage
  kubectl top pods                                                 # Show pod resource usage

  # Resource Discovery (Enterprise)
  kubectl get pods                                                    # List pods in current namespace
  kubectl get pods --all-namespaces                                  # List pods in all namespaces
  kubectl get pods -o wide                                         # Show pod node assignment
  kubectl get deployments                                             # List deployments
  kubectl get services                                               # List services
  kubectl get svc                                                     # Short form for services
  kubectl get configmaps                                            # List config maps
  kubectl get secrets                                               # List secrets (careful with sensitive data)
  kubectl get ingress                                                # List ingress resources
  kubectl get namespaces                                            # List namespaces
  kubectl get ns                                                      # Short form for namespaces

  # Resource Details and Inspection (Enterprise)
  kubectl describe pod pod_name                                       # Pod details and events
  kubectl describe deployment deployment_name                         # Deployment details
  kubectl describe service service_name                               # Service details and endpoints
  kubectl describe node node_name                                   # Node details and capacity
  kubectl logs pod_name                                               # View pod logs
  kubectl logs -f pod_name                                         # Follow logs in real-time
  kubectl logs --tail=100 pod_name                                  # Show last 100 log lines
  kubectl logs deployment/deployment_name                           # View deployment logs
  kubectl exec -it pod_name -- bash                                   # Enter pod shell
  kubectl exec -it pod_name -- sh                                    # Enter pod with alternative shell

  # Enterprise Resource Management (Enterprise)
  kubectl apply -f file.yaml                                          # Apply configuration file
  kubectl apply -f directory/                                        # Apply all files in directory
  kubectl apply -k dir/ -o yaml                                   # Apply with kustomization
  kubectl create -f file.yaml                                        # Create resource from file
  kubectl delete -f file.yaml                                         # Delete resource from file
  kubectl delete pod pod_name                                        # Delete specific pod
  kubectl delete deployment deployment_name                           # Delete deployment
  kubectl delete service service_name                               # Delete service
  kubectl delete all --all                                          # Delete all resources in namespace

  # Enterprise Scaling and Updates (Enterprise)
  kubectl scale deployment deploy_name --replicas=3                   # Scale deployment
  kubectl autoscale deployment deploy_name --min=2 --max=5   # Enable horizontal pod autoscaler
  kubectl set image deployment/deploy_name container=image:tag  # Update container image
  kubectl rollout status deployment/deploy_name                   # Check rollout status
  kubectl rollout history deployment/deploy_name                  # View rollout history
  kubectl rollout undo deployment/deploy_name                     # Undo last rollout
  kubectl rollout restart deployment/deploy_name                # Restart deployment

  # Enterprise Configuration Management (Enterprise)
  kubectl config view                                                # Show current configuration
  kubectl config set-context context_name                           # Set current context
  kubectl config use-context context_name                            # Switch context
  kubectl config get-contexts                                       # List available contexts
  kubectl config set-cluster cluster_name --server=...           # Add cluster config
  kubectl config set-credentials user_name --token=...          # Add credentials
  kubectl config delete-context context_name                        # Delete context

  # Enterprise Debugging and Troubleshooting (Enterprise)
  kubectl get events                                                 # Show cluster events
  kubectl get events --sort-by='.lastTimestamp'                # Show recent events first
  kubectl explain pod                                                # Get pod field documentation
  kubectl explain service.spec                                      # Get service specification details
  kubectl auth can-i create pods --as=user --namespace=dev       # Check permissions
  kubectl get pod pod_name -o yaml                               # Get pod as YAML
  kubectl get pod pod_name -o json                               # Get pod as JSON
  kubectl port-forward pod_name 8080:80                         # Forward port to local

  # Enterprise Best Practices (Enterprise)
  # - Always use namespaces for resource isolation and organization
  # - Implement proper resource requests and limits for QoS
  # - Use labels and selectors for resource organization
  # - Implement health checks and readiness probes
  # - Use ConfigMaps and Secrets for configuration management
  # - Implement proper RBAC for access control
  # - Use network policies for traffic control
  # - Document and version control all manifest files
  # - Use Helm for complex application management

  # Enterprise Security Considerations (Enterprise)
  # - Implement proper RBAC (Role-Based Access Control)
  # - Use Secrets for sensitive data, never in ConfigMaps
  # - Implement network policies for traffic segmentation
  # - Use pod security policies and security contexts
  # - Regular security audits of cluster configurations
  # - Use image scanning for container security
  # - Implement proper logging and monitoring
  # - Use admission controllers for security validation
  # - Document and follow Kubernetes security best practices
</code></pre>
</div>

<div class="command-block">
  <h4>Ansible - Enterprise Automation and Configuration Management</h4>
  <pre><code>
  # Enterprise Ansible Automation (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Basic Ansible Operations (Enterprise)
  ansible all -m ping                                                 # Test connectivity to all hosts
  ansible all -m setup                                               # Gather facts about all hosts
  ansible all -m command -a "uptime"                               # Execute command on all hosts
  ansible all -m shell -a "ps aux | grep nginx"                  # Execute shell command
  ansible all -m raw -a "rpm -qa | grep httpd"                  # Raw command execution
  ansible-inventory inventory.yml --list                              # Test inventory file

  # Enterprise Playbook Execution (Enterprise)
  ansible-playbook playbook.yml                                       # Run playbook against default inventory
  ansible-playbook -i inventory.ini playbook.yml                      # Specify custom inventory
  ansible-playbook -i hosts.yml, playbook.yml                     # Multiple inventory files
  ansible-playbook --limit webservers playbook.yml                  # Limit to specific host group
  ansible-playbook --check playbook.yml                               # Dry run (no changes made)
  ansible-playbook --diff playbook.yml                                # Show changes that would be made
  ansible-playbook --syntax-check playbook.yml                        # Validate playbook syntax
  ansible-playbook --start-at-task="task_name" playbook.yml      # Start from specific task
  ansible-playbook --step playbook.yml                               # Step through tasks interactively

  # Enterprise Module Operations (Enterprise)
  ansible all -m yum -a "name=package state=present"                  # Install package (RHEL/CentOS)
  ansible all -m apt -a "name=package state=present"                  # Install package (Debian/Ubuntu)
  ansible all -m dnf -a "name=package state=present"                  # Install package (RHEL 8+)
  ansible all -m copy -a "src=file dest=/tmp/"                        # Copy file to remote hosts
  ansible all -m copy -a "src=file dest=/tmp/ backup=yes"        # Copy with backup
  ansible all -m template -a "src=file.j2 dest=/tmp/file"             # Template file with Jinja2
  ansible all -m lineinfile -a "path=/tmp/file line='content'"       # Ensure line in file
  ansible all -m blockinfile -a "path=/tmp/file block=''"             # Ensure block in file
  ansible all -m service -a "name=httpd state=started"                # Manage service state
  ansible all -m systemd -a "name=httpd state=started"              # Manage systemd service
  ansible all -m user -a "name=user state=present"                   # Manage user accounts
  ansible all -m group -a "name=group state=present"                  # Manage groups
  ansible all -m cron -a "name='backup job' job='/script.sh'"        # Manage cron jobs
  ansible all -m firewalld -a "service=http state=enabled permanent=yes"  # Manage firewall rules

  # Enterprise Role and Galaxy Management (Enterprise)
  ansible-galaxy install role_name                                    # Install role from Ansible Galaxy
  ansible-galaxy install -r requirements.yml                          # Install multiple roles from file
  ansible-galaxy list                                                  # List installed roles
  ansible-galaxy search search_term                                   # Search for roles
  ansible-galaxy info role_name                                      # Get role information
  ansible-galaxy role init role_name                                 # Initialize new role structure

  # Enterprise Vault Operations (Security)
  ansible-vault encrypt file.yml                                      # Encrypt sensitive file
  ansible-vault decrypt file.yml                                      # Decrypt encrypted file
  ansible-vault encrypt_string --output='encrypted.txt' 'secret'    # Encrypt string
  ansible-vault view encrypted_file.yml                              # View encrypted file content
  ansible-playbook --ask-vault-pass playbook.yml                    # Run with vault password
  ansible-playbook --vault-password-file=vault.txt playbook.yml    # Use vault password file
  ansible-vault rekey file.yml                                     # Change vault password

  # Enterprise Inventory Management (Enterprise)
  ansible-inventory inventory.yml --list                              # List all hosts in inventory
  ansible-inventory inventory.yml --host hostname                   # Get specific host variables
  ansible all -m debug -a "var=hostvars[inventory_hostname]"        # Debug host variables
  ansible all -m setup | grep ansible_distribution                 # Get OS distribution
  ansible all -m setup | grep ansible_os_family                    # Get OS family

  # Enterprise Best Practices (Enterprise)
  # - Use idempotent modules for configuration management
  # - Implement proper error handling and rollback strategies
  # - Use roles for reusable and modular playbooks
  # - Implement proper inventory management and organization
  # - Use Ansible Vault for sensitive data management
  # - Implement proper logging and monitoring
  # - Use check mode for testing before applying changes
  # - Document playbooks and roles thoroughly
  # - Implement proper version control for all Ansible content

  # Enterprise Security Considerations (Enterprise)
  # - Use Ansible Vault for all sensitive data (passwords, keys)
  # - Implement proper SSH key management for host access
  # - Use privilege escalation (become) properly and securely
  # - Implement proper logging and audit trails
  # - Use encrypted communication channels (SSH, HTTPS)
  # - Regular security audits of Ansible configurations
  # - Implement proper access control and RBAC
  # - Use signed roles and verify integrity
  # - Document and follow Ansible security best practices

  # Enterprise Performance Optimization (Enterprise)
  # - Use fact caching for large inventories
  # - Implement parallel execution with forks parameter
  # - Use connection pipelining for faster execution
  # - Optimize inventory for better performance
  # - Use strategy plugins for complex deployments
  # - Implement proper resource management
  # - Monitor and optimize playbook execution times
</code></pre>
</div>

<div class="command-block">
  <h4>Terraform - Enterprise Infrastructure as Code</h4>
  <pre><code>
  # Enterprise Terraform IaC Management (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Terraform Workspace and Initialization (Enterprise)
  terraform init                                                      # Initialize working directory
  terraform init -backend-config=backend.tf                        # Initialize with backend config
  terraform init -migrate-state                                     # Migrate state file
  terraform init -upgrade=false                                     # Skip provider upgrades
  terraform workspace new production                              # Create new workspace
  terraform workspace select production                           # Switch to workspace
  terraform workspace list                                        # List all workspaces
  terraform workspace show                                       # Show current workspace

  # Enterprise Planning and Validation (Enterprise)
  terraform plan                                                      # Show execution plan
  terraform plan -out=tfplan                                      # Save plan to file
  terraform plan -destroy                                          # Plan for destruction
  terraform plan -var="region=us-east-1"                       # Pass variables
  terraform plan -var-file=vars.tfvars                          # Load variables from file
  terraform validate                                               # Validate configuration syntax
  terraform fmt                                                   # Format configuration files
  terraform fmt -check                                            # Check formatting without changes
  terraform fmt -diff                                            # Show formatting changes
  terraform graph                                                # Generate dependency graph

  # Enterprise Infrastructure Management (Enterprise)
  terraform apply                                                     # Apply infrastructure changes
  terraform apply -auto-approve                                     # Apply without manual approval
  terraform apply -var-file=vars.tfvars                        # Apply with variables
  terraform apply -target=resource.name                             # Apply specific resource
  terraform apply -state-out=new_state.tfstate                 # Output new state file
  terraform destroy                                                  # Destroy all infrastructure
  terraform destroy -target=resource.name                        # Destroy specific resource
  terraform destroy -auto-approve                                  # Destroy without confirmation

  # Enterprise State Management (Enterprise)
  terraform state list                                                # List all managed resources
  terraform state show resource_name                               # Show specific resource state
  terraform state pull                                             # Pull state from remote backend
  terraform state push                                             # Push state to remote backend
  terraform state mv old_name new_name                            # Rename resource in state
  terraform state rm resource_name                                 # Remove resource from state
  terraform state taint resource_name                              # Mark resource for recreation
  terraform state untaint resource_name                            # Remove taint from resource
  terraform import resource_name resource_id                          # Import existing resource

  # Enterprise Provider and Module Management (Enterprise)
  terraform providers                                                 # Show required providers
  terraform providers lock                                          # Lock provider versions
  terraform providers mirror dir                                    # Mirror providers locally
  terraform providers schema -json                                # Get provider schema
  terraform get providers                                         # Install providers
  terraform init -upgrade=true                                   # Upgrade providers
  terraform module registry                                        # Browse module registry
  terraform module install module_name                           # Install module

  # Enterprise Enterprise Features (Enterprise)
  terraform enterprise list                                       # List enterprise features
  terraform enterprise apply                                       # Apply with enterprise features
  terraform enterprise plan                                        # Plan with enterprise features
  terraform workspace new production -enterprise-organization  # Enterprise workspace
  terraform workspace list -enterprise-organization             # List enterprise workspaces

  # Enterprise Security and Best Practices (Enterprise)
  # - Use remote state backends with proper access controls
  # - Implement state locking and encryption for team collaboration
  # - Use workspaces for environment separation (dev/staging/prod)
  # - Implement proper variable management and secret handling
  # - Use semantic versioning for infrastructure releases
  # - Implement proper CI/CD integration with automated testing
  # - Use Terraform Cloud for team collaboration and governance
  # - Implement proper cost management and monitoring
  # - Document all infrastructure decisions and architectures

  # Enterprise Security Considerations (Enterprise)
  # - Use encrypted remote state storage (S3, Azure Blob, GCS)
  # - Implement proper IAM roles and permissions for Terraform
  # - Use Terraform Cloud for SSO and team management
  # - Implement proper secret management (AWS Secrets Manager, Vault)
  # - Use network security groups and firewall rules properly
  # - Regular security audits of infrastructure configurations
  # - Implement proper logging and monitoring of all resources
  # - Use cost allocation tags and budgeting for cost control
  # - Document and follow Terraform security best practices

  # Enterprise Performance Optimization (Enterprise)
  # - Use parallel execution with proper resource targeting
  # - Implement proper state management and cleanup
  # - Use provider-specific optimizations and features
  # - Implement proper dependency management and module caching
  # - Use Terraform Cloud for remote operations and caching
  # - Monitor and optimize Terraform execution times
  # - Use appropriate instance types and resource sizing
  # - Implement proper resource lifecycle management
</code></pre>
</div>

<div class="command-block">
  <h4>AWS CLI - Enterprise Cloud Management</h4>
  <pre><code>
  # Enterprise AWS CLI Management (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # AWS Configuration and Authentication (Enterprise)
  aws configure                                                       # Configure profile & region
  aws configure --profile production                              # Configure specific profile
  aws configure set region us-east-1                            # Set default region
  aws configure set profile.default.region us-east-1              # Set region via CLI
  aws configure set profile.default.output json                 # Set output format
  aws configure set profile.default.cli_pager ''               # Disable pager
  aws configure list-profiles                                    # List all configured profiles
  aws sts get-caller-identity                                   # Verify current credentials

  # Enterprise S3 Storage Operations (Enterprise)
  aws s3 ls                                                           # List S3 buckets
  aws s3 ls s3://bucket-name/                                  # List bucket contents
  aws s3 cp file.txt s3://bucket/path/                        # Upload file to S3
  aws s3 cp s3://bucket/file.txt ./                          # Download file from S3
  aws s3 sync local/ s3://bucket/                            # Sync directory to S3
  aws s3 sync s3://bucket/ local/                            # Sync S3 to local directory
  aws s3 rb s3://bucket-name --force                          # Delete bucket (force)
  aws s3api create-bucket bucket-name --region us-east-1       # Create bucket with region
  aws s3api put-bucket-versioning --bucket bucket-name --versioning-configuration Status=Enabled  # Enable versioning

  # Enterprise EC2 Compute Operations (Enterprise)
  aws ec2 describe-instances                                          # List EC2 instances
  aws ec2 describe-instances --filters Name=tag:Environment,Values=production  # Filter by tags
  aws ec2 run-instances --image-id ami-12345 --count 1 --instance-type t3.micro  # Launch instance
  aws ec2 start-instances --instance-ids i-12345                      # Start EC2 instance
  aws ec2 stop-instances --instance-ids i-12345                       # Stop EC2 instance
  aws ec2 terminate-instances --instance-ids i-12345                 # Terminate EC2 instance
  aws ec2 create-image --instance-id i-12345 --name "My AMI"      # Create AMI from instance
  aws ec2 describe-images --owners self                              # List custom AMIs

  # Enterprise IAM Security Operations (Enterprise)
  aws iam list-users                                                  # List IAM users
  aws iam create-user --user-name newuser                           # Create IAM user
  aws iam delete-user --user-name username                           # Delete IAM user
  aws iam list-roles                                                 # List IAM roles
  aws iam create-role --role-name MyRole --assume-role-policy-document file://policy.json  # Create role
  aws iam list-policies                                              # List managed policies
  aws iam create-policy --policy-name MyPolicy --policy-document file://policy.json  # Create policy
  aws iam attach-user-policy --user-name username --policy-arn arn:aws:iam::aws:policy/MyPolicy  # Attach policy to user

  # Enterprise RDS Database Operations (Enterprise)
  aws rds describe-db-instances                                       # List RDS instances
  aws rds create-db-instance --db-instance-identifier mydb --db-instance-class db.t3.micro --engine mysql  # Create RDS instance
  aws rds delete-db-instance --db-instance-identifier mydb --skip-final-snapshot  # Delete RDS instance
  aws rds create-db-snapshot --db-instance-identifier mydb --db-snapshot-identifier my-snapshot  # Create snapshot
  aws rds describe-db-snapshots                                         # List RDS snapshots
  aws rds restore-db-instance-from-db-snapshot --db-instance-identifier newdb --db-snapshot-identifier my-snapshot  # Restore from snapshot

  # Enterprise CloudWatch Monitoring Operations (Enterprise)
  aws cloudwatch get-metric-statistics --namespace AWS/EC2 --metric-name CPUUtilization --dimensions Name=InstanceId,Value=i-12345  # Get EC2 metrics
  aws cloudwatch put-metric-data --namespace MyApp --metric-data MetricName=RequestCount,Value=42,Unit=Count  # Put custom metrics
  aws cloudwatch describe-alarms                                    # List CloudWatch alarms
  aws cloudwatch set-alarm-state --alarm-name MyAlarm --state-value ALARM  # Set alarm state
  aws logs describe-log-groups                                    # List CloudWatch log groups
  aws logs describe-log-streams --log-group-name /aws/ec2/instance-id  # Describe log streams

  # Enterprise Lambda Serverless Operations (Enterprise)
  aws lambda list-functions                                          # List Lambda functions
  aws lambda create-function --function-name MyFunction --runtime python3.8 --handler lambda_function.lambda_handler  # Create Lambda
  aws lambda invoke --function-name MyFunction --payload '{"key":"value"}' output.json  # Invoke Lambda
  aws lambda update-function-code --function-name MyFunction --zip-file fileb://deployment-package.zip  # Update function code
  aws lambda delete-function --function-name MyFunction               # Delete Lambda function
  aws lambda create-event-source-mapping --function-name MyFunction --event-source arn:aws:s3:::bucket-name  # Add trigger

  # Enterprise Security and Best Practices (Enterprise)
  # - Use IAM roles instead of access keys when possible
  # - Implement proper resource tagging for cost allocation
  # - Use AWS KMS for encryption of sensitive data
  # - Enable CloudTrail for API audit logging
  # - Use VPCs for network isolation and security
  # - Implement proper security groups and NACLs
  # - Use AWS Config for compliance monitoring
  # - Regular security audits with AWS Security Hub
  # - Use AWS Organizations for multi-account management

  # Enterprise Cost and Performance Optimization (Enterprise)
  # - Use appropriate instance types and sizing
  # - Implement auto-scaling for cost optimization
  # - Use reserved instances for predictable workloads
  # - Monitor costs with AWS Cost Explorer
  # - Use AWS Budgets for cost control
  # - Implement proper data lifecycle policies for S3
  # - Use AWS Trusted Advisor for optimization recommendations
  # - Use CloudWatch for monitoring and alerting
  # - Document and follow AWS security and cost best practices
</code></pre>
</div>

<div class="command-block">
  <h4>Shell Automation - Enterprise Scripting and Scheduling</h4>
  <pre><code>
  # Enterprise Shell Automation (Enterprise Standards)
  # Compatible with: Red Hat, Debian, Ubuntu, openSUSE, Arch

  # Script Structure and Best Practices (Enterprise)
  #!/bin/bash                        # Script shebang (portable)
  #!/usr/bin/env bash              # Environment-based shebang (more portable)
  set -euo pipefail               # Strict error handling (exit on error)
  set -x                           # Debug mode (show commands)
  set +x                           # Disable debug mode
  trap 'echo "Error on line $LINENO"; exit 1' ERR  # Error trapping

  # Enterprise Loop Constructs (Enterprise)
  for i in {1..5}; do echo "Processing item $i"; done    # For loop with logging
  for file in *.txt; do echo "Processing $file"; done        # File processing loop
  while [ $count -lt 10 ]; do echo "Count: $count"; ((count++)); done  # While loop with counter
  until [ $condition = true ]; do echo "Waiting..."; sleep 1; done  # Until loop

  # Enterprise Conditional Logic (Enterprise)
  if [ -f "$file" ]; then echo "File exists"; else echo "File not found"; fi  # File existence check
  if [ "$status" = "success" ]; then echo "Operation completed"; fi  # String comparison
  if [ $num -gt 10 ]; then echo "Number is greater than 10"; fi  # Numeric comparison
  case $choice in
  start) echo "Starting service..." ;;
  stop) echo "Stopping service..." ;;
  restart) echo "Restarting service..." ;;
  *) echo "Invalid option: $choice" >&2; exit 1 ;;
  esac

  # Enterprise Input and Output (Enterprise)
  read -p "Enter username: " username                              # Prompt for input
  read -s -p "Enter password: " password                        # Secure password input
  read -t 30 -p "Enter value (30s timeout): " value         # Input with timeout
  echo "Operation completed successfully" | tee -a operation.log  # Output with logging
  printf "Formatted output: %s - %d\n" "$string" "$number"   # Formatted output

  # Enterprise File Operations (Enterprise)
  for file in *.log; do
  echo "Processing: $file"
  cp "$file" "backup_$file"                                     # Create backup
  gzip "backup_$file"                                           # Compress backup
  rm "$file"                                                    # Remove original
  done
  </code></pre>
</div>

<div class="command-block">
  <h4>Process Management</h4>
  <pre><code>
  # Enterprise Process Management (Enterprise)
  pgrep -f process_name                                            # Find process ID
  kill -TERM $PID                                                 # Graceful process termination
  kill -KILL $PID                                                # Force process termination
  wait $PID                                                        # Wait for process completion
  nohup command &                                                  # Run process in background

  # Enterprise Systemd Services (Enterprise)
  systemctl list-timers                                             # List systemd timers
  systemctl start myservice.timer                                   # Start timer service
  systemctl enable myservice.timer                                  # Enable timer at boot
  systemctl status myservice.service                                # Check service status
  journalctl -u myservice.service -f                              # Follow service logs

  # Enterprise Cron Scheduling (Enterprise)
  crontab -e                                                        # Edit cron jobs
  crontab -l                                                        # List current cron jobs
  echo "0 2 * * * /path/to/script.sh" | crontab -              # Add cron job
  echo "0 3 * * * /path/to/script.sh" | crontab - username    # Add cron for specific user
  at 12:00 -f /path/to/script.sh 2>/dev/null                  # Schedule one-time task
  atq                                                              # List pending at jobs
  atrm job_number                                                    # Remove scheduled job

  # Enterprise Error Handling and Logging (Enterprise)
  exec 2> >(tee -a error.log)                                     # Redirect stderr to log
  exec 1> >(tee -a output.log)                                    # Redirect stdout to log
  log_message() { echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a app.log; }  # Logging function
  cleanup() { echo "Cleaning up..."; rm -f /tmp/tempfile; }  # Cleanup function
  trap cleanup EXIT                                                   # Execute cleanup on exit

  # Enterprise Security Considerations (Enterprise)
  # - Always quote variables: "$var" instead of $var
  # - Use set -euo pipefail for strict error handling
  # - Validate input parameters before processing
  # - Use absolute paths for critical operations
  # - Implement proper logging and audit trails
  # - Use secure methods for password handling
  # - Set appropriate file permissions: umask 077
  # - Use process isolation and resource limits
  # - Document script purpose and usage

  # Enterprise Performance Optimization (Enterprise)
  # - Use built-in commands instead of external programs
  # - Minimize subprocess calls for better performance
  # - Use appropriate data structures (arrays vs strings)
  # - Implement parallel processing where appropriate
  # - Use efficient file processing methods
  # - Cache frequently accessed data
  # - Monitor script performance and optimize bottlenecks

  # Enterprise Integration and Automation (Enterprise)
  # - Integrate with monitoring systems (Nagios, Zabbix)
  # - Use configuration management tools (Ansible, Puppet)
  # - Implement proper CI/CD pipeline integration
  # - Use API calls for external system integration
  # - Implement proper notification systems
  # - Use containerization for consistent environments
  # - Document all automation procedures and dependencies
</code></pre>
</div>
</div>

<div class="navigation" style="margin-top: 40px; text-align: center">
  <strong>End of Part 2 - DevOps Tools</strong><br />
  <a href="system-admin.html">💻 Go to Part 1</a>
  <a href="monitoring-security.html">🔒 Go to Part 3</a>
</div>
</div>

<script>
  // Auto-generate TOC based on <h2> headings
  const tocList = document.getElementById("toc-list");
  const headings = document.querySelectorAll("h2");

  headings.forEach((heading, index) => {
  const li = document.createElement("li");
  const a = document.createElement("a");
  a.href = `#${heading.id}`; // link to section
  a.textContent = `${heading.textContent}`; // same text
  li.appendChild(a);
  tocList.appendChild(li);
  });
</script>

<script>
  // JavaScript for download functionality
  function downloadAsFile(filename) {
  // Get the entire HTML content
  const htmlContent = document.documentElement.outerHTML;

  // Create a blob with the HTML content
  const blob = new Blob([htmlContent], { type: "text/html" });

  // Create a temporary link element
  const link = document.createElement("a");
  link.href = URL.createObjectURL(blob);
  link.download = filename || "linux-devops-devops-tools.html";

  // Append to body, click, and remove
  document.body.appendChild(link);
  link.click();
  document.body.removeChild(link);
  }

  function printPage() {
  window.print();
  }

  // Add smooth scrolling for navigation links
  document.querySelectorAll('a[href^="#"]').forEach((anchor) => {
  anchor.addEventListener("click", function (e) {
  e.preventDefault();

  const targetId = this.getAttribute("href");
  if (targetId === "#") return;

  const targetElement = document.querySelector(targetId);
  if (targetElement) {
  window.scrollTo({
  top: targetElement.offsetTop - 20,
  behavior: "smooth",
  });
  }
  });
  });


  // Get all pre elements (command blocks)
  const preElements = document.querySelectorAll("pre");
  let results = [];

  preElements.forEach((pre, index) => {
  const text = pre.textContent.toLowerCase();
  if (text.includes(searchTerm)) {
  // Get the heading for context
  let heading = "Unknown Section";
  let currentElement = pre.previousElementSibling;

  // Find the nearest heading
  while (currentElement) {
  if (currentElement.tagName.match(/^H[2-4]$/)) {
  heading = currentElement.textContent;
  break;
  }
  currentElement = currentElement.previousElementSibling;
  }

  // Extract matching lines
  const lines = pre.textContent.split("\n");
  const matchingLines = lines
  .filter((line) => line.toLowerCase().includes(searchTerm))
  .slice(0, 3); // Show first 3 matching lines

  if (matchingLines.length > 0) {
  results.push({
  heading: heading,
  lines: matchingLines,
  element: pre,
  });
  }
  }
  });

  // Display results
  if (results.length > 0) {
  let html = `<h3>Found ${results.length} results for "${searchTerm}"</h3>`;

  results.forEach((result, index) => {
  html += `
  <div style="background-color: #f8f9fa; padding: 15px; margin: 10px 0; border-left: 4px solid #3498db;">
    <h4 style="margin-top: 0;">${result.heading}</h4>
    <pre style="margin: 10px 0;">${result.lines.join(
    "\n"
    )}</pre>
    <button onclick="scrollToResult(${index})"
    style="padding: 5px 10px; background-color: #2ecc71; color: white; border: none; border-radius: 3px;">
    Go to Command
  </button>
</div>
`;
});

resultsContainer.innerHTML = html;
resultsContainer.style.display = "block";

// Store results for scrolling
window.searchResults = results;
} else {
resultsContainer.innerHTML = `<p style="color: #e74c3c;">No results found for "${searchTerm}"</p>`;
resultsContainer.style.display = "block";
}

function scrollToResult(index) {
if (window.searchResults && window.searchResults[index]) {
const element = window.searchResults[index].element;
window.scrollTo({
top: element.offsetTop - 20,
behavior: "smooth",
});

// Highlight the element temporarily
element.style.backgroundColor = "#fff3cd";
element.style.borderColor = "#ffc107";

setTimeout(() => {
element.style.backgroundColor = "";
element.style.borderColor = "";
}, 2000);
}
}

function clearSearch() {
document.getElementById("commandSearch").value = "";
const resultsContainer = document.getElementById("searchResults");
resultsContainer.style.display = "none";
resultsContainer.innerHTML = "";
}

</script>

<style>
</style>

<div class="info-box" style="border-left-color: #ff6b9d; margin-top: 50px;">
  <h3 style="color: #ff6b9d">🎯 Section Complete?</h3>
  <ul>
    <li><strong>📚 Next Steps:</strong> Continue to <a href="index.html" style="color: #ff6b9d;">main page</a> or explore other sections</li>
      <li><strong>🔍 Quick Review:</strong> Use Ctrl+F to find specific commands</li>
        <li><strong>💼 Career Focus:</strong> Practice these commands for senior-level positions</li>
          <li><strong>📖 Documentation:</strong> Check <a href="README.md" style="color: #ff6b9d;">README.md</a> for full overview</li>
          </ul>
        </div>

        <div style="text-align: center; margin-top: 30px; padding: 20px; background: #0d1117; border-radius: 10px; border: 2px solid #30363d;">
          <h4 style="color: #00d9ff; margin-top: 0;">🏆 Ready for Next Level?</h4>
          <p style="color: #c9d1d9; font-size: 1em; margin-bottom: 15px;">
          Master this section and advance your DevOps career
        </p>
        <div style="margin-top: 15px;">
          <a href="index.html" style="display: inline-block; background: #00d9ff; color: #0d1117; padding: 10px 20px; border-radius: 5px; text-decoration: none; font-weight: bold; margin: 5px;">🏠 Back to Home</a>
          <a href="system-admin.html" style="display: inline-block; background: #28a745; color: white; padding: 10px 20px; border-radius: 5px; text-decoration: none; font-weight: bold; margin: 5px;">💻 System Admin</a>
          <a href="monitoring-security.html" style="display: inline-block; background: #ff6b9d; color: white; padding: 10px 20px; border-radius: 5px; text-decoration: none; font-weight: bold; margin: 5px;">🔒 Security</a>
        </div>
      </div>
    </div>

    <!-- SCRIPT SECTION -->
    <div id="script">
      <script>
        // Dark Mode Toggle
        function initDarkMode() {
        const theme = localStorage.getItem("theme") || "light";
        if (theme === "dark") {
        document.body.classList.add("dark-mode");
        }

        const themeToggle = document.getElementById("themeToggle");
        if (themeToggle) {
        themeToggle.addEventListener("click", toggleTheme);
        }
        }

        function toggleTheme() {
        document.body.classList.toggle("dark-mode");
        const theme = document.body.classList.contains("dark-mode")
        ? "dark"
        : "light";
        localStorage.setItem("theme", theme);
        }

        // Comment Formatter - Fixed version
        function formatComments() {
        document.querySelectorAll("pre code").forEach((codeBlock) => {
        let text = codeBlock.innerText;

        // Remove all leading spaces from all lines first
        let lines = text.split("\n");
        lines = lines.map(line => line.replace(/^\s+/, ""));

        // Find the longest command length for proper alignment
        let maxCmdLen = 0;
        lines.forEach(line => {
        const idx = line.indexOf("#");
        if (idx > 0 && !line.trim().startsWith("#")) {
        const cmd = line.slice(0, idx).trimEnd();
        maxCmdLen = Math.max(maxCmdLen, cmd.length);
        }
        });

        // Format each line
        const MIN_GAP = 13;
        const MAX_LINE_LENGTH = 113;
        const baseColumn = Math.max(maxCmdLen + 2, MIN_GAP);
        const result = [];

        lines.forEach(line => {
        const idx = line.indexOf("#");

        // Skip pure comments and lines without comments
        if (idx === -1 || line.trim().startsWith("#")) {
        result.push(line);
        return;
        }

        const cmd = line.slice(0, idx).trimEnd();
        const commentText = line.slice(idx).replace(/^#\s*/, "# "); // Ensure single space after #

        // Calculate proper spacing
        const spacesNeeded = Math.max(2, baseColumn - cmd.length);
        const paddedLine = cmd + " ".repeat(spacesNeeded) + commentText;

        // If too long, move comment to next line
        if (paddedLine.length > MAX_LINE_LENGTH) {
        result.push(cmd);
        result.push("# " + commentText.slice(2)); // Remove # and add it back properly
        } else {
        result.push(paddedLine);
        }
        });

        codeBlock.innerText = result.join("\n");
        });
        }

        // Clean Code Blocks - Remove leading spaces
        function cleanCodeBlocks() {
        document.querySelectorAll("pre code").forEach((block) => {
        const lines = block.innerText.split("\n");
        const cleanedLines = lines.map((line) => line.replace(/^\s+/, ""));
        block.innerText = cleanedLines.join("\n");
        });
        }

        // Accessibility Improvements
        function improveAccessibility() {
        // Add ARIA labels to command blocks
        document.querySelectorAll(".command-block pre").forEach((block) => {
        if (!block.getAttribute("aria-label")) {
        block.setAttribute("aria-label", "Command block with code examples");
        }
        });

        // Add keyboard navigation
        document.querySelectorAll("a[href]").forEach((link) => {
        if (!link.getAttribute("role")) {
        link.setAttribute("role", "navigation");
        }
        });
        }

        // Table of Contents Generator
        function generateTOC() {
        const tocList = document.getElementById('toc-list');
        if (!tocList) return;

        const headings = document.querySelectorAll('h2, h3');
        const tocItems = [];

        headings.forEach((heading, index) => {
        if (heading.id) {
        const level = parseInt(heading.tagName.charAt(1));
        const title = heading.textContent.trim().replace(/\b\w/g, char => char.toUpperCase());

        const li = document.createElement('li');
        li.className = `toc-level-${level}`;

        const a = document.createElement('a');
        a.href = `#${heading.id}`;
        a.textContent = title;

        li.appendChild(a);
        tocItems.push(li);
        }
        });

        tocList.innerHTML = '';
        tocItems.forEach(item => tocList.appendChild(item));
        }

        // Initialize everything on page load
        window.addEventListener("DOMContentLoaded", function () {
        initDarkMode();
        formatComments();
        cleanCodeBlocks();
        generateTOC();
        });

        // Initialize accessibility on load
        window.addEventListener("load", improveAccessibility);

      </script>
        </div>
      </div>
        </div>
      </div>
    </div>
    </div>
  </body>
</html>
